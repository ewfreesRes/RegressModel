# Variable Selection

**Chapter description**

This chapter describes tools and techniques to help you select variables to enter into a linear regression model, beginning with an iterative model selection process. In applications with many potential explanatory variables, automatic variable selection procedures are available that will help you quickly evaluate many models. Nonetheless, automatic procedures have serious limitations including the inability to account properly for nonlinearities such as the impact of unusual points; this chapter expands upon the Chapter 2 discussion of unusual points. It also describes collinearity, a common feature of regression data where explanatory variables are linearly related to one another. Other topics that impact variable selection, including out-of-sample validation, are also introduced.


```{r comment = "", echo = FALSE, eval = FALSE, warning = FALSE}
# Reformat Data
RSurvey <- read.csv("CSVData\\RiskSurvey.csv", header = TRUE)
str(RSurvey)
RSurvey$firmcost <- RSurvey$FIRMCOST
RSurvey$assume <- RSurvey$ASSUME
RSurvey$cap <- RSurvey$CAP
RSurvey$logsize <- RSurvey$SIZELOG
RSurvey$indcost <- RSurvey$INDCOST
RSurvey$central <- RSurvey$CENTRAL
RSurvey$soph <- RSurvey$SOPH
RSurvey2 <- RSurvey[,c("firmcost", "assume", "cap", "logsize", "indcost", "central", "soph")]
str(RSurvey2)
#write.csv(RSurvey2,"CSVData\\Risk_survey.csv",row.names = FALSE)

```



## An iterative approach to data analysis and modeling

***

In this section, you learn how to:
  
-  Describe the iterative approach to data analysis and modeling.

***

###Video 

<center>
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/sp/166090200/embedIframeJs/uiconf_id/25916071/partner_id/1660902?iframeembed=true&playerId=kaltura_player&entry_id=1_53yv68dd&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;&wid=0_yxfgvaql" width="649" height="401" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" frameborder="0" title="Kaltura Player"></iframe>
</center>

#### Video Overhead Details {-}
<div class="tab">
  <button class="tablinks" onclick="openTab(event, 'Over4.1A')">A Details. Iterative approach</button>
  <button class="tablinks" onclick="openTab(event, 'Over4.1B')">B Details. Many possible models</button>
  <button class="tablinks" onclick="openTab(event, 'Over4.1C')">C Details. Model validation</button>
</div>


<div id="Over4.1A" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <h3>A Details. Iterative approach</h3>
  <p>

-  Model formulation stage
-  Fitting
-  Diagnostic checking - the data and model must be consistent with one another before additional inferences can be made.

```{r comment = "", eval = EVALUATE_CHAP4}
plot.new()
par(mar=c(0,0,0,0), cex=0.9)
plot.window(xlim=c(0,18),ylim=c(-5,5))

text(1,3,labels="DATA",adj=0, cex=0.8)
text(1,0,labels="PLOTS",adj=0, cex=0.8)
text(1,-3,labels="THEORY",adj=0, cex=0.8)
text(3.9,0,labels="MODEL\nFORMULATION",adj=0, cex=0.8)
text(8.1,0,labels="FITTING",adj=0, cex=0.8)
text(11,0,labels="DIAGNOSTIC\nCHECKING",adj=0, cex=0.8)
text(15,0,labels="INFERENCE",adj=0, cex=0.8)
text(14.1,0.5,labels="OK",adj=0, cex=0.6)

rect(0.8,2.0,2.6,4.0)
arrows(1.7,2.0,1.7,1.0,code=2,lwd=2,angle=25,length=0.10)
rect(0.8,-1.0,2.6,1.0)
arrows(1.7,-2.0,1.7,-1.0,code=2,lwd=2,angle=25,length=0.10)
rect(0.8,-4.0,2.6,-2.0)

arrows(2.6,0,3.2,0,code=2,lwd=2,angle=25,length=0.10)

x<-c(5,7.0,5,3.2)
y<-c(2,0,-2,0)
polygon(x,y)
arrows(7.0,0,8.0,0,code=2,lwd=2,angle=25,length=0.10)

rect(8.0,-1.0,9.7,1.0)
arrows(9.7,0,10.2,0,code=2,lwd=2,angle=25,length=0.10)

x1<-c(12,14.0,12,10.2)
y1<-c(2,0,-2,0)
polygon(x1,y1)
arrows(14.0,0,14.8,0,code=2,lwd=2,angle=25,length=0.10)

rect(14.8,-1.0,17.5,1.0)
arrows(12,-2.0,12,-3,code=2,lwd=2,angle=25,length=0.10)
arrows(12,-3.0,5,-3,code=2,lwd=2,angle=25,length=0.10)
arrows(5,-3.0,5,-2,code=2,lwd=2,angle=25,length=0.10)

```

</p>
</div> 

<div id="Over4.1B" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <h3>B Details. Many possible models</h3>
  <p>

$$
\begin{array}{l|r}
\hline
\text{E }y = \beta _{0} &     \text{1 model with no variables }  \\
\text{E }y = \beta _{0}+\beta_1 x_{i}, &  \text{4 models with one variable}  \\
\text{E }y = \beta _{0}+\beta_1 x_{i}+\beta_{2} x_{j}, &  \text{6 models with two variables}  \\
\text{E }y = \beta _{0}+\beta_{1} x_{1}+\beta_{2} x_{j} +\beta_{3} x_{k},&  \text{4 models with three variables} \\
\text{E }y = \beta_{0} + \beta_{1} x_{1} + \beta_{2} x_{2} +\beta_{3} x_{3}+\beta_{4} x_{4} &  \text{1 model with all variables}  \\ 
\hline
\end{array}
$$

- With *k* explanatory variables, there are $2^k$ possible linear models
- There are infinitely many nonlinear ones!!

</p>
</div> 

<div id="Over4.1C" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <h3>C Details. Model validation</h3>
  <p>

- Model validation is the process of confirming our proposed model.
- Concern: *data-snooping* - fitting many models to a single set of data.
    - Response to concern: *out-of-sample validation*.
    - Divide the data into *model development*, or *training* and *validation*, or *test*, subsamples.
    
```{r comment = "", eval = EVALUATE_CHAP4}
par(mai=c(0,0.1,0,0))
plot.new()
plot.window(xlim=c(0,18),ylim=c(-10,10))
rect(1,-1.2,14,1.2)
rect(7,4,15,8)
rect(1,-8,6,-4)
x<-seq(1.5,9,length=6)
y<-rep(0,6)
text(x,y,labels=c(1:6),cex=1.5)
x1<-seq(10.5,11.5,length=3)
y1<-rep(0,3)
text(x1,y1,labels=rep(".",3),cex=3)
text(13,0,labels="n",cex=1.5)

text(15,0,labels="ORIGINAL\nSAMPLE\nSIZE n",adj=0)
text(7.5,6,labels="MODEL DEVELOPMENT\nSUBSAMPLE SIZE",adj=0)
text(12.5,5.3, expression(n[1]), adj=0, cex=1.1)
text(1.4,-6,labels="VALIDATION\nSUBSAMPLE\nSIZE",adj=0)
text(2.8,-7.2,expression(n[2]),adj=0, cex=1.1)

arrows(1.8,0.8,8.3,3.9,code=2,lwd=2,angle=15,length=0.2)
arrows(4.8,0.8,9,3.8,code=2,lwd=2,angle=15,length=0.2)
arrows(9.1,0.9,9.5,3.8,code=2,lwd=2,angle=15,length=0.2)
arrows(12.8,0.8,10,3.8,code=2,lwd=2,angle=15,length=0.2)
arrows(2.9,-0.9,2.5,-3.8,code=2,lwd=2,angle=15,length=0.2)
arrows(5.9,-0.9,3.1,-3.8,code=2,lwd=2,angle=15,length=0.2)
arrows(7.4,-0.9,3.5,-3.8,code=2,lwd=2,angle=15,length=0.2)

```

</p>
</div> 

###MC Exercise. An iterative approach to data modeling

Which of the following is not true?

- A. Diagnostic checking reveals symptoms of mistakes made in previous specifications.
- B. Diagnostic checking provides ways to correct mistakes made in previous specifications.
- C. Model formulation is accomplished by using prior knowledge of relationships.
- D. Understanding theoretical model properties is not really helpful when matching a model to data or inferring general relationships based on the data. 


## Automatic variable selection procedures

***

In this section, you learn how to:
  
- Identify some examples of automatic variable selection procedures
- Describe the purpose of automatic variable selection procedures and their limitations
- Describe "data-snooping"

***

###Video 

<center>
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/sp/166090200/embedIframeJs/uiconf_id/25916071/partner_id/1660902?iframeembed=true&playerId=kaltura_player&entry_id=1_94bk7tyn&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;&wid=0_vq3apay6" width="649" height="401" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" frameborder="0" title="Kaltura Player"></iframe>
</center>

#### Video Overhead Details {-}
<div class="tab">
  <button class="tablinks" onclick="openTab(event, 'Over4.2A')">A Details. Classic stepwise regression algorithm</button>
  <button class="tablinks" onclick="openTab(event, 'Over4.2B')">B Details. Drawbacks of stepwise regression</button>
  <button class="tablinks" onclick="openTab(event, 'Over4.2C')">C Details. Data-snooping in stepwise regression</button>
  <button class="tablinks" onclick="openTab(event, 'Over4.2D')">D Details. Variants of stepwise regression</button>
  <button class="tablinks" onclick="openTab(event, 'Over4.2E')">E Details. Automatic variable selection procedures</button>
</div>


<div id="Over4.2A" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <h3>A Details. Classic stepwise regression algorithm</h3>
  <p>

Suppose that the analyst has identified one variable as the outcome, $y$, and $k$ potential explanatory variables, $x_1, x_2, \ldots, x_k$.

- (i). Consider all possible regressions using one explanatory variable. Choose the one with the highest *t*-statistic.
- (ii). Add a variable to the model from the previous step. The variable to enter is with the highest *t*-statistic.
- (iii). Delete a variable to the model from the previous step. Delete the variable with the small *t*-statistic if the statistic is less than, e.g., 2 in absolute value.
- (iv). Repeat steps (ii) and (iii) until all possible additions and deletions are performed.

</p>
</div> 


<div id="Over4.2B" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <h3>B Details. Drawbacks of stepwise regression</h3>
  <p>

- The procedure "snoops" through a large number of models and may fit the data "too well."
- There is no guarantee that the selected model is the best. 
    - The algorithm does not consider models that are based on nonlinear combinations of explanatory variables. 
    - It ignores the presence of outliers and high leverage points.

</p>
</div> 

<div id="Over4.2C" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <h3>C Details. Data-snooping in stepwise regression</h3>
  <p>

- Generate $y$ and $x_1 - x_{50}$ using a random number generator
- By design, there is no relation between $y$ and $x_1 - x_{50}$.
- **But**, through stepwise regression, we **"discover"** a relationship that explains 14\% of the variation!!!

```
Call: lm(formula = y ~ xvar27 + xvar29 + xvar32, data = X)

Coefficients:
            Estimate Std. Error t value Pr(>|t|)  
(Intercept) -0.04885    0.09531  -0.513   0.6094  
xvar27        0.21063    0.09724   2.166   0.0328 *
xvar29        0.24887    0.10185   2.443   0.0164 *
xvar32        0.25390    0.09823   2.585   0.0112 *

Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.9171 on 96 degrees of freedom
Multiple R-squared:  0.1401,    Adjusted R-squared:  0.1132 
F-statistic: 5.212 on 3 and 96 DF,  p-value: 0.002233
```

</p>
</div> 

<div id="Over4.2D" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <h3>D Details. Variants of stepwise regression</h3>
  <p>

This uses the `R` function [step()](https://www.rdocumentation.org/packages/stats/versions/3.5.0/topics/step)

- The option `direction` can be used to change how variables enter
    - Forward selection. Add one variable at a time without trying to delete variables.
    - Backwards selection. Start with the full model and delete one variable at a time without trying to add variables. 
- The option `scope` can be used to specify which variables must be included

</p>
</div> 

<div id="Over4.2E" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <h3>E Details. Automatic variable selection procedures</h3>
  <p>

- Stepwise regression is a type of automatic variable selection procedure.
- These procedures are useful because they can quickly search through several candidate models. They  mechanize certain routine tasks and are excellent at discovering patterns in data.
- They are so good at detecting patterns that they analyst must be wary of overfitting (data-snooping)   
- They can miss certain patterns (nonlinearities, unusual points)
- A model suggested by automatic variable selection procedures should be subject to the same careful diagnostic checking procedures as a model arrived at by any other means

</p>
</div>

###Exercise. Data-snooping in stepwise regression

**Assignment Text**

Automatic variable selection procedures, such as the classic stepwise regression algorithm, are very good at detecting patterns. Sometimes they are too good in the sense that they detect patterns in the sample that are not evident in the population from which the data are drawn. The detect "spurious" patterns.

This exercise illustrates this phenomenom by using a simulation, designed so that the outcome variable (*y*) and the explanatory variables are mutually independent. So, by design, there is no relationship between the outcome and the explanatory variables.

As part of the code set-up, we have *n* = 100 observations generated of the outcome *y* and 50 explanatory variables, `xvar1` through `xvar50`. As anticipated, collections of explanatory variables are not statistically significant. However, with the [step()](https://www.rdocumentation.org/packages/stats/versions/3.5.0/topics/step) function, you will find some statistically significant relationships!


**Instructions**

- Fit a basic linear regression model and MLR model with the first ten explanatory variables. Compare the models via an *F* test.
- Fit a multiple linear regression model with all fifty explanatory variables. Compare this model to the one with ten variables via an *F* test.
- Use the `step` function to find the best model starting with the fitted model containing all fifty explanatory variables and summarize the fit. 

**Hint.** The code shows stepwise regression using BIC, a criterion that results in simpler models than AIC. For AIC, use the option `k=2` in the [step()] function (the default)

```{r ex="ExerRegMod4.2.2", type="hint", tut=TRUE}
The `step()` function automatically selects which explanatory variables are significant enough to belong in the model given a goodness of fit criterion. Having k=log(nrow(x)) specifies using BIC.
```

```{r ex="ExerRegMod4.2.2", type="pre-exercise-code", tut=TRUE}
set.seed(1237)
X <- as.data.frame(matrix(rnorm(100*50, mean = 0, sd = 1), ncol = 50))
colnames(X) <- paste("xvar", 1:50, sep = "")
X$y <- with(X, matrix(rnorm(100*1, mean = 0, sd = 1), ncol = 1))
#cor(X[,c("xvar1","xvar2","xvar3","xvar4","xvar5","xvar6","xvar7","xvar8","xvar9","xvar10","y")], use = "complete.obs")
```

```{r ex="ExerRegMod4.2.2", type="sample-code", tut=TRUE}
# Fit a basic linear regression model and MLR model with the first ten explanatory variables. Compare the models via an *F* test.
model_step1 <- lm(y ~ xvar1, data = X)
model_step10 <- lm(y ~ xvar1 + xvar2 + xvar3 + xvar4 + xvar5 + xvar6 + xvar7 + xvar8 + xvar9 + xvar10, data = X)
anova(___, ___)

# Fit a multiple linear regression model with all fifty explanatory variables. Compare this model to the one with ten variables via an *F* test.
model_step50 <- lm(y ~ xvar1 + xvar2 + xvar3 + xvar4 + xvar5 + xvar6 + xvar7 + xvar8 + xvar9 + xvar10 + xvar11 + xvar12 + xvar13 + xvar14 + xvar15 + xvar16 + xvar17 + xvar18 + xvar19 + xvar20 + xvar21 + xvar22 + xvar23 + xvar24 + xvar25 + xvar26 + xvar27 + xvar28 + xvar29 + xvar30 + xvar31 + xvar32 + xvar33 + xvar34 + xvar35 + xvar36 + xvar37 + xvar38 + xvar39 + xvar40 + xvar41 + xvar42 + xvar43 + xvar44 + xvar45 + xvar46 + xvar47 + xvar48 + xvar49 + xvar50, data = X)
anova(___, ___)

# Use the `step` function, starting with the fitted model containing all fifty explanatory variables and summarize the fit.
#For BIC: 
model_stepwise <- step(___, data = X, direction= "both", k = log(nrow(X)), trace = 0) 
summary(model_stepwise)

```


```{r ex="ExerRegMod4.2.2", type="solution", tut=TRUE}
model_step1 <- lm(y ~ xvar1, data = X)
model_step10 <- lm(y ~ xvar1 + xvar2 + xvar3 + xvar4 + xvar5 + xvar6 + xvar7 + xvar8 + xvar9 + xvar10, data = X)
anova(model_step1,model_step10)
model_step50 <- lm(y ~ xvar1 + xvar2 + xvar3 + xvar4 + xvar5 + xvar6 + xvar7 + xvar8 + xvar9 + xvar10 + xvar11 + xvar12 + xvar13 + xvar14 + xvar15 + xvar16 + xvar17 + xvar18 + xvar19 + xvar20 + xvar21 + xvar22 + xvar23 + xvar24 + xvar25 + xvar26 + xvar27 + xvar28 + xvar29 + xvar30 + xvar31 + xvar32 + xvar33 + xvar34 + xvar35 + xvar36 + xvar37 + xvar38 + xvar39 + xvar40 + xvar41 + xvar42 + xvar43 + xvar44 + xvar45 + xvar46 + xvar47 + xvar48 + xvar49 + xvar50, data = X)
anova(model_step10,model_step50)

#For BIC: 
model_stepwise <- step(model_step50, data = X, direction= "both", k = log(nrow(X)), trace = 0) 
summary(model_stepwise)

# An example with scope
#model_step5a <- step(model_step4, data = X, direction= "both", k=log(nrow(X)), trace = 0,
    #            scope = list(lower = ~xvar1+xvar2, upper = model_step4)) 
#summary(model_step5a)
#For AIC: 
#step(model_step4, data = X, direction= "both", k=2, trace = 0) # k=2 is by default 

```

```{r ex="ExerRegMod4.2.2", type="sct", tut=TRUE}
ex() %>% check_object("model_step1", undefined_msg="Please use `lm` to create a linear model, and name it `model_step1`.") %>% check_equal(incorrect_msg="Make sure to model `y` using `xvar1` from the dataframe `x`.")
ex() %>% check_object("model_step10", undefined_msg="Please use `lm` to create a linear model, and name it `model_step10`.") %>% check_equal(incorrect_msg="Make sure to model `y` using `xvar` from 1 to 10 in the dataframe `x`.")
ex() %>% check_function("anova",index=1, not_called_msg="Use `anova` to compare the two linear models we have just created. ") %>% check_result() %>% check_equal(incorrect_msg="Make sure to compare `model_Step1` to `model_step10`.")
ex() %>% check_object("model_step50", undefined_msg="Please use `lm` to create a linear model, and name it `model_step50`.") %>% check_equal(incorrect_msg="Make sure to model `y` using all 50 `xvar`s from the dataframe `x`.")
ex() %>% check_function("anova",index=2, not_called_msg="Use `anova` to compare the two most recent linear models. ") %>% check_result() %>% check_equal(incorrect_msg="Make sure to compare `model_step10` with `model_step50`.")
ex() %>% check_object("model_stepwise", undefined_msg="Make sure to use the `step` functions to transform `model_step50` into a more useful model, named `model_stepwise’. ") %>% check_equal(incorrect_msg="Make sure to specify that we want to edit `model_step50`, using the data found in `x` in `both` directions, a trace value of `0` and k set to the log of the number of rows in `x`.")
ex() %>% check_function("summary", not_called_msg="Use `summary` to take a look at `model_stepwise`.") %>% check_arg(., "object") %>% check_equal(incorrect_msg="Make sure to call `summary` on `model_stepwise`.")
success_msg("Excellent! The step procedure repeatedly fits many models to a data set. We summarize each fit with hypothesis testing statistics like t-statistics and p-values. But, remember that hypothesis tests are designed to falsely detect a relationship a fraction of the time (typically 5%). For example, if you run a t-test 50 times (for each explanatory variable), you can expect to get two or three statistically significant explanatory variables even for unrelated variables (because 50 times 0.05 = 2.5).")

```


## Residual analysis

***

In this section, you learn how to:

- Explain how residual analysis can be used to improve a model specification
- Use relationships between residuals and potential explanatory variables to improve model specification

***

###Video 

<center>
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/sp/166090200/embedIframeJs/uiconf_id/25916071/partner_id/1660902?iframeembed=true&playerId=kaltura_player&entry_id=1_bwhmsm8k&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;&wid=0_zvacn361" width="649" height="401" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" frameborder="0" title="Kaltura Player"></iframe>
</center>

#### Video Overhead Details {-}

<div class="tab">
  <button class="tablinks" onclick="openTab(event, 'Over4.3A')">A Details. Residual analysis</button>
  <button class="tablinks" onclick="openTab(event, 'Over4.3B')">B Details. Using residuals to select explanatory variables</button>
  <button class="tablinks" onclick="openTab(event, 'Over4.3C')">C Details. Detecting relationships between residuals and explanatory variables</button>
</div>


<div id="Over4.3A" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <h3>A Details. Residual analysis</h3>
  <p>

- Use $e_i = y_i - \hat{y}_i$ as the *i*th residual.
- Later, I will discuss rescaling by, for example, $s$, to get a standardized residual.
- *Role of residuals*: If the model formulation is correct, then residuals should be approximately equal to random errors or "white noise."
- *Method of attack*: Look for patterns in the residuals. Use this information to improve the model specification.

</p>
</div> 

<div id="Over4.3B" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <h3>B Details. Using residuals to select explanatory variables</h3>
  <p>

- Residual analysis can help identify additional explanatory variables that may be used to improve the formulation of the model.
- If the model is correct, then residuals should resemble random errors and contain no discernible patterns.
- Thus, when comparing residuals to explanatory variables, we do not expect any relationships.
- If we do detect a relationship, then this suggests the need to control for this additional variable.

</p>
</div> 

<div id="Over4.3C" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <h3>C Details. Detecting relationships between residuals and explanatory variables</h3>
  <p>

- Calculate summary statistics and display the distribution of  residuals to identify outliers.
- Calculate the correlation between the residuals and additional explanatory variables to search for linear relationships.
- Create scatter plots between the residuals and additional explanatory variables to search for nonlinear relationships.

</p>
</div> 


###Exercise. Residual analysis and risk manager survey

**Assignment Text**

This exercise examines data, pre-loaded in the dataframe `survey`, from a survey on the cost effectiveness of risk management practices. Risk management practices are activities undertaken by a firm to minimize the potential cost of future losses, such as the event of a fire in a warehouse or an accident that injures employees. This exercise develops a model that can be used to make statements about cost of managing risks.

A measure of risk management cost effectiveness, `logcost`, is the outcome variable. This variable is defined as total property and casualty premiums and uninsured losses as a proportion of total assets, in logarithmic units. It is a proxy for annual expenditures associated with insurable events, standardized by company size. Explanatory variables include 
`logsize`, the logarithm of total firm assets, and `indcost`, a measure of the firm's industry risk.

**Instructions**

- Fit and summarize a MLR model using `logcost` as the outcome variable and `logsize` and `indcost` as explanatory variables.
- Plot residuals of the fitted model versus `indcost` and superimpose a locally fitted line using the `R` function [lowess()](https://www.rdocumentation.org/packages/stats/versions/3.5.0/topics/lowess).
- Fit and summarize a MLR model of `logcost` on `logsize`, `indcost` and a squared version of `indcost`.
- Plot residuals of the fitted model versus `indcost' and superimpose a locally fitted line using [lowess()](https://www.rdocumentation.org/packages/stats/versions/3.5.0/topics/lowess).

**Hint.** You can access model residuals using `mlr.survey1$residuals` or `mlr.survey1($residuals)`

```{r ex="ExerRegMod4.3.2", type="hint", tut=TRUE}
The best way to utilize the residuals from a linear model is `model.name$residuals`.
```

```{r ex="ExerRegMod4.3.2", type="pre-exercise-code", tut=TRUE}
#survey <- read.csv("CSVData\\Risk_survey.csv", header=TRUE)
survey <- read.csv("https://assets.datacamp.com/production/repositories/2610/datasets/dc1c5bce43ef076aa77169a242118e2e58d01f82/Risk_survey.csv", header=TRUE)
survey$logcost <- log(survey$firmcost)
#str(survey)
```

```{r ex="ExerRegMod4.3.2", type="sample-code", tut=TRUE}
# Regress `logcost` on `logsize` and `indcost` 
mlr.survey1 <- lm(logcost ~ logsize + indcost, data = survey)
summary(___)

# Plot residuals of the fitted model versus `indcost` and superimpose a locally fitted line using the  function [lowess()]
plot(survey$indcost,  ___)
lines(lowess(survey$indcost, ___))

# Regress `logcost` on `logsize` and `indcost` and `indcost` squared
mlr.survey2 <- lm(___ ~ logsize + poly(indcost,2), data = survey)
summary(___)

# Plot residuals of this fitted model and superimpose a locally fitted line using the function [lowess()]
plot(survey$indcost, ___)
lines(lowess(survey$indcost, ___))

```


```{r ex="ExerRegMod4.3.2", type="solution", tut=TRUE}
mlr.survey1 <- lm(logcost ~ logsize + indcost, data = survey)
summary(mlr.survey1)

plot(survey$indcost, mlr.survey1$residuals)
lines(lowess(survey$indcost,mlr.survey1$residuals))

mlr.survey2 <- lm(logcost ~ logsize + poly(indcost,2), data = survey)
summary(mlr.survey2)

plot(survey$indcost, mlr.survey2$residuals)
lines(lowess(survey$indcost,mlr.survey2$residuals))

```

```{r ex="ExerRegMod4.3.2", type="sct", tut=TRUE}
ex() %>% check_object("mlr.survey1", undefined_msg="Use `lm` to create a linear model named `mlr.survey1`.") %>% check_equal(incorrect_msg="make sure to model `logcost` using `logsize` and `indcost` from the dataframe `survey`.")
ex() %>% check_function("summary",index=1, not_called_msg="Use the `summary` function to view the summary stats of `mlr.survey1`.") %>% check_arg(., "object") %>% check_equal(incorrect_msg="Make sure to specify that we would like the summary stats of `mlr.survey1`.")
ex() %>% check_function("plot",index=1, not_called_msg="please create a plot the shows `indcosts` and `mlr.survey1`’s residuals. ") %>%{
  check_arg(., "x") %>% check_equal(incorrect_msg="The independent variable should be `indcosts`.")
  check_arg(., "y") %>% check_equal(incorrect_msg="The dependent variable should be the residuals of `mlr.survey1`.")
}
ex() %>% check_function("lines",index=1, not_called_msg="Please use the `lines` function to add a line to the plot. ")
ex() %>% check_function("lowess",index=1, not_called_msg="Please use the `lowess` function to add a smoothed effect on our line. ") %>%{
  check_arg(., "x") %>% check_equal(incorrect_msg="The first argument should be `indcosts`.")
  check_arg(., "y") %>% check_equal(incorrect_msg="The second argument should be the residuals from `mlr.survey1`.")
}
ex() %>% check_object("mlr.survey2", undefined_msg="Use `lm` to create a linear model that is stored under the name `mlr.survey2`.") %>% check_equal(incorrect_msg="Make sure to model `logcost` using `logsize` and `indcost` squared, which can be found using `poly(indcost,2)`. ")
ex() %>% check_function("summary",index=2, not_called_msg="Use `summary` to take a look at `mlr.survey2`.") %>% check_arg(., "object") %>% check_equal(incorrect_msg="Make sure to specify that we want a summary of `mlr.survey2`.")
ex() %>% check_function("plot",index=2, not_called_msg="Now make a plot that shows the `indcost` based on the residuals of `mlr.survey2`.") %>%{
  check_arg(., "x") %>% check_equal(incorrect_msg="The independent variable should be `indcost`.")
  check_arg(., "y") %>% check_equal(incorrect_msg="The dependent variable should be the residuals of `mlr.survey2`.")
}
ex() %>% check_function("lines",index=2, not_called_msg="Make sure to add a line to this new graph using `lines`.")
ex() %>% check_function("lowess",index=2, not_called_msg="make sure to use the `lowess` function to add a smoothed effect to the line. ") %>%{
  check_arg(., "x") %>% check_equal(incorrect_msg="The first argument should be `indcost`.")
  check_arg(., "y") %>% check_equal(incorrect_msg="The second argument should be the residuals of `mlr.survey2`.")
}
success_msg("Excellent! In this exercise, you examined residuals from a preliminary model fit and detected a mild quadratic pattern in a variable. This suggested entering the squared term of that variable into the model specification. The refit of this new model suggests that the squared term has important explanatory information. The squared term is a nonlinear alternative that is not available in many automatic variable selection procedures.")

```


###Exercise. Added variable plot and refrigerator prices

**Assignment Text**

What characteristics of a refrigerator are important in determining its price (`price`)? We consider here several characteristics of a refrigerator, including the size of the
refrigerator in cubic feet (`rsize`), the size of the freezer compartment in cubic feet (`fsize`), the average amount of money spent per year to operate the refrigerator (`ecost`, for energy cost), the number of shelves in the refrigerator and freezer doors (`shelves`), and the number of features (`features`). The features variable includes shelves for cans, see-through crispers, ice makers, egg racks and so on.

Both consumers and manufacturers are interested in models of refrigerator prices. Other things equal, consumers generally prefer larger refrigerators with lower energy costs that have more features. Due to forces of supply and demand, we would expect consumers to pay more for these refrigerators. A larger refrigerator with lower energy costs that has more features at the similar price is considered a bargain to the consumer. How much extra would the consumer be willing to pay for this additional space? A model of prices for refrigerators on the market provides some insight to this question.

To this end, we analyze data from *n* = 37 refrigerators. 



**Instructions**


```{r comment = "", eval = EVALUATE_CHAP4}
# Pre-exercise code
Refrig <- read.table("CSVData\\Refrig.csv", header = TRUE, sep = ",")
summary(Refrig)
Refrig1 <- Refrig[c("price", "ecost", "rsize", "fsize", "shelves", "s_sq_ft", "features")]
round(cor(Refrig1), digits = 3)
refrig_mlr1 <- lm(price ~ rsize + fsize + shelves + features, data = Refrig)
summary(refrig_mlr1)
Refrig$residuals1 <- residuals(refrig_mlr1)
refrig_mlr2 <- lm(ecost ~ rsize + fsize + shelves + features, data = Refrig)
summary(refrig_mlr2)
Refrig$residuals2 <- residuals(refrig_mlr2)
plot(Refrig$residuals2, Refrig$residuals1)

#library(Rcmdr)
#refrig_mlr3 <- lm(price ~ rsize + fsize + shelves + features + ecost, data = Refrig)
#avPlots(refrig_mlr3, terms = "ecost")

```

## Unusual observations

***

In this section, you learn how to:

- Compare and contrast three alternative definitions of a standardized residual
- Evaluate three alternative options for dealing with outliers
- Assess the impact of a high leverage observation
- Evaluate options for dealing with high leverage observations
- Describe the notion of influence and Cook's Distance for quantifying influence

***

###Video 

<center>
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/sp/166090200/embedIframeJs/uiconf_id/25916071/partner_id/1660902?iframeembed=true&playerId=kaltura_player&entry_id=1_rhh81ftg&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;&wid=1_58daiyn0" width="649" height="401" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" frameborder="0" title="Kaltura Player"></iframe>
</center>

#### Video Overhead Details {-}
<div class="tab">
  <button class="tablinks" onclick="openTab(event, 'Over4.4A')">A Details. Unusual observations</button>
  <button class="tablinks" onclick="openTab(event, 'Over4.4B')">B Details. Standardized residuals</button>
  <button class="tablinks" onclick="openTab(event, 'Over4.4C')">C Details. Outlier - an unusal standardized residual</button>
  <button class="tablinks" onclick="openTab(event, 'Over4.4D')">D Details. High leverage points</button>
  <button class="tablinks" onclick="openTab(event, 'Over4.4E')">E Details. High leverage point graph</button>
  <button class="tablinks" onclick="openTab(event, 'Over4.4F')">F Details. Leverage</button>
</div>


<div id="Over4.4A" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <h3>A Details. Unusual observations</h3>
  <p>

- Regression coefficients can be expressed as (matrix) weighted averages of outcomes
    - Averages, even weighted averages can be strongly influenced by unusual observations
- Observations may be unusual in the *y* direction or in the *X* space
- For unusual in the *y* direction, we use a residual $e = y - \hat{y}$
    - By subtracting the fitted value $\hat{y}$, we look to the *y* distance from the regression plane
    - In this way, we "control" for values of explanatory variables

</p>    
</div> 

<div id="Over4.4B" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <h3>B Details. Standardized residuals</h3>
  <p>

We standardize residuals so that we can focus on relationships of interest and achieve carry-over of experience from one data set to another.

Three commonly used definitions of standardize residuals are:

$$
\text{(a) }\frac{e_i}{s}, \ \ \ \text{ (b) }\frac{e_i}{s\sqrt{1-h_{ii}}}, \  \   \    
\text{(c)}\frac{e_i}{s_{(i)}\sqrt{1-h_{ii}}}.
$$

- First choice is simple
- Second choice, from theory, $\mathrm{Var}(e_i)=\sigma ^{2}(1-h_{ii}).$ Here, $h_{ii}$ is the $i$th *leverage* (defined later).
- Third choice is termed "studentized residuals". Idea: numerator is independent of the denominator.

</p>
</div> 

<div id="Over4.4C" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <h3>C Details. Outlier - an unusal standardized residual</h3>
  <p>

- An *outlier* is an observation that is not well fit by the model; these are observations where the residual is unusually large.
- Unusual means what?  Many packages mark a point if the |standardized residual| > 2.
- Options for handling outliers
    - Ignore them in the analysis but be sure to discuss their effects.
    - Delete them from the data set (but be sure to discuss their effects).
    - Create a binary variable to indicator their presence. (This will increase your $R^2$!)

</p>
</div> 

<div id="Over4.4D" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <h3>D Details. High leverage points</h3>
  <p>

- A high leverage point is an observation that is "far away" in the $x$-space from others.
- One can get a feel for high leverage observations by looking a summary statistics (mins, maxs) for each explanatory variable.
- Options for dealing with high leverage points are comparable to outliers, we can ignore their effects, delete them, or mark them with a binary indicator variable.

</p>
</div> 

<div id="Over4.4E" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <h3>E Details. High leverage point graph</h3>
  <p>

```{r comment = "", warning = FALSE, eval = EVALUATE_CHAP4}
library(cluster)
#library(MASS)
par(mar=c(3.2,5.4,.2,.2))
plot(1,5,type="p",pch=19,cex=1.5,xlab="",ylab="",cex.lab=1.5,xaxt="n",yaxt="n",xlim=c(-3,5),ylim=c(-12,12))
mtext(expression(x[2]), side=1,line=2, cex=2.0)
mtext(expression(x[1]), side=2, line=2, las=2, cex=2.0)
arrows(1.5,5,4,5,code=1,lwd=2,angle=15,length=0.25)
xycov<-matrix(c(2, -5,-5, 20),nrow=2,ncol=2)
xyloc<-matrix(c(0, 0),nrow=1,ncol=2)
polygon(ellipsoidPoints(xycov, d2 = 2, loc=xyloc),col="black")
```

</p>
</div> 

<div id="Over4.4F" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <h3>F Details. Leverage</h3>
  <p>

- Using matrix algebra, one can express the *i*th fitted value as a linear combination of observations

$$
\hat{y}_{i} = h_{i1} y_{1} + \cdots +h_{ii}y_{i}+\cdots+h_{in}y_{n}.
$$

- The term $h_{ii}$ is known as the *i*th leverage
    - The larger the value of $h_{ii}$, the greater the effect of the *i*th observation $y_i$ on the *i*th fitted value $\hat{y}_i$.
    - Statistical routines have values of the leverage coded, so computing this quantity. The key thing to know is that  $h_{ii}$ is based solely on the explanatory variables. If you change the $y$ values, the leverage does not change.
    - As a commonly used rule of thumb, a leverage is deemed to be "unusual" if its value exceeds three times the average (= number of regression coefficients divided by the number of observations.)
    
</p>    
</div> 

###Exercise. Outlier example

In chapter 2, we consider a fictitious data set of 19 "base" points plus three different types of unusual points. In this exercise, we consider the effect of one unusal point, "C", this both an outlier (unusual in the "y" direction) and a high leverage point (usual in the x-space). The data have been pre-loaded in the dataframe `outlrC`.

**Instructions**

- Fit a basic linear regression model of `y` on `x` and store the result in an object.
- Use the function [rstandard()](https://www.rdocumentation.org/packages/stats/versions/3.5.0/topics/influence.measures) to extract the standardized residuals from the fitted regression model object and summarize them. 
- Use the function [hatvalues()](https://www.rdocumentation.org/packages/stats/versions/3.5.0/topics/influence.measures) to extract the leverages from the model fitted and summarize them. 
- Plot the standardized residuals versus the leverages to see the relationship between these two measures that calibrate how unusual an observation is.

```{r ex="ExerRegMod4.4.2", type="hint", tut=TRUE}
There is nothing to change in the sample code. Simply play around with the code to figure out what exactly it does.
```

```{r ex="ExerRegMod4.4.2", type="pre-exercise-code", tut=TRUE}
#outlr <- read.csv("CSVData\\Outlier.csv", header = TRUE)
outlr <- read.csv("https://assets.datacamp.com/production/repositories/2610/datasets/7a38912e544c31fc6f5fca12b9a2eb645f2bcd32/Outlier.csv", header = TRUE)
outlrC <- outlr[-c(20,21),c("x","y")]
```

```{r ex="ExerRegMod4.4.2", type="sample-code", tut=TRUE}
outlrC <- outlr[-c(20,21),c("x","y")]

# Fit a basic linear regression model of `y` on `x` and store the result in an object.
model_outlrC <- lm(y ~ x, data = outlrC)

# Extract the standardized residuals from the fitted regression model object and summarize them.
ri <- rstandard(model_outlrC)
summary(ri)

# Extract the leverages from the model fitted and summarize them. 
hii <- hatvalues(model_outlrC)
summary(hii)

# Plot the standardized residuals versus the leverages
plot(hii,ri)

```

```{r ex="ExerRegMod4.4.2", type="solution", tut=TRUE}
plot(outlrC)
model_outlrC <- lm(y ~ x, data = outlrC)
ri <- rstandard(model_outlrC)
summary(ri)
hii <- hatvalues(model_outlrC)
summary(hii)
plot(hii,ri)

```

```{r ex="ExerRegMod4.4.2", type="sct", tut=TRUE}
ex() %>% check_function("plot",index=1, not_called_msg="Please use `plot` to create a plot of the data in `outlrC`.") %>% check_arg(., "x") %>% check_equal(incorrect_msg="Make sure to specify that we want to plot the data found in `outlrC`.")
ex() %>% check_object("model_outlrC", undefined_msg="Make sure to create a linear model and store it under the name `model_outlrC`.") %>% check_equal(incorrect_msg="This linear model should explain `y` based on `x` which is found in the dataframe `outlrC`.")
ex() %>% check_object("ri", undefined_msg="Make sure to extract the residuals to the variable `ri`.") %>% check_equal(incorrect_msg="You can extract the standardized residuals by using `rstandard` on `model_outlrC`.")
ex() %>% check_function("summary",index=1, not_called_msg="Use the `summary` function to view the summary of `ri`.") %>% check_arg(., "object") %>% check_equal(incorrect_msg="Make sure to specify that we want to look at the summary of `ri`.")
ex() %>% check_object("hii", undefined_msg="Make sure to extract the leverage values to a variable named `hii`.") %>% check_equal(incorrect_msg="You can extract the leverage values by using the `hatvalues` function on `model_outlrC`.")
ex() %>% check_function("summary",index=2, not_called_msg="Use the `summary` function to view the summary of `hii`.") %>% check_arg(., "object") %>% check_equal(incorrect_msg="Make sure the specify that we want to look at the summary of `hii`. ")
ex() %>% check_function("plot",index=2, not_called_msg="Please use `plot` to create a plot of the standardized residuals versus the leverages. ") %>% {
  check_arg(., "x") %>% check_equal(incorrect_msg="We would like the leverages to be on the horizontal axis. ")
  check_arg(., "y") %>% check_equal(incorrect_msg="We would like the standardized residuals to be on the vertical axis. ")
}
success_msg("Excellent! With only two variables, we could argue graphically that observations were unusual. In this exercise, we showed how statistics could also be used to identify usual observations. Although not really necessary in basic linear regression, the main advantage of the statistics is that they work readily in a multivariate setting.")

```

###Exercise. High leverage and risk manager survey


**Assignment Text**

In a prior exercise, we fit a regression model of `logcost` on `logsize`, `indcost` and a squared version of `indcost`. This model is summarized in the object `mlr_survey2`. In this exercise, we examine the robustness of the model to unusual observations. 

**Instructions**

- Use the `R` functions [rstandard()](https://www.rdocumentation.org/packages/stats/versions/3.5.0/topics/influence.measures) and [hatvalues()](https://www.rdocumentation.org/packages/stats/versions/3.5.0/topics/influence.measures) to extract the standardized residuals and leverages from the model fitted. Summarize the distributions graphically.
- You will see that there are two observations where the leverages are high, numbers 10 and 16. On looking at the dataset, these turn out to be observations in a high risk industry. Create a histogram of the variable `indcost` to corroborate this.
- Re-run the regression omitting observations 10 and 16. Summarize this regression and the regression in the object  `mlr_survey2`, noting differences in the coefficients.

```{r ex="ExerRegMod4.4.3", type="hint", tut=TRUE}
`rstandard()` and `hatvalues()` can be used to extract teh standardized residuals and leverages from a fitted model.
```

```{r ex="ExerRegMod4.4.3", type="pre-exercise-code", tut=TRUE}
#survey <- read.csv("CSVData\\Risk_survey.csv", header=TRUE)
survey <- read.csv("https://assets.datacamp.com/production/repositories/2610/datasets/dc1c5bce43ef076aa77169a242118e2e58d01f82/Risk_survey.csv", header=TRUE)
survey$logcost <- log(survey$firmcost)
mlr.survey2 <- lm(logcost ~ logsize + poly(indcost,2), data = survey)
```

```{r ex="ExerRegMod4.4.3", type="sample-code", tut=TRUE}
mlr.survey2 <- lm(logcost ~ logsize + poly(indcost,2), data = survey)
# Extract the standardized residuals and leverages from the model fitted. Summarize the distributions graphically.
ri <- ___(mlr.survey2)
hii <- ___(mlr.survey2)
par(mfrow=c(1, 2))
hist(ri, nclass=16, main="", xlab="Standardized Residuals")
hist(hii, nclass=16, main="", xlab="Leverages")

# Create a histogram of the variable `indcost`
par(mfrow=c(1, 1))
hist(___, nclass=16)

# Re-run the regression omitting observations 10 and 16. Summarize this regression and the regression in the object  `mlr_survey2`, noting differences in the coefficients.
mlr.survey3 <- lm(___ ~ logsize + poly(indcost,2), data = survey, subset =-c(10,16))
summary(mlr.survey2)
summary(mlr.survey3)

```


```{r ex="ExerRegMod4.4.3", type="solution", tut=TRUE}
ri <- rstandard(mlr.survey2)
hii <- hatvalues(mlr.survey2)

par(mfrow=c(1, 2))
hist(ri, nclass=16, main="", xlab="Standardized Residuals")
hist(hii, nclass=16, main="", xlab="Leverages")
par(mfrow=c(1, 1))
hist(survey$indcost, nclass=16)
mlr.survey3 <- lm(logcost ~ logsize + poly(indcost,2), data =  survey, subset =-c(10,16))
summary(mlr.survey2)
summary(mlr.survey3)

```

```{r ex="ExerRegMod4.4.3", type="sct", tut=TRUE}
ex() %>% check_object("ri", undefined_msg="Please extract the standardized residuals from `mlr.survey1` into `ri`.") %>% check_equal(incorrect_msg="You can extract the standardized residuals from a linear model by using `rstandard()`.")
ex() %>% check_object("hii", undefined_msg="Please extract the leverage values from `mlr.survey1` into `hii`.") %>% check_equal(incorrect_msg="You can extract the leverage values from a linear model by using `hatvalues()`.")
ex() %>% check_function("par",index=1,not_called_msg="use the `par` command to alter your plot window. ") %>% check_arg(., "mfrow") %>% check_equal(incorrect_msg="to create, 2 side-by-side graphs, set mfrow to be c(1,2). ")
ex() %>% check_function("hist",index=1, not_called_msg="use `hist` to create a histogram of the standardized residuals. ") %>% {
  check_arg(., "x") %>% check_equal(incorrect_msg="Please create a histogram using the standardized residuals. ")
  check_arg(., "nclass") %>% check_equal(incorrect_msg="Please set the number of classes to be 16. ")
}
ex() %>% check_function("hist",index=2, not_called_msg="Use `hist` to create a histogram of the leverage values.") %>% {
  check_arg(., "x") %>% check_equal(incorrect_msg="Please create a histogram using the leverage values. ")
  check_arg(., "nclass") %>% check_equal(incorrect_msg="Please set the number of classes to be 16. ")
}
ex() %>% check_function("par",index=2, not_called_msg="Please reset the plot to contain a single plot. ") %>% check_arg(., "mfrow") %>% check_equal(incorrect_msg="To change back to a single plot, set mfrow to be c(1,1). ")
ex() %>% check_function("hist",index=3, not_called_msg="Use `hist` to create a histogram of `indcost`.") %>% {
  check_arg(., "x") %>% check_equal(incorrect_msg="Please create a histogram of `indcost`.")
  check_arg(., "nclass") %>% check_equal(incorrect_msg="Please set the number of classes to be 16. ")
}
ex() %>% check_object("mlr.survey3",undefined_msg="Please create a linear model where we excluded observations 10 and 16 named `mlr.survey3`.") %>% check_equal(incorrect_msg="Model `logcost` as a function of `logsize` and `indcost` squared. You can exclude the two values by using the subset argument in `lm`.")
ex() %>% check_function("summary",index=1,not_called_msg="Please use `summary` to view a summary of `mlr.survey2`.") %>% check_arg(., "object") %>% check_equal(incorrect_msg="Make sure to specify that we would like a summary of `mlr.survey2`.")
ex() %>% check_function("summary",index=2,not_called_msg="Please use `summary` to view a summary of `mlr.survey3`.") %>% check_arg(., "object") %>% check_equal(incorrect_msg="Make sure to specify that we would like a summary of `mlr.survey3`.")
success_msg("Excellent! You will have noted that after removing these two influential observations from a high risk industry, the variable associated with the `indcost` squared became less statistically significant. This illustrates a general phenomena; sometimes, the 'signicance' of a variable may actually due to a few unusual observations, not the entire variable.")

```

## Collinearity

***

In this section, you learn how to:

- Define collinearity and describe its potential impact on regression inference
- Define a variance inflation factor and describe its effect on a regression coefficients standard error
- Describe rules of thumb for assessing collinearity and options for model reformulation in the presence of severe collinearity
- Compare and contrast effects of leverage and collinearity

***

###Video 

<center>
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/sp/166090200/embedIframeJs/uiconf_id/25916071/partner_id/1660902?iframeembed=true&playerId=kaltura_player&entry_id=1_30hrfg77&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;&wid=1_khmh83je" width="649" height="401" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" frameborder="0" title="Kaltura Player"></iframe>
</center>

#### Video Overhead Details {-}

<div class="tab">
  <button class="tablinks" onclick="openTab(event, 'Over4.5A')">A Details. Collinearity</button>
  <button class="tablinks" onclick="openTab(event, 'Over4.5B')">B Details. Quantifying collinearity</button>
  <button class="tablinks" onclick="openTab(event, 'Over4.5C')">C Details. Options for handling collinearity</button>
</div>


<div id="Over4.5A" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <h3>A Details. Collinearity</h3>
  <p>

- *Collinearity*, or *multicollinearity*, occurs when one explanatory variable is, or nearly is, a linear combination of the other explanatory variables.
    - Useful to think of the explanatory variables as being highly correlated with one another.
- Collinearity neither precludes us from getting good fits nor from making predictions of new observations.
    - Estimates of error variances and, therefore, tests of model adequacy, are
still reliable.
- In cases of serious collinearity, standard errors of individual
regression coefficients can be large.
    - With large standard errors, individual regression coefficients may not be meaningful.
    - Because a large standard error means that the corresponding
*t*-ratio is small, it is difficult to detect the importance of a variable.

</p>
</div> 

<div id="Over4.5B" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <h3>B Details. Quantifying collinearity</h3>
  <p>

A common way to quantify collinearity is through the *variance inflation factor (VIF)*.

- Suppose that the set of explanatory variables is labeled $x_{1},x_{2},\dots,x_{k}$.
- Run the regression using $x_{j}$ as the "outcome" and the other $x$'s  as the explanatory variables.
- Denote the coefficient of determination from this regression by $R_j^2$.
- Define the variance inflation factor

$$
VIF_{j}=\frac{1}{1-R_{j}^{2}},\ \ \ \text{ for } j = 1,2,\ldots, k.
$$

</p>
</div> 


<div id="Over4.5C" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <h3>C Details. Options for handling collinearity</h3>
  <p>

- Rule of thumb: When $VIF_{j}$ exceeds 10 (which is equivalent to $R_{j}^{2}>90\%$), we say that severe collinearity exists. This may signal is a need for action.
- Recode the variables by "centering" - that is, subtract the mean and divide by the standard deviation.
- Ignore the collinearity in the analysis but comment on it in the interpretation. Probably the most common approach.
- Replace one or more variables by auxiliary variables or  transformed versions.
- Remove one or more variables.  Easy.  Which One? is hard.
    - Use interpretation.  Which variable(s) do you feel most comfortable with?
    - Use automatic variable selection procedures to suggest a model.

</p>
</div>

###Exercise. Collinearity and term life

**Assignment Text**
We have seen that adding an explanatory variable $x^2$ to a model is sometimes helpful even though it is perfectly related to $x$ (such as through the function $f(x)=x^2$). But, for some data sets, higher order polynomials and interactions can be approximately linearly related (depending on the range of the data). 

This exercise returns to our term life data set `Term1` (preloaded) and demonstrates that collinearity can be severe when introducing interaction terms.

**Instructions**

- Fit a MLR model of `logface` on explantory variables `education`, `numhh` and `logincome`
- Use the function [vif()](https://www.rdocumentation.org/packages/car/versions/3.0-0/topics/vif) from the `car` package (preloaded) to calculate variance inflation factors.
- Fit and summarize a MLR model of `logface` on explantory variables `education` , `numhh` and `logincome` with an interaction between `numhh` and `logincome`, then extract variance inflation factors.

**Hint.** If the `car` package is not available to you, then you could calculate vifs using the [lm()] function, treating each variable separately. For example

`1/(1-summary(lm(education ~ numhh + logincome, data = Term1))$r.squared)`

gives the `education` vif.

```{r ex="ExerRegMod4.5.2", type="hint", tut=TRUE}
If the `car` package is not available to you, then you could calculate vifs using the [lm()] function, treating each variable separately. For example

`1/(1-summary(lm(education ~ numhh + logincome, data = Term1))$r.squared)`

gives the `education` vif.
```

```{r ex="ExerRegMod4.5.2", type="pre-exercise-code", tut=TRUE}
#Term <- read.csv("CSVData\\term_life.csv", header = TRUE)
Term <- read.csv("https://assets.datacamp.com/production/repositories/2610/datasets/efc64bc2d78cf6b48ad2c3f5e31800cb773de261/term_life.csv", header = TRUE)
Term1 <- subset(Term, subset = face > 0)
#str(Term1)
```

```{r ex="ExerRegMod4.5.2", type="sample-code", tut=TRUE}
# Fit a MLR model of `logface` on explantory variables `education`, `numhh` and `logincome`
Term_mlr <- lm(logface ~ education + numhh + logincome, data = Term1)

# Calculate the variance inflation factors.
car::vif(Term_mlr)

# Fit and summarize a MLR model of `logface` on explantory variables `education` , `numhh` and `logincome` with an interaction between `numhh` and `logincome`, then extract variance inflation  factors.
Term_mlr1 <- lm(logface ~ education + numhh*logincome , data = Term1)
summary(Term_mlr1)
car::vif(Term_mlr1)

```


```{r ex="ExerRegMod4.5.2", type="solution", tut=TRUE}
Term_mlr <- lm(logface ~ education + numhh + logincome, data = Term1)
car::vif(Term_mlr)
Term_mlr1 <- lm(logface ~ education + numhh*logincome , data = Term1)
summary(Term_mlr1)
car::vif(Term_mlr1)

```

```{r ex="ExerRegMod4.5.2", type="sct", tut=TRUE}
ex() %>% check_object("Term_mlr", undefined_msg="Please create a linear model named `Term_mlr`.") %>% check_equal(incorrect_msg="Utilize `lm` to create a linear model that models `logface` using `education`, `numhh`, and `logincome` from the dataset `Term1`.")
ex() %>% check_function("vif",index=1, not_called_msg="Utilize `vif` to calculate the variance inflation factors. ") %>% check_arg(., "mod") %>% check_equal(incorrect_msg="Using `vif` on our model will allow us to get the variance inflation factors. ")
ex() %>% check_object("Term_mlr1", undefined_msg="Please create a linear model named `Term_mlr1`.") %>% check_equal(incorrect_msg="Using `lm`, create a linear model that estimates `logincome` using `education`, as well as the product of `numhh` and `logincome`.")
ex() %>% check_function("summary", not_called_msg="Utilize `summary` to view a summary of `Term_mlr1`.") %>% check_arg(., "object") %>% check_equal(incorrect_msg="Make sure to specify that we would like a summary of `Term_mlr1`.")
ex() %>% check_function("vif",index=2, not_called_msg="Utilize `vif` to calculate the variance inflation factors for `Term_mlr`.") %>% check_arg(., "mod") %>% check_equal(incorrect_msg="Make sure to specify that we would like the variance inflation factors of `Term_mlr1`.")
success_msg("Excellent! This exercise underscores that colinearity among explanatory variables can be induced when introducing higher order terms such as interactions. Note that in the interaction model the variable 'numhh' does not appear to be statistically signficant effect. This is one of the big dangers of collinearity - it can mask important effects.")

```

## Selection criteria

***

In this section, you learn how to:

- Summarize a regression fit using alternative goodness of fit measures
- Validate a model using in-sample and out-of-sample data to mitigate issues of data-snooping
- Compare and contrast *SSPE* and *PRESS* statistics for model validation

***

###Video 

<center>
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/sp/166090200/embedIframeJs/uiconf_id/25916071/partner_id/1660902?iframeembed=true&playerId=kaltura_player&entry_id=0_i5dlmsna&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;&wid=0_kvkokgm2" width="649" height="401" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" frameborder="0" title="Kaltura Player"></iframe>
</center>

#### Video Overhead Details {-}

<div class="tab">
  <button class="tablinks" onclick="openTab(event, 'Over4.6A')">A Details. Goodness of fit</button>
  <button class="tablinks" onclick="openTab(event, 'Over4.6B')">B Details. Goodness of fit and information criteria</button>
  <button class="tablinks" onclick="openTab(event, 'Over4.6C')">C Details. Out of sample validation</button>
  <button class="tablinks" onclick="openTab(event, 'Over4.6D')">D Details. Out of sample validation procedure</button>
  <button class="tablinks" onclick="openTab(event, 'Over4.6E')">E Details. Cross - validation</button>
</div>


<div id="Over4.6A" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <h3>A Details. Goodness of fit</h3>
  <p>

- Criteria that measure the proximity of the fitted model and realized data are known as
*goodness of fit* statistics.
- Basic examples include:
    - the coefficient of determination $(R^{2})$, 
    - an adjusted version $(R_{a}^{2})$, 
    - the size of the typical error $(s)$, and 
    - $t$-ratios for each regression coefficient.

</p>    
</div> 

<div id="Over4.6B" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <h3>A Details. Goodness of fit</h3>
  <p>

A general measure is *Akaike's Information Criterion*, defined as

$$
AIC = -2 \times (fitted~log~likelihood) + 2 \times
(number~of~parameters)
$$

- For model comparison, the smaller the $AIC,$ the better is the fit.
- This measures balances the fit (in the first part) with a penalty for complexity (in the second part)
- It is a general measure - for linear regression, it reduces to

$$
AIC = n \ln (s^2) + n \ln (2 \pi) +n +k + 3 .
$$

- So, selecting a model to minimize $s$ or $s^2$ is equivalent to model selection based on minimizing $AIC$ (same *k*).

</p>
</div> 


<div id="Over4.6C" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <h3>C Details. Out of sample validation</h3>
  <p>
  
- When you choose a model to minimize $s$ or $AIC$, it is based on how well the model fits the data at hand, or the *model development*, or *training*, data
- As we have seen, this approach is susceptible to overfitting.
- A better approach is to validate the model on a *model validation*, or *test* data set, held out for this purpose.

</p>
</div> 

<div id="Over4.6D" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <h3>D Details. Out of sample validation procedure</h3>
  <p>

- (i) Using the model development subsample, fit a candidate model.
- (ii) Using the  Step (ii) model and the explanatory variables from the validation subsample, "predict" the dependent variables in the validation subsample, $\hat{y}_i$, where $i=n_{1}+1,...,n_{1}+n_{2}$.
- (iii) Calc the *sum of absolute prediction errors**

$$SAPE=\sum_{i=n_{1}+1}^{n_{1}+n_{2}} |y_{i}-\hat{y}_{i}| . $$

Repeat Steps (i) through (iii) for each candidate model. Choose the model with the smallest *SAPE*.

</p>
</div> 

<div id="Over4.6E" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <h3>E Details. Cross - validation</h3>
  <p>

- With out-of-sample validation, the statistic depends on a random split between in-sample and out-of-sample data (a problem for data sets that are not large)
- Alternatively, one may use *cross-validation*
    - Use a random mechanism to split the data into *k* subsets, (e.g., 5-10) 
    - Use the first *k-1* subsamples to estimate model parameters. Then, "predict" the outcomes for the *k*th subsample and use  *SAE* to summarize the fit
    - Repeat this by holding out each of the *k* sub-samples, summarizing with a cumulative *SAE*.
-  Repeat these steps for several candidate models. 
    - Choose the model with the lowest cumulative *SAE* statistic.

</p>
</div> 

###Exercise. Cross-validation and term life


**Assignment Text**

Here is some sample code to give you a better feel for cross-validation.

The first part of the randomly re-orders ("shuffles") the data. It also identifies explanatory variables `explvars`.

The function starts by pulling out only the needed data into `cvdata`. Then, for each subsample, a model is fit based on all the data except for the subsample, in `train_mlr` with the subsample in `test`. This is repeated for each subsample, then results are summarized.

<h6 style="text-align: center;"><a id="displayExer4.6" href="javascript:togglecode('toggleExer4.6','displayExer4.6');"><i><strong>Show Code</strong></i></a> </h6>
<div id="toggleExer4.6" style="display: none">

```
# Randomly re-order data - "shuffle it"
n <- nrow(Term1)
set.seed(12347)
shuffled_Term1 <- Term1[sample(n), ]
explvars <- c("education", "numhh", "logincome")

## Cross - Validation
crossvalfct <- function(explvars){
  cvdata   <- shuffled_Term1[, c("logface", explvars)]
  crossval <- 0
  k <- 5
  for (i in 1:k) {
    indices <- (((i-1) * round((1/k)*nrow(cvdata))) + 1):((i*round((1/k) * nrow(cvdata))))
    # Exclude them from the train set
    train_mlr <- lm(logface ~ ., data = cvdata[-indices,])
    # Include them in the test set
    test  <- data.frame(cvdata[indices, explvars])
    names(test)  <- explvars
    predict_test <- exp(predict(train_mlr, test))
    # Compare predicted to held-out and summarize
    predict_err  <- exp(cvdata[indices, "logface"]) - predict_test
    crossval <- crossval + sum(abs(predict_err))
  }
  crossval/1000
}

crossvalfct(explvars)
```

</div>

**Instructions**

- Calculate the cross-validation statistic using only logarithmic income, `logincome`.
- Calculate the cross-validation statistic using `logincome`, `education` and `numhh`.
- Calculate the cross-validation statistic using `logincome`, `education`, `numhh` and `marstat`.

The best model has the lowest cross-validation statistic.

```{r ex="ExerRegMod4.6.2", type="hint", tut=TRUE}
The function [sample()] is for taking random samples. We use it without replacement so it results in a re-ordering of data.
```

```{r ex="ExerRegMod4.6.2", type="pre-exercise-code", tut=TRUE}
#Term <- read.csv("CSVData\\term_life.csv", header = TRUE)
Term <- read.csv("https://assets.datacamp.com/production/repositories/2610/datasets/efc64bc2d78cf6b48ad2c3f5e31800cb773de261/term_life.csv", header = TRUE)
Term1 <- subset(Term, subset = face > 0)
Term1$marstat <- as.factor(Term1$marstat)

crossvalfct <- function(explvars){
  cvdata   <- shuffled_Term1[, c("logface", explvars)]
  crossval <- 0
  k <- 5
  for (i in 1:k) {
    indices <- (((i-1) * round((1/k)*nrow(cvdata))) + 1):((i*round((1/k) * nrow(cvdata))))
    # Exclude them from the train set
    train_mlr <- lm(logface ~ ., data = cvdata[-indices,])
    # Include them in the test set
    test  <- data.frame(cvdata[indices, explvars])
    names(test)  <- explvars
    predict_test <- exp(predict(train_mlr, test))
    # Compare predicted to held-out and summarize
    predict_err  <- exp(cvdata[indices, "logface"]) - predict_test
    crossval <- crossval + sum(abs(predict_err))
  }
  crossval/1000000
}
```


```{r ex="ExerRegMod4.6.2", type="sample-code", tut=TRUE}
# Randomly re-order data - "shuffle it"
n <- nrow(Term1)
set.seed(12347)
shuffled_Term1 <- Term1[sample(n), ]

# Calculate the cross-validation statistic using only logarithmic income, `logincome`.
explvars.1 <- c("logincome")
crossvalfct(explvars)

# Calculate the cross-validation statistic using `logincome`, `education` and `numhh`.
explvars.2 <- c("education", "numhh", "logincome")
crossvalfct(___)

# Calculate the cross-validation statistic using `logincome`, `education`, `numhh` and `marstat`.
explvars.3 <- c(___)
crossvalfct(explvars)
```


```{r ex="ExerRegMod4.6.2", type="solution", tut=TRUE}
# Randomly re-order data - "shuffle it"
n <- nrow(Term1)
set.seed(12347)
shuffled_Term1 <- Term1[sample(n), ]
# Cross - Validation
explvars.1 <- c("logincome")
crossvalfct(explvars.1)
explvars.2 <- c("education", "numhh", "logincome")
crossvalfct(explvars.2)
explvars.3 <- c("education", "numhh", "logincome", "marstat")
crossvalfct(explvars.3)
```


```{r ex="ExerRegMod4.6.2", type="sct", tut=TRUE}
ex() %>% check_object("explvars.1",undefined_msg="Create an object named `explvars.1` that is equal to `logincome`.") %>% check_equal(incorrect_msg="Make sure to set `explvars.1` to be the character string `logincome` as opposed to the values in `logincome`.")
ex() %>% check_function("crossvalfct",index=1,not_called_msg="Utilize the custom build function `crossvalfct` to calculate the cross-validation statistic for `explvars.1`.") %>% check_arg(., "explvars") %>% check_equal(incorrect_msg="The only argument that you need to set in `crossvalfct` is `explvars` which should be set equal to `explvars.1`.")
ex() %>% check_object("explvars.2",undefined_msg="Create an object named `explvars.2` that is equal to `logincome`, `numhh`, and `logincome`.") %>% check_equal(incorrect_msg="Make sure to set `explvars.2` to be the character strings `logincome`, `numhh`, and `logincome` as opposed to the values inside of those vectors. ")
ex() %>% check_function("crossvalfct",index=2,not_called_msg="Utilize the custom build function `crossvalfct` to calculate the cross-validation statistic for `explvars.2`.") %>% check_arg(., "explvars") %>% check_equal(incorrect_msg="The only argument that you need to set in `crossvalfct` is `explvars` which should be set equal to `explvars.2`.")
ex() %>% check_object("explvars.3",undefined_msg="Create an object named `explvars.3` that is equal to `logincome`, `numhh`, `logincome`, and `marstat`.") %>% check_equal(incorrect_msg="Make sure to set `explvars.3` to be the character strings `logincome`, `numhh`, `logincome`, and `marstat` as opposed to the values inside of those vectors. ")
ex() %>% check_function("crossvalfct",index=3,not_called_msg="Utilize the custom build function `crossvalfct` to calculate the cross-validation statistic for `explvars.3`.") %>% check_arg(., "explvars") %>% check_equal(incorrect_msg="The only argument that you need to set in `crossvalfct` is `explvars` which should be set equal to `explvars.3`.")
success_msg("Excellent! This exercises demonstrates the use of cross-validation, a very important technique in model selection. The exercise builds the procedure from the ground up so that you can see all the steps involved. Further, it illustrates how you can develop your own functions to automate procedures and save steps.")
```


