[
["index.html", "Online Tutorial on Regression Modeling with Actuarial and Financial Applications Preface", " Online Tutorial on Regression Modeling with Actuarial and Financial Applications Edward W. (Jed) Frees, University of Wisconsin-Madison Preface Date: 18 November 2018 About Regression Modeling Statistical techniques can be used to address new situations. This is important in a rapidly evolving risk management world. Analysts with a strong analytical background understand that a large data set can represent a treasure trove of information to be mined and can yield a strong competitive advantage. This book and online tutorial provides budding analysts with a foundation in multiple reression. Viewers will learn about these statistical techniques using data on the demand for insurance, healthcare expenditures, and other applications. Although no specific knowledge of actuarial or risk management is presumed, the approach introduces applications in which statistical techniques can be used to analyze real data of interest. Resources This tutorial is based on the book Regression Modeling with Actuarial and Financial Applications. For resources associated with the book, please visit the Regression Modeling book web site. For advanced regression applications in insurance, you may be interested in the series, Predictive Modeling Applications in Actuarial Science. Sample code and data for the series are available at series website. An earlier version of this tutorial, a Short Course constructed for Indonesian actuaries, uses the Datacamp learning platform. Tutorial Description This online tutorial is designed to guide you through the foundations of regession with applications in actuarial science. Anticipated completion time is approximately six hours. The tutorial assumes that you are familiar with the foundations in the statistical software R, such as Datacamp’s Introduction to R. General Layout. There are five chapters in this tutorial that summarize the foundations of multiple linear regression. Each chapter is subdivided into several sections. At the beginning of each section is a short video, typically 4-8 minutes, that summarizes the section key learning outcomes. Following the video, you can see more details about the underlying R code for the analysis presented in the video. Role of Exercises. Following each video, there are one or two exercises that allow you to practice skills to make sure that you fully grasp the learning outcomes. The exercises are implented using an online learning platfor provided by Datacamp so that you need not install R. Feedback is programmed into the exercises so that you will learn a lot by making mistakes! You will be pacing yourself, so always feel free to reveal the answers by hitting the Solution tab. Remember, going through quickly is not equivalent to learning deeply. Use this tool to enhance your understanding of one of the foundations of data science, regression analysis. Welcome to the Tutorial Video In this video, you learn how to: Describe regression briefly, i.e., in a nutshell Explain Galton’s height example as a regression application Video Overhead A. Galton’s 1885 Regression Data B. Supporting R Code Hide A. Galton’s 1885 Regression Data \\[ \\small{\\begin{array}{l|ccccccccccc|c} \\hline \\text{Height of }&amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\\\ \\text{adult child }&amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\\\ \\text{in inches }&amp; &lt;64.0 &amp; 64.5 &amp; 65.5 &amp; 66.5 &amp; 67.5 &amp; 68.5 &amp; 69.5 &amp; 70.5 &amp; 71.5 &amp; 72.5 &amp; &gt;73.0 &amp; \\text{Totals} \\\\ \\hline &gt;73.7 &amp; - &amp; - &amp; - &amp; - &amp; - &amp; - &amp; 5 &amp; 3 &amp; 2 &amp; 4 &amp; - &amp; 14 \\\\ 73.2 &amp; - &amp; - &amp; - &amp; - &amp; - &amp; 3 &amp; 4 &amp; 3 &amp; 2 &amp; 2 &amp; 3 &amp; 17 \\\\ 72.2 &amp; - &amp; - &amp; 1 &amp; - &amp; 4 &amp; 4 &amp; 11 &amp; 4 &amp; 9 &amp; 7 &amp; 1 &amp; 41 \\\\ 71.2 &amp; - &amp; - &amp; 2 &amp; - &amp; 11 &amp; 18 &amp; 20 &amp; 7 &amp; 4 &amp; 2 &amp; - &amp; 64 \\\\ 70.2 &amp; - &amp; - &amp; 5 &amp; 4 &amp; 19 &amp; 21 &amp; 25 &amp; 14 &amp; 10 &amp; 1 &amp; - &amp; 99 \\\\ 69.2 &amp; 1 &amp; 2 &amp; 7 &amp; 13 &amp; 38 &amp; 48 &amp; 33 &amp; 18 &amp; 5 &amp; 2 &amp; - &amp; 167 \\\\ 68.2 &amp; 1 &amp; - &amp; 7 &amp; 14 &amp; 28 &amp; 34 &amp; 20 &amp; 12 &amp; 3 &amp; 1 &amp; - &amp; 120 \\\\ 67.2 &amp; 2 &amp; 5 &amp; 11 &amp; 17 &amp; 38 &amp; 31 &amp; 27 &amp; 3 &amp; 4 &amp; - &amp; - &amp; 138 \\\\ 66.2 &amp; 2 &amp; 5 &amp; 11 &amp; 17 &amp; 36 &amp; 25 &amp; 17 &amp; 1 &amp; 3 &amp; - &amp; - &amp; 117 \\\\ 65.2 &amp; 1 &amp; 1 &amp; 7 &amp; 2 &amp; 15 &amp; 16 &amp; 4 &amp; 1 &amp; 1 &amp; - &amp; - &amp; 48 \\\\ 64.2 &amp; 4 &amp; 4 &amp; 5 &amp; 5 &amp; 14 &amp; 11 &amp; 16 &amp; - &amp; - &amp; - &amp; - &amp; 59 \\\\ 63.2 &amp; 2 &amp; 4 &amp; 9 &amp; 3 &amp; 5 &amp; 7 &amp; 1 &amp; 1 &amp; - &amp; - &amp; - &amp; 32 \\\\ 62.2 &amp; - &amp; 1 &amp; - &amp; 3 &amp; 3 &amp; - &amp; - &amp; - &amp; - &amp; - &amp; - &amp; 7 \\\\ &lt;61.2 &amp; 1 &amp; 1 &amp; 1 &amp; - &amp; - &amp; 1 &amp; - &amp; 1 &amp; - &amp; - &amp; - &amp; 5 \\\\ \\hline \\text{Totals }&amp; 14 &amp; 23 &amp; 66 &amp; 78 &amp; 211 &amp; 219 &amp; 183 &amp; 68 &amp; 43 &amp; 19 &amp; 4 &amp; 928 \\\\ \\hline \\end{array}} \\] Hide B. Supporting R Code # Reformat Data Set #heights &lt;- read.csv(&quot;CSVData\\\\GaltonFamily.csv&quot;,header = TRUE) heights &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/c85ede6c205d22049e766bd08956b225c576255b/galton_height.csv&quot;, header = TRUE) str(heights) head(heights) heights$child_ht &lt;- heights$CHILDC heights$parent_ht &lt;- heights$PARENTC heights2 &lt;- heights[c(&quot;child_ht&quot;,&quot;parent_ht&quot;)] #heights &lt;- read.csv(&quot;CSVData\\\\galton_height.csv&quot;,header = TRUE) heights &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/c85ede6c205d22049e766bd08956b225c576255b/galton_height.csv&quot;, header = TRUE) plot(jitter(heights$parent_ht),jitter(heights$child_ht), ylim = c(60,80), xlim = c(60,80), ylab = &quot;height of child&quot;, xlab = &quot;height of parents&quot;) abline(lm(heights$child_ht~heights$parent_ht)) abline(0,1,col = &quot;red&quot;, lty=2) summary(lm(heights$child_ht~heights$parent_ht)) Call: lm(formula = heights$child_ht ~ heights$parent_ht) Residuals: Min 1Q Median 3Q Max -8.2577 -1.4280 0.1323 1.5720 5.7918 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 25.84856 2.69009 9.609 &lt;2e-16 *** heights$parent_ht 0.60992 0.03882 15.710 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 2.26 on 926 degrees of freedom Multiple R-squared: 0.2104, Adjusted R-squared: 0.2096 F-statistic: 246.8 on 1 and 926 DF, p-value: &lt; 2.2e-16 "],
["regression-and-the-normal-distribution.html", "Chapter 1 Regression and the Normal Distribution 1.1 Fitting a normal distribution 1.2 Visualizing distributions 1.3 Summarizing distributions 1.4 Transformations", " Chapter 1 Regression and the Normal Distribution Chapter description Regression analysis is a statistical method that is widely used in many fields of study, with actuarial science being no exception. This chapter introduces the role of the normal distribution in regression and the use of logarithmic transformations in specifying regression relationships. 1.1 Fitting a normal distribution In this section, you learn how to: Calculate and interpret two basic summary statistics Fit a data set to a normal curve Calculate probabilities under a standard normal curve 1.1.1 Video Video Overhead Details A Details. Description of the data B Details. Read and examine data structure C Details. Summary stats for parents’ height D. Fit a normal curve to parents’ height details E Details. Use the normal approximation to determine the probability of the height of tall parents Hide A Details. Description of the data To illustrate a data set that can be analyzed using regression methods, we consider some data included in Galton’s 1885 paper. These data include the heights of 928 adult children (child_ht), together with an index of their parents’ height (parent_ht). Here, all female heights were multiplied by 1.08, and the index was created by taking the average of the father’s height and rescaled mother’s height. Galton was aware that the parents’ and the adult child’s height could each be adequately approximated by a normal curve. In developing regression analysis, he provided a single model for the joint distribution of heights. heights &lt;- read.csv(&quot;CSVData\\\\galton_height.csv&quot;, header = TRUE) #heights &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/c85ede6c205d22049e766bd08956b225c576255b/galton_height.csv&quot;, header = TRUE) plot(jitter(heights$parent_ht),jitter(heights$child_ht), ylim = c(60,80), xlim = c(60,80), ylab = &quot;height of child&quot;, xlab = &quot;height of parents&quot;) abline(lm(heights$child_ht~heights$parent_ht)) abline(0,1,col = &quot;red&quot;) Hide B Details. Read and examine data structure The data has already been read into a dataset called heights. Examine the structure of the data with the function str() and use the head() command to looks at the first few records. heights &lt;- read.csv(&quot;CSVData\\\\galton_height.csv&quot;,header = TRUE) #heights &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/c85ede6c205d22049e766bd08956b225c576255b/galton_height.csv&quot;, header = TRUE) str(heights) head(heights) &#39;data.frame&#39;: 928 obs. of 2 variables: $ child_ht : num 72.2 73.2 73.2 73.2 68.2 ... $ parent_ht: num 74.5 74.5 74.5 74.5 73.5 73.5 73.5 73.5 73.5 73.5 ... child_ht parent_ht 1 72.2 74.5 2 73.2 74.5 3 73.2 74.5 4 73.2 74.5 5 68.2 73.5 6 69.2 73.5 Hide C Details. Summary stats for parents’ height Next, examine the distribution of the child’s height and then examine the distribution of the parents height. ht_par &lt;- heights$parent_ht hist(ht_par) mean(ht_par) sd(ht_par) [1] 69.26293 [1] 1.912274 Hide D. Fit a normal curve to parents’ height details (mparent &lt;- mean(ht_par)) (sdparent &lt;- sd(ht_par)) x &lt;- seq(60, 80,by = 0.1) hist(ht_par, freq = FALSE) lines(x, dnorm(x, mean = mparent, sd = sdparent), col = &quot;blue&quot;) [1] 69.26293 [1] 1.912274 Hide E Details. Use the normal approximation to determine the probability of the height of tall parents TallHeight &lt;- 72 pnorm(TallHeight, mean = mparent, sd = sdparent) pnorm(72, mean = mean(ht_par), sd = sd(ht_par)) (StdUnitsTallHeight &lt;- (TallHeight - mparent)/sdparent) pnorm(StdUnitsTallHeight, mean = 0, sd = 1) [1] 0.9238302 [1] 0.9238302 [1] 1.431317 [1] 0.9238302 1.1.2 Exercise. Fitting Galton’s height data Assignment Text The Galton data has already been read into a dataframe called heights. These data include the heights of 928 adult children child_ht, together with an index of their parents’ height parent_ht. The video explored the distribution of the parents’ height; in this assignment, we investigate the distribution of the heights of the adult children. Instructions Define the height of an adult child as a global variable Use the function mean() to calculate the mean and the function sd() to calculate the standard deviation Use the normal approximation and the function pnorm() determine the probability that an adult child’s height is less than 72 inches Hint. Remember that we can reference a variable, say var, from a data set such as heights, as heights$var. eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6IiNoZWlnaHRzIDwtIHJlYWQuY3N2KFwiQ1NWRGF0YVxcXFxnYWx0b25faGVpZ2h0LmNzdlwiLGhlYWRlciA9IFRSVUUpXG5oZWlnaHRzIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9hc3NldHMuZGF0YWNhbXAuY29tL3Byb2R1Y3Rpb24vcmVwb3NpdG9yaWVzLzI2MTAvZGF0YXNldHMvYzg1ZWRlNmMyMDVkMjIwNDllNzY2YmQwODk1NmIyMjVjNTc2MjU1Yi9nYWx0b25faGVpZ2h0LmNzdlwiLCBoZWFkZXIgPSBUUlVFKSIsInNhbXBsZSI6IiNEZWZpbmUgdGhlIGdsb2JhbCB2YXJpYWJsZVxuaHRfY2hpbGQgPC0gX19fXG5cbiNDYWxjdWxhdGUgdGhlIG1lYW4gaGVpZ2h0XG5tY2hpbGQgPC0gX19fXG5tY2hpbGRcblxuI0NhbGN1bGF0ZSB0aGUgc3RhbmRhcmQgZGV2aWF0aW9uIG9mIGhlaWdodHNcbnNkY2hpbGQgPC0gX19fXG5zZGNoaWxkXG5cbiNEZXRlcm1pbmUgdGhlIHByb2JhYmlsaXR5IHRoYXQgdGhlIGhlaWdodCBpcyBsZXNzIHRoYW4gNzJcbnByPV9fXyg3MiwgbWVhbj1tY2hpbGQsIHNkPXNkY2hpbGQpIiwic29sdXRpb24iOiIjIFNvbHV0aW9uXG5odF9jaGlsZCA8LSBoZWlnaHRzJGNoaWxkX2h0XG5tY2hpbGQgPC0gbWVhbihodF9jaGlsZClcbnNkY2hpbGQgPC0gc2QoaHRfY2hpbGQpXG5wcj1wbm9ybSg3MiwgbWVhbiA9IG1jaGlsZCwgc2QgPSBzZGNoaWxkKSIsInNjdCI6ImV4KCkgJT4lIGNoZWNrX29iamVjdChcImh0X2NoaWxkXCIsdW5kZWZpbmVkX21zZz1cIk1ha2Ugc3VyZSB5b3UgYXNzaWduIHRoZSBjaGlsZHJlbidzIGhpZ2h0IHRvIGh0X2NoaWxkXCIpICU+JSBjaGVja19lcXVhbChpbmNvcnJlY3RfbXNnPVwiUmVtZW1iZXIgdGhhdCBpbiBvcmRlciB0byBjYWxsIGEgc3BlY2lmaWMgY29sdW1uIGZyb20gYSBkYXRhZnJhbWUsIHVzZSB0aGUgJCBvcGVyYXRvclwiKVxuZXgoKSAlPiUgY2hlY2tfb2JqZWN0KFwibWNoaWxkXCIpICAlPiUgY2hlY2tfZXF1YWwoKVxuZXgoKSAlPiUgY2hlY2tfb2JqZWN0KFwic2RjaGlsZFwiKSAlPiUgY2hlY2tfZXF1YWwoKVxuZXgoKSAlPiUgY2hlY2tfb2JqZWN0KFwicHJcIikgJT4lIGNoZWNrX2VxdWFsKClcbnN1Y2Nlc3NfbXNnKFwiRXhjZWxsZW50ISBXaXRoIHRoaXMgcHJvY2VkdXJlLCB5b3UgY2FuIG5vdyBjYWxjdWxhdGUgcHJvYmFiaWxpdGllcyBmb3IgYW55IGRpc3RyaWJ1dGlvbiB1c2luZyBhIG5vcm1hbCBjdXJ2ZSBhcHByb3hpbWF0aW9uLlwiKSJ9 1.1.3 Exercise. Visualizing child’s height distribution Assignment Text As in the prior exercise, from the Galton dataset heights, the heights of 928 adult children have been used to create a global variable called ht_child. We also have basic summary statistics, the mean height mchild and the standard deviation of heights in sdchild. In this exercise, we explore the fit of the normal curve to this distribution. Instructions To visualize the distribution, use the function hist() to calculate the histogram. Use the freq = FALSE option to give a histogram with proportions instead of counts. Use the function seq() to determine a sequence that can be used for plotting. Then, with the function lines(), superimpose a normal curve on the histogram Determine the probability that a child’s height is greater than 72 inches Hint 1. Use the function dnorm() to calculate the normal density, similar to the cumulative probabilites that you calculated using pnorm() Hint 2. To calculate probabilities greater that an amount, simply use 1 minus the cumulative probability Pre-exercise code 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 1.2 Visualizing distributions In this section, you learn how to: Calculate and interpret distributions using histograms Calculate and interpret distributions using density plots 1.2.1 Video Video Overhead Details Details. Data description A Details. Bring in Data, Introduce Logarithmic Claims B Details. Show how to get a finer grid for histograms C Details. Introduce the density plot Hide Details. Data descriptions For our first look at an insurance data set, we consider data from Rempala and Derrig (2005). They considered claims arising from automobile bodily injury insurance coverages. These are amounts incurred for outpatient medical treatments that arise from automobile accidents, typically sprains, broken collarbones and the like. The data consists of a sample of 272 claims from Massachusetts that were closed in 2001 (by “closed,” we mean that the claim is settled and no additional liabilities can arise from the same accident). Rempala and Derrig were interested in developing procedures for handling mixtures of “typical” claims and others from providers who reported claims fraudulently. For this sample, we consider only those typical claims, ignoring the potentially fraudulent ones. # Reformat Data Set injury &lt;- read.csv(&quot;CSVData\\\\MassBodilyInjury.csv&quot;,header = TRUE) str(injury) head(injury) # PICK THE SUBSET OF THE DATA CORRESPONDING TO PROVIDER A injury2 &lt;- subset(injury, providerA ! = 0 ) injury2$claims &lt;- 1000*injury2$claims injury2$logclaims &lt;- log(injury2$claims) injury3 &lt;- injury2[c(&quot;claims&quot;,&quot;logclaims&quot;)] #write.csv(injury3,&quot;CSVData\\\\MassBI.csv&quot;,row.names = FALSE) Hide A Details. Bring in Data, Introduce Logarithmic Claims injury &lt;- read.csv(&quot;CSVData\\\\MassBI.csv&quot;,header = TRUE) # CHECK THE NAMES, DIMENSION IN THE FILE AND LIST THE FIRST 8 OBSERVATIONS ; str(injury) head(injury) attach(injury) claims &lt;- injury$claims par(mfrow = c(1, 2)) hist(claims) hist(logclaims) &#39;data.frame&#39;: 272 obs. of 2 variables: $ claims : int 45 47 70 75 77 92 117 117 140 145 ... $ logclaims: num 3.81 3.85 4.25 4.32 4.34 ... claims logclaims 1 45 3.806662 2 47 3.850148 3 70 4.248495 4 75 4.317488 5 77 4.343805 6 92 4.521789 Hide B Details. Show how to get a finer grid for histograms par(mfrow = c(1, 2)) hist(logclaims) hist(logclaims,breaks = 15) Hide C Details. Introduce the density plot par(mfrow = c(1, 2)) plot(density(logclaims)) hist(logclaims, breaks = 15,freq = FALSE) lines(density(logclaims)) 1.2.2 Exercise. Visualizing bodily injury claims with density plots Assignment Text In the prior video, you learned about the Massachusetts bodily injury dataset. This dataframe, injury, has been read in and the global variable claims has been created. This assignment reviews the hist() function for visualizing distributions and allows you to explore density plotting, a smoothed version of the histogram. Instructions Use the function log() to create the logarithmic version of the claims variable Calculate a histogram of logarithmic with 40 bins using an option in the hist() function, breaks =. Create a density plot of logarithmic claims using the functions plot() and density(). Repeat the density plot, this time using a more refined bandwidth equal to 0.03. Use an option in the density() function, bw =. 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 1.3 Summarizing distributions In this section, you learn how to: Calculate and interpret basic summary statistics Calculate and interpret distributions using boxplots Calculate and interpret distributions using qq plots 1.3.1 Video Video Overhead Details A Details. Summary statistics B Details. Boxplot C Details. QQ Plot Hide A Details. Summary statistics injury &lt;- read.csv(&quot;CSVData\\\\MassBI.csv&quot;,header = TRUE) #injury &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/8cca19d0503fcf6e9d30d9cb912de5ba95ecb9c1/MassBI.csv&quot;, header = TRUE) attach(injury) # SUMMARY STATISTICS summary(injury) sd(claims);sd(logclaims) length(claims) claims logclaims Min. : 45.0 Min. : 3.807 1st Qu.: 892.5 1st Qu.: 6.794 Median : 2210.0 Median : 7.701 Mean : 2697.7 Mean : 7.388 3rd Qu.: 3215.0 3rd Qu.: 8.076 Max. :50000.0 Max. :10.820 [1] 3944.445 [1] 1.10093 [1] 272 Hide B Details. Boxplot # BASIC BOXPLOT boxplot(logclaims) quantile(logclaims, probs = 0.75) # BOXPLOT WITH ANNOTATION boxplot(logclaims, main = &quot;Boxplot of logclaims&quot;) text(1, 7.6, &quot;median&quot;, cex = 0.7) text(1, 6.55, &quot;25th percentile&quot;, cex = 0.7) text(1, 7.95, &quot;75th percentile&quot;, cex = 0.7) arrows(1.05, 4.9, 1.05, 3.6, col = &quot;blue&quot;, code = 3, angle = 20, length = 0.1) text(1.1, 4.4, &quot;outliers&quot;, cex = 0.7) text(1.1, 10.9, &quot;outlier&quot;, cex = 0.7) 75% 8.075579 Hide C Details. QQ Plot summary(injury) quantile(claims, probs = 0.75) quantile(logclaims, probs = 0.75) log(quantile(claims, probs = 0.75)) qnorm(p = 0.75, mean = mean(logclaims), sd = sd(logclaims)) (qnorm(p = 0.75, mean = mean(logclaims), sd = sd(logclaims)) -mean(logclaims)) / sd(logclaims) qnorm(p = 0.75, mean = 0, sd = 1) # QUANTILE - QUANTILE PLOT qqnorm(logclaims) qqline(logclaims) claims logclaims Min. : 45.0 Min. : 3.807 1st Qu.: 892.5 1st Qu.: 6.794 Median : 2210.0 Median : 7.701 Mean : 2697.7 Mean : 7.388 3rd Qu.: 3215.0 3rd Qu.: 8.076 Max. :50000.0 Max. :10.820 75% 3215 75% 8.075579 75% 8.075583 [1] 8.131056 [1] 0.6744898 [1] 0.6744898 1.3.2 Exercise. Summarizing bodily injury claims with box and qq plots Assignment Text The Massachusetts bodily injury data has already been read and used to create the global variable claims representing bodily injury claims. The previous video showed how to present the distribution of logarithmic claims which appeared to be approximately normally distributed. However, users are not really interested in log dollars but want to know about a unit of measurement that is more intuitive, such as dollars. So this assignment is based on claims, not the logarithmic version. You will use the functions boxplot() and qqnorm() to visualize the distribution through boxplots and quantile-quantile, or qq-, plots. But, because we are working with such a skewed distribution, do not be surprised that it is difficult to interpret these results readily. Instructions Produce a box plot for claims Determine the 25th empirical percentile for claims using the quantile() function. Determine the 25th percentile for claims based on a normal distribution using the qnorm() function. Produce a normal qq plot for claims using the function qqnorm(). The qqline() function is handy for producing a reference line. Hint. Note that qnorm() (one q) is for a normal quantile and qqnorm(). (two q’s!) is for the normal qq plot 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 1.3.3 Exercise. Effects on distributions of removing the largest claim Assignment Text The Massachusetts bodily injury dataframe injury has been read in; our focus is on the claims variable in that dataset. In the previous exercise, we learned that the Massachusetts bodily injury claims distribution was not even close to approximately normal (as evidenced by the box and qq- plots). Non-normality may be induced by skewness (that we will handle via transformations in the next section). But, seeming non-normality can also be induced by one or two very large observations (called an outlier later in the course). So, this exercise examines the effects on the distribution of removing the largest claims. Instructions Use the function tail() to examine the injury dataset and identify the largest claim Use the function subset() to create a subset omitting the largest claim Compare the summary statistics of the omitted claim distribution to the full distribution Compare the two distributions visually via histograms plotted next to another. par(mfrow = c(1, 2)) is used to organize the plots you create. Do not alter this code. Hint. For this data set, the [subset()] argument claims &lt; 25000 will keep all but the largest claim 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 1.4 Transformations In this exercise, you learn how to: Symmetrize a skewed distribution using a logarithmic transformation 1.4.1 Video Video Overhead Details A Details. Simulate a moderately skewed distribution, with transforms B Details. Visualize the distributions Hide A Details. Simulate a moderately skewed distribution, with transforms # FIGURE 1.7 - SIMULATE CHI-SQUARE, CREATE 3 TRANSFORMATIONS set.seed(1237) # set the seed of the random number generator # allows us to replicate results X1 &lt;- 10000*rchisq(500, df = 2) # generate variables randomly from a skewed distribution X2 &lt;- X1^(0.5) # square root transform, could also use sqrt(X1) X3 &lt;- log(X1) # logarithmic transform X4 &lt;- -1/X1 # negative reciprocal transform Hide B Details. Visualize the distributions par(mfrow = c(2, 2), cex = .75, mar = c(3,5,1.5,0)) hist(X1, freq = FALSE, nclass = 16, main = &quot;&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;, las = 1, yaxt = &quot;n&quot;,xlim = c(0,200000),ylim = c(0,.00005)) axis(2, at = seq(0,.00005,.00001),las = 1, cex = .3, labels = c(&quot;0&quot;, &quot;0.00001&quot;, &quot;0.00002&quot;,&quot;0.00003&quot;, &quot;0.00004&quot;, &quot;0.00005&quot;)) mtext(&quot;Density&quot;, side = 2, at = .000055, las = 1, cex = .75) mtext(&quot;y&quot;, side = 1, cex = .75, line = 2) par(mar = c(3,4,1.5,0.2)) hist(X2, freq = FALSE, nclass = 16, main = &quot;&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;, las = 1,xlim = c(0,400), ylim = c(0,.008)) mtext(&quot;Density&quot;, side = 2, at = .0088, las = 1, cex = .75) mtext(&quot;Square root of y&quot;, side = 1, cex = .75, line = 2) par(mar = c(3.2,5,1,0)) hist(X3, freq = FALSE, nclass = 16, main = &quot;&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;, las = 1, ylim = c(0,.4)) mtext(&quot;Density&quot;, side = 2, at = .44, las = 1, cex = .75) mtext(&quot;Logarithmic y&quot;, side = 1, cex = .75, line = 2) par(mar = c(3.2,4,1,0.2)) hist(X4, freq = FALSE, nclass = 16, main = &quot;&quot;,xlab = &quot;&quot;, ylab = &quot;&quot;, las = 1, ylim = c(0,100)) mtext(&quot;Density&quot;, side = 2, at = 110, las = 1, cex = .75) mtext(&quot;Negative reciprocal of y&quot;, side = 1, cex = .75, line = 2) 1.4.2 Exercise. Distribution of transformed bodily injury claims Assignment Text We have now examined the distributions of bodily injury claims and its logarithmic version. Grudgingly, we have concluded that to fit a normal curve the logarithmic version of claims is a better choice (again, we really do not like log dollars but you’ll get used to it in this course). But, why logarithmic and not some other transformations? A partial response to this question will appear in later chapters when we describe interpretation of regression coefficients. Another partial response is that the log transform seems to work well with skewed insurance data sets, as we demonstrate visually in this exercise. Instructions Use the code par(mfrow = c(2, 2)) so that four graphs appear in a 2 by 2 matrix format for easy comparisons. Plot the density() of claims square root of claims logarithmic claims negative reciprocal of claims Hint. For negative reciprocal claims, use plot(density(-claims^(-1))) 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 "],
["basic-linear-regression.html", "Chapter 2 Basic Linear Regression 2.1 Correlation 2.2 Method of least squares 2.3 Understanding variability 2.4 Statistical inference 2.5 Diagnostics", " Chapter 2 Basic Linear Regression Chapter description This chapter considers regression in the case of only one explanatory variable. Despite this seeming simplicity, many deep ideas of regression can be developed in this framework. By limiting ourselves to the one variable case, we can illustrate the relationships between two variables graphically. Graphical tools prove to be important for developing a link between the data and a predictive model. 2.1 Correlation In this section, you learn how to: Calculate and interpret a correlation coefficient Interpret correlation coefficients by visualizing scatter plots 2.1.1 Video Video Overhead Details A Details. Wisconsin lottery data description B Details. Summary statistics C Details. Visualizing skewed distributions D Details. Visualizing relationships with a scatter plot E Details. Correlation coefficient Hide A Details. Wisconsin lottery data description Lot &lt;- read.csv(&quot;CSVData\\\\Wisc_lottery.csv&quot;) #Lot &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/a792b30fb32b0896dd6894501cbab32b5d48df51/Wisc_lottery.csv&quot;, header = TRUE) str(Lot) &#39;data.frame&#39;: 50 obs. of 3 variables: $ pop : int 435 4823 2469 2051 13337 17004 38283 9859 4464 20958 ... $ sales : num 1285 3571 2407 1224 15046 ... $ medhome: num 71.3 98 58.7 65.7 96.7 66.4 91 61 91.5 68.8 ... Hide B Details. Summary statistics #options(scipen = 100, digits = 4) #numSummary(Lot[,c(&quot;pop&quot;, &quot;sales&quot;)], statistics = c(&quot;mean&quot;, &quot;sd&quot;, &quot;quantiles&quot;), quantiles = c(0,.5,1)) (as.data.frame(psych::describe(Lot)))[,c(2,3,4,5,8,9)] #Rcmdr::numSummary(Lot[,c(&quot;pop&quot;, &quot;sales&quot;)], statistics = c(&quot;mean&quot;, &quot;sd&quot;, &quot;quantiles&quot;), quantiles = c(0,.5,1)) n mean sd median min max pop 50 9311.040 11098.15695 4405.500 280.0 39098.0 sales 50 6494.829 8103.01250 2426.406 189.0 33181.4 medhome 50 57.092 18.37312 53.900 34.5 120.0 Hide C Details. Visualizing skewed distributions par(mfrow = c(1, 2)) hist(Lot$pop, main = &quot;&quot;, xlab = &quot;population&quot;) hist(Lot$sales, main = &quot;&quot;, xlab = &quot;sales&quot;) Hide D Details. Visualizing relationships with a scatter plot plot(Lot$pop, Lot$sales, xlab = &quot;population&quot;, ylab = &quot;sales&quot;) Hide E Details. Correlation coefficient cor(Lot$pop, Lot$sales) [1] 0.8862827 2.1.2 Exercise. Correlations and the Wisconsin lottery Assignment Text The Wisconsin lottery dataset, Wisc_lottery,has already been read into a dataframe Lot. Like insurance, lotteries are uncertain events and so the skills to work with and interpret lottery data are readily applicable to insurance. It is common to report sales and population in thousands of units, so this exercise gives you practice in rescaling data via linear transformations. Instructions From the available population and sales variables, create new variables in the dataframe Lot, pop_1000 and sales_1000 that are in thousands (of people and of dollars, respectively). Create summary statistics for the dataframe that includes these new variables. Plot pop_1000 versus sales_1000. Calculate the correlation between pop_1000 versus sales_1000 using the function cor(). How does this differ between the correlation between population and sales in the original units? Hint. Use the dataframe to refer to pop and sales as Lot$pop and Lot$sales, respectively eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6IiNMb3QgPC0gcmVhZC5jc3YoXCJDU1ZEYXRhXFxcXFdpc2NfbG90dGVyeS5jc3ZcIiwgaGVhZGVyID0gVFJVRSlcbkxvdCA8LSByZWFkLmNzdihcImh0dHBzOi8vYXNzZXRzLmRhdGFjYW1wLmNvbS9wcm9kdWN0aW9uL3JlcG9zaXRvcmllcy8yNjEwL2RhdGFzZXRzL2E3OTJiMzBmYjMyYjA4OTZkZDY4OTQ1MDFjYmFiMzJiNWQ0OGRmNTEvV2lzY19sb3R0ZXJ5LmNzdlwiLCBoZWFkZXIgPSBUUlVFKSIsInNhbXBsZSI6IiMgQ3JlYXRlIG5ldyB2YXJpYWJsZXMsIHNheSwgYHBvcF8xMDAwYCBhbmQgYHNhbGVzXzEwMDBgXG5Mb3QkcG9wXzEwMDAgPC0gX19fXG5fX18gPC0gTG90JHNhbGVzLzEwMDBcblxuIyBDcmVhdGUgc3VtbWFyeSBzdGF0aXN0aWNzIGZvciB0aGUgZGF0YWZyYW1lXG5zdW1tYXJ5KF9fXylcblxuIyBQbG90IGBwb3BfMTAwMGAgdmVyc3VzIGBzYWxlc18xMDAwYC5cbnBsb3QoX19fLCBfX18pXG5cbiMgQ2FsY3VsYXRlIHRoZSBjb3JyZWxhdGlvbiBiZXR3ZWVuIGBwb3BfMTAwMGAgdmVyc3VzIGBzYWxlc18xMDAwYCBcbmNvcihfX18sIF9fXykiLCJzb2x1dGlvbiI6IkxvdCRwb3BfMTAwMCA8LSBMb3QkcG9wLzEwMDBcbkxvdCRzYWxlc18xMDAwIDwtIExvdCRzYWxlcy8xMDAwXG5zdW1tYXJ5KExvdClcbnBsb3QoTG90JHBvcF8xMDAwLCBMb3Qkc2FsZXNfMTAwMClcbmNvcihMb3QkcG9wXzEwMDAsIExvdCRzYWxlc18xMDAwKSIsInNjdCI6ImV4KCkgJT4lIGNoZWNrX29iamVjdChcIkxvdFwiKSAlPiUge1xuICBjaGVja19jb2x1bW4oLiwgXCJwb3BfMTAwMFwiKSAlPiUgY2hlY2tfZXF1YWwoKVxuICBjaGVja19jb2x1bW4oLiwgXCJzYWxlc18xMDAwXCIpICU+JSBjaGVja19lcXVhbCgpXG59XG5leCgpICU+JSBjaGVja19mdW5jdGlvbihcInN1bW1hcnlcIikgJT4lIGNoZWNrX3Jlc3VsdCgpICU+JSBjaGVja19lcXVhbCgpXG5leCgpICU+JSBjaGVja19mdW5jdGlvbihcInBsb3RcIikgJT4le1xuICBjaGVja19hcmcoLiwgXCJ4XCIpICU+JSBjaGVja19lcXVhbCgpXG4gIGNoZWNrX2FyZyguLCBcInlcIikgJT4lIGNoZWNrX2VxdWFsKClcbn1cbmV4KCkgJT4lIGNoZWNrX2Z1bmN0aW9uKFwiY29yXCIpICU+JSBjaGVja19yZXN1bHQoKSAlPiUgY2hlY2tfZXF1YWwoKVxuc3VjY2Vzc19tc2coXCJDb25ncmF0dWxhdGlvbnMhIFdlIHdpbGwgcmVzY2FsZSBkYXRhIHVzaW5nICdsaW5lYXInIHRyYW5zZm9ybWF0aW9ucyByZWd1bGFybHkuIEluIHBhcnQgd2UgZG8gdGhpcyBmb3IgY29tbXVuaWNhdGluZyBvdXIgYW5hbHlzaXMgdG8gb3RoZXJzLiBBbHNvIGluIHBhcnQsIHRoaXMgaXMgZm9yIG91ciBvd24gY29udmVuaWVuY2UgYXMgaXQgY2FuIGFsbG93IHVzIHRvIHNlZSBwYXR0ZXJucyBtb3JlIHJlYWRpbHkuXCIpIn0= 2.2 Method of least squares In this section, you learn how to: Fit a line to data using the method of least squares Predict an observation using a least squares fitted line 2.2.1 Video 2.2.1.1 Video Overheads A Details. Where to fit the line? B Details. Method of least squares C Details. Regression coefficients D Details. Prediction Hide A Details. Where to fit the line? model_blr &lt;- lm(sales ~ pop, data = Lot) plot(Lot$pop, Lot$sales,xlab = &quot;population&quot;, ylab = &quot;sales&quot;) abline(model_blr, col=&quot;blue&quot;) abline(0,1, col=&quot;red&quot;) Hide B Details. Method of least squares For observation \\(\\{(y, x)\\}\\), the height of the regression line is \\[b_0 + b_1 x.\\] Thus, \\(y - (b_0 + b_1 x)\\) represents the deviation. The sum of squared deviations is \\[SS(b_0, b_1) = \\sum (y - (b_0 + b_1 x))^2 .\\] The method of least squares – determine values of \\(b_0, b_1\\) that minimize \\(SS\\). Hide C Details. Regression coefficients model_blr &lt;- lm(sales ~ pop, data = Lot) round(coefficients(model_blr), digits=4) plot(Lot$pop, Lot$sales,xlab = &quot;population&quot;, ylab = &quot;sales&quot;) abline(model_blr, col=&quot;blue&quot;) (Intercept) pop 469.7036 0.6471 Hide D Details. Prediction round(coefficients(model_blr), digits=6) coefficients(model_blr)[1] + coefficients(model_blr)[2]*30000 newdata &lt;- data.frame(pop = 30000) predict(model_blr, newdata) (Intercept) pop 469.703598 0.647095 (Intercept) 19882.55 1 19882.55 2.2.2 Exercise. Least squares fit using housing prices Assignment Text The prior video analyzed the effect that a zip code’s population has on lottery sales. Instead of population, suppose that you wish to understand the effect that housing prices have on the sale of lottery tickets. The dataframe Lot, read in from the Wisconsin lottery dataset Wisc_lottery, contains the variable medhome which is the median house price for each zip code, in thousands of dollars. In this exercise, you will get a feel for the distribution of this variable by examining summary statistics, examine its relationship with sales graphically and via correlations, fit a basic linear regression model and use this model to predict sales. Instructions Summarize the dataframe Lot that contains medhome and sales. Plot medhome versus sales. Summarize this relationship by calculating the corresponding correlation coefficient using the function cor(). Using the function lm(), regress medhome, the explanatory variable, on sales, the outcome variable. Display the regression coefficients to four significant digits. Use the function predict() and the fitted regression model to predict sales assuming that the median house price for a zip code is 50 (in thousands of dollars). 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 2.3 Understanding variability In this section, you learn how to: Visualize the ANOVA decomposition of variability Calculate and interpret \\(R^2\\), the coefficient of determination Calculate and interpret \\(s^2\\) the mean square error Explain the components of the ANOVA table 2.3.1 Video Video Overhead Details A and B Details. Visualizing the uncertainty about a line C, D and E Details. ANOVA Table Hide A and B Details. Visualizing the uncertainty about a line par(mar=c(2.2,2.1,.2,.2),cex=1.2) x &lt;- seq(-4, 4, len=101) y &lt;- x plot(x, y, type = &quot;l&quot;, xlim=c(-3, 4), xaxt=&quot;n&quot;, yaxt=&quot;n&quot;, xlab=&quot;&quot;, ylab=&quot;&quot;) axis(1, at = c(-1, 1),lab = expression(bar(x), x)) axis(2, at = c(-1, 1, 3),lab = expression(bar(y), hat(y), y), las=1) abline(-1, 0, lty = 2) segments(-4, 1, 1, 1, lty=2) segments(-4, 3, 1, 3, lty = 2) segments(1, -4, 1, 3, lty = 2) segments(-1, -4, -1, -1, lty = 2) points(1, 3, cex=1.5, pch=19) arrows(1.0, 1, 1.0, 3, code = 3, lty = 1, angle=15, length=0.12, lwd=2) text(1.3, 2.2, expression( y-hat(y)),cex=0.8) text(-.3,2.2,&quot;&#39;unexplained&#39; deviation&quot;, cex=.8) arrows(1.0, -1, 1.0, 1, code = 3, lty = 1, angle=15, length=0.12, lwd=2) text(1.85, 0, expression(hat(y)-bar(y) == b[1](x-bar(x)) ), cex=0.8 ) text(2.1, -0.5, &quot; &#39;explained&#39; deviation&quot;, cex=0.8) arrows(-1, -1.0, 1, -1.0, code = 3, lty = 1, angle=15, length=0.12, lwd = 2) text(0, -1.3, expression( x-bar(x)), cex=0.8 ) text(3.5, 2.7, expression( hat(y)== b[0]+ b[1]*x), cex=0.8 ) Hide C, D and E Details. ANOVA Table model_blr &lt;- lm(sales ~ pop, data = Lot) anova(model_blr) sqrt(anova(model_blr)$Mean[2]) summary(model_blr)$r.squared Analysis of Variance Table Response: sales Df Sum Sq Mean Sq F value Pr(&gt;F) pop 1 2527165015 2527165015 175.77 &lt; 2.2e-16 *** Residuals 48 690116755 14377432 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 [1] 3791.758 [1] 0.7854969 2.3.2 Exercise. Summarizing measures of uncertainty Assignment Text In a previous exercise, you developed a regression line to fit the variable medhome, the median house price for each zip code, as a predictor of lottery sales. The regression of medhome on sales has been summarized in the R object model_blr. How reliable is the regression line? In this excercise, you will compute some of the standard measures that are used to summarize the goodness of this fit. Instructions Summarize the fitted regression model in an ANOVA table. Determine the size of the typical residual, \\(s\\). Determine the coefficient of determination, \\(R^2\\). Hint. Learn more about possibilities through the Rdocumentation site. If you have not done so already, check out the function anova() eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6IiNMb3QgPC0gcmVhZC5jc3YoXCJDU1ZEYXRhXFxcXFdpc2NfbG90dGVyeS5jc3ZcIiwgaGVhZGVyID0gVFJVRSlcbkxvdCA8LSByZWFkLmNzdihcImh0dHBzOi8vYXNzZXRzLmRhdGFjYW1wLmNvbS9wcm9kdWN0aW9uL3JlcG9zaXRvcmllcy8yNjEwL2RhdGFzZXRzL2E3OTJiMzBmYjMyYjA4OTZkZDY4OTQ1MDFjYmFiMzJiNWQ0OGRmNTEvV2lzY19sb3R0ZXJ5LmNzdlwiLCBoZWFkZXIgPSBUUlVFKSIsInNhbXBsZSI6Im1vZGVsX2JsciA8LSBsbShzYWxlcyAgfiBtZWRob21lLCBkYXRhID0gTG90KVxuXG4jIFN1bW1hcml6ZSB0aGUgZml0dGVkIHJlZ3Jlc3Npb24gbW9kZWwgaW4gYW4gQU5PVkEgdGFibGUuXG5hbm92YShfX18pXG5cbiMgRGV0ZXJtaW5lIHRoZSBzaXplIG9mIHRoZSB0eXBpY2FsIHJlc2lkdWFsLCAkcyQuXG5zcXJ0KGFub3ZhKF9fXykkTWVhblsyXSlcblxuIyBEZXRlcm1pbmUgdGhlIGNvZWZmaWNpZW50IG9mIGRldGVybWluYXRpb24sICRSXjIkLiBcbnN1bW1hcnkoX19fKSRyLnNxdWFyZWQiLCJzb2x1dGlvbiI6Im1vZGVsX2JsciA8LSBsbShzYWxlcyAgfiBtZWRob21lLCBkYXRhID0gTG90KVxuYW5vdmEobW9kZWxfYmxyKVxuc3FydChhbm92YShtb2RlbF9ibHIpJE1lYW5bMl0pXG5zdW1tYXJ5KG1vZGVsX2Jscikkci5zcXVhcmVkIiwic2N0IjoiZXgoKSAlPiUgY2hlY2tfb2JqZWN0KFwibW9kZWxfYmxyXCIpICU+JSBjaGVja19lcXVhbCgpXG5leCgpICU+JSBjaGVja19mdW5jdGlvbihcImxtXCIpICU+JSB7XG4gIGNoZWNrX2FyZyguLCBcImZvcm11bGFcIikgJT4lIGNoZWNrX2VxdWFsKClcbiAgY2hlY2tfYXJnKC4sIFwiZGF0YVwiKSAlPiUgY2hlY2tfZXF1YWwoKVxufVxuZXgoKSAlPiUgY2hlY2tfZnVuY3Rpb24oXCJhbm92YVwiKSAlPiUgY2hlY2tfYXJnKFwib2JqZWN0XCIpICU+JSBjaGVja19lcXVhbCgpXG5leCgpICU+JSBjaGVja19mdW5jdGlvbihcInNxcnRcIikgJT4lIGNoZWNrX3Jlc3VsdCgpICU+JSBjaGVja19lcXVhbCgpXG5leCgpICU+JSBjaGVja19mdW5jdGlvbihcInN1bW1hcnlcIikgJT4lIGNoZWNrX3Jlc3VsdCgpICU+JSBjaGVja19lcXVhbCgpXG5zdWNjZXNzX21zZyhcIkNvbmdyYXR1bGF0aW9ucyEgSXQgd2lsbCBiZSBoZWxwZnVsIGlmIHlvdSBjb21wYXJlIHRoZSByZXN1bHRzIG9mIHRoaXMgZXhlcmNpc2UgdG8gdGhlIHJlZ3Jlc3Npb24gb2YgYHBvcGAgb24gYHNhbGVzYCBmcm9tIHRoZSBwcmlvciB2aWRlby4gV2UgaGF2ZSBzZWVuIHRoYXQgYHBvcGAgaXMgbW9yZSBoaWdobHkgY29ycmVsYXRlZCB3aXRoIGBzYWxlc2AgdGhhbiBgbWVkaG9tZWAsIHNvIHdlIGFyZSBleHBlY3RpbmcgZ3JlYXRlciB1bmNlcnRhaW50eSBpbiB0aGlzIHJlZ3Jlc3Npb24gZml0LlwiKSJ9 2.3.3 Exercise. Effects of linear transforms on measures of uncertainty Assignment Text Let us see how rescaling, a linear transformation, affects our measures of uncertainty. As before, the Wisconsin lottery dataset Wisc_lottery has been read into a dataframe Lot that also contains sales_1000, sales in thousands of dollars, and pop_1000, zip code population in thousands. How do measures of uncertainty change when going from the original units to thousands of those units? Instructions Run a regression of pop on sales_1000 and summarize this in an ANOVA table. For this regression, determine the \\(s\\) and the coefficient of determination, \\(R^2\\). Run a regression of pop_1000 on sales_1000 and summarize this in an ANOVA table. For this regression, determine the \\(s\\) and the coefficient of determination, \\(R^2\\). Hint. The residual standard error is also available as summary(model_blr1)$sigma. The coefficient of determination is also available as summary(model_blr1)$r.squared. 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 2.4 Statistical inference In this section, you learn how to: Conduct a hypothesis test for a regression coefficient using either a rejection/acceptance procedure or a p-value Calculate and interpret a confidence interval for a regression coefficient Calculate and interpret a prediction interval at a specific value of a predictor variable 2.4.1 Video Video Overhead Details A Details. Summary of basic linear regression model B Details. Hypothesis testing C Details. Confidence intervals D Details. Confidence intervals check E Details. Prediction intervals Hide A Details. Summary of basic linear regression model Introduce the output in the summary of the basic linear regression model. Lot &lt;- read.csv(&quot;CSVData\\\\Wisc_lottery.csv&quot;, header = TRUE) #Lot &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/a792b30fb32b0896dd6894501cbab32b5d48df51/Wisc_lottery.csv&quot;, header = TRUE) #options(scipen = 8, digits = 4) model_blr &lt;- lm(sales ~ pop, data = Lot) summary(model_blr) Call: lm(formula = sales ~ pop, data = Lot) Residuals: Min 1Q Median 3Q Max -6046.7 -1460.9 -670.5 485.6 18229.5 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 469.70360 702.90619 0.668 0.507 pop 0.64709 0.04881 13.258 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 3792 on 48 degrees of freedom Multiple R-squared: 0.7855, Adjusted R-squared: 0.781 F-statistic: 175.8 on 1 and 48 DF, p-value: &lt; 2.2e-16 Hide B Details. Hypothesis testing &gt; summary(model_blr)$coefficients Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 469.7036 702.90619 0.6682 5.072e-01 pop 0.6471 0.04881 13.2579 1.158e-17 Hide C Details. Confidence intervals confint(model_blr, level = .90) confint(model_blr, level = .95) 5 % 95 % (Intercept) -709.2276710 1648.6348666 pop 0.5652327 0.7289569 2.5 % 97.5 % (Intercept) -943.5840183 1882.99121 pop 0.5489596 0.74523 Hide D Details. Confidence intervals check # Just for checking summary(model_blr)$coefficients[2,1] summary(model_blr)$coefficients[2,2] qt(.975, 48) summary(model_blr)$coefficients[2,1] - summary(model_blr)$coefficients[2,2]*qt(.975, 48) confint(model_blr, level = .95) confint(model_blr, level = .95) [1] 0.6470948 [1] 0.04880808 [1] 2.010635 [1] 0.5489596 2.5 % 97.5 % (Intercept) -943.5840183 1882.99121 pop 0.5489596 0.74523 2.5 % 97.5 % (Intercept) -943.5840183 1882.99121 pop 0.5489596 0.74523 Hide E Details. Prediction intervals NewData &lt;- data.frame(pop = 10000) predict(model_blr, NewData, interval = &quot;prediction&quot;, level = .90) predict(model_blr, NewData, interval = &quot;prediction&quot;, level = .99) fit lwr upr 1 6940.651 517.4933 13363.81 fit lwr upr 1 6940.651 -3331.214 17212.52 2.4.2 Exercise. Statistical inference and Wisconsin lottery Assignment Text In a previous exercise, you developed a regression line with the variable medhome, the median house price for each zip code, as a predictor of lottery sales. The regression of medhome on sales has been summarized in the R object model_blr. This exercise allows you to practice the standard inferential tasks: hypothesis testing, confidence intervals, and prediction. Instructions Summarize the regression model and identify the t-statistic for testing the importance of the regression coefficient associated with medhome. Use the function confint() to provide a 95% confidence interval for the regression coefficient associated with medhome. Consider a zip code with a median housing price equal to 50 (in thousands of dollars). Use the function predict() to provide a point prediction and a 95% prediction interval for sales. Hint. Taking a [summary()] of a regression object produces a new objeect. You can use the [str()] structure command to learn more about the new object. Try out a command such as str(summary(model_blr)) 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 2.5 Diagnostics In this section, you learn how to: Describe how diagnostic checking and residual analysis are used in a statistical analysis Describe several model misspecifications commonly encountered in a regression analysis 2.5.1 Video Video Overhead Details A Details. Unusual observations in regression B Details. Example. Outliers and High Leverage Points C Details. Regression fit with 19 base observations D Details. Regression fit with 19 base observations plus C E Details. R code F Details. Visualizing four regression fits G Details. Results from four regression models Hide A Details. Unusual observations in regression We have defined regression estimates as minimizers of a least squares objective function. An appealing intuitive feature of linear regressions is that regression estimates can be expressed as weighted averages of outcomes. The weights vary by observation, some observations are more important than others. “Unusual” observations are far from the majority of the data set: Unusual in the vertical direction is called an outlier. Unusual in the horizontal directional is called a high leverage point. Hide B Details. Example. Outliers and High Leverage Points outlr &lt;- read.csv(&quot;CSVData\\\\Outlier.csv&quot;, header = TRUE) # FIGURE 2.7 plot(outlr$x, outlr$y, xlim = c(0, 10), ylim = c(2, 9), xlab = &quot;x&quot;, ylab = &quot;y&quot;) text(4.5, 8.0, &quot;A&quot;) text(9.8, 8.0, &quot;B&quot;) text(9.8, 2.5, &quot;C&quot;) Hide C Details. Regression fit with 19 base observations model_outlr0 &lt;- lm(y ~ x, data = outlr, subset = -c(20,21,22)) summary(model_outlr0) plot(outlr$x[1:19], outlr$y[1:19], xlab = &quot;x&quot;, ylab = &quot;y&quot;, xlim = c(0, 10), ylim = c(2, 9)) abline(model_outlr0) Call: lm(formula = y ~ x, data = outlr, subset = -c(20, 21, 22)) Residuals: Min 1Q Median 3Q Max -0.4790 -0.2708 0.0711 0.2263 0.4094 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 1.86874 0.19583 9.543 3.06e-08 *** x 0.61094 0.05219 11.705 1.47e-09 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.2883 on 17 degrees of freedom Multiple R-squared: 0.8896, Adjusted R-squared: 0.8831 F-statistic: 137 on 1 and 17 DF, p-value: 1.471e-09 Hide D Details. Regression fit with 19 base observations plus C model_outlrC &lt;- lm(y ~ x, data = outlr, subset = -c(20,21)) summary(model_outlrC) plot(outlr$x[c(1:19,22)], outlr$y[c(1:19,22)], xlab = &quot;x&quot;, ylab = &quot;y&quot;, xlim = c(0, 10), ylim = c(2, 9)) text(9.8, 2.5, &quot;C&quot;, col = &quot;blue&quot;) abline(model_outlrC) Call: lm(formula = y ~ x, data = outlr, subset = -c(20, 21)) Residuals: Min 1Q Median 3Q Max -2.32947 -0.57819 0.09772 0.67240 1.09097 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 3.3559 0.4560 7.360 7.87e-07 *** x 0.1551 0.1078 1.439 0.167 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.8648 on 18 degrees of freedom Multiple R-squared: 0.1031, Adjusted R-squared: 0.0533 F-statistic: 2.07 on 1 and 18 DF, p-value: 0.1674 Hide E Details. R code model_outlr0 &lt;- lm(y ~ x, data = outlr, subset = -c(20,21,22)) summary(model_outlr0) model_outlrA &lt;- lm(y ~ x, data = outlr, subset = -c(21,22)) summary(model_outlrA) model_outlrB &lt;- lm(y ~ x, data = outlr, subset = -c(20,22)) summary(model_outlrB) model_outlrC &lt;- lm(y ~ x, data = outlr, subset = -c(20,21)) summary(model_outlrC) Call: lm(formula = y ~ x, data = outlr, subset = -c(20, 21, 22)) Residuals: Min 1Q Median 3Q Max -0.4790 -0.2708 0.0711 0.2263 0.4094 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 1.86874 0.19583 9.543 3.06e-08 *** x 0.61094 0.05219 11.705 1.47e-09 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.2883 on 17 degrees of freedom Multiple R-squared: 0.8896, Adjusted R-squared: 0.8831 F-statistic: 137 on 1 and 17 DF, p-value: 1.471e-09 Call: lm(formula = y ~ x, data = outlr, subset = -c(21, 22)) Residuals: Min 1Q Median 3Q Max -0.7391 -0.3928 -0.1805 0.1225 3.2689 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 1.7500 0.5736 3.051 0.006883 ** x 0.6933 0.1517 4.570 0.000237 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.8455 on 18 degrees of freedom Multiple R-squared: 0.5371, Adjusted R-squared: 0.5114 F-statistic: 20.89 on 1 and 18 DF, p-value: 0.0002374 Call: lm(formula = y ~ x, data = outlr, subset = -c(20, 22)) Residuals: Min 1Q Median 3Q Max -0.51763 -0.28094 0.03452 0.23586 0.44581 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 1.77463 0.15020 11.81 6.48e-10 *** x 0.63978 0.03551 18.02 5.81e-13 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.2849 on 18 degrees of freedom Multiple R-squared: 0.9474, Adjusted R-squared: 0.9445 F-statistic: 324.5 on 1 and 18 DF, p-value: 5.808e-13 Call: lm(formula = y ~ x, data = outlr, subset = -c(20, 21)) Residuals: Min 1Q Median 3Q Max -2.32947 -0.57819 0.09772 0.67240 1.09097 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 3.3559 0.4560 7.360 7.87e-07 *** x 0.1551 0.1078 1.439 0.167 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.8648 on 18 degrees of freedom Multiple R-squared: 0.1031, Adjusted R-squared: 0.0533 F-statistic: 2.07 on 1 and 18 DF, p-value: 0.1674 Hide F Details. Visualizing four regression fits plot(outlr$x, outlr$y, xlim = c(0, 10), ylim = c(2, 9), xlab = &quot;x&quot;, ylab = &quot;y&quot;) text(4.5, 8.0, &quot;A&quot;, col = &quot;red&quot;) text(9.8, 8.0, &quot;B&quot;, col = &quot;green&quot;) text(9.8, 2.5, &quot;C&quot;, col = &quot;blue&quot;) abline(model_outlr0) abline(model_outlrA, col = &quot;red&quot;) abline(model_outlrB, col = &quot;green&quot;) abline(model_outlrC, col = &quot;blue&quot;) Hide G Details. Results from four regression models \\[\\begin{matrix} \\begin{array}{c} \\text{Results from Four Regressions} \\end{array}\\\\\\scriptsize \\begin{array}{l|rrrrr} \\hline \\text{Data} &amp; b_0 &amp; b_1 &amp; s &amp; R^2(\\%) &amp; t(b_1) \\\\ \\hline \\text{19 Base Points} &amp; 1.869 &amp; 0.611 &amp; 0.288 &amp; 89.0 &amp; 11.71 \\\\ \\text{19 Base Points} ~+~ A &amp; 1.750 &amp; 0.693 &amp; 0.846 &amp; 53.7 &amp; 4.57 \\\\ \\text{19 Base Points} ~+~ B &amp; 1.775 &amp; 0.640 &amp; 0.285 &amp; 94.7 &amp; 18.01 \\\\ \\text{19 Base Points} ~+~ C &amp; 3.356 &amp; 0.155 &amp; 0.865 &amp; 10.3 &amp; 1.44 \\\\ \\hline \\end{array} \\end{matrix}\\] 2.5.2 Exercise. Assessing outliers in lottery sales Assignment Text In an earlier video, we made a scatter plot of population versus sales. This plot exhibits an outlier; the point in the upper left-hand side of the plot represents a zip code that includes Kenosha, Wisconsin. Sales for this zip code are unusually high given its population. This exercise summarizes the regression fit both with and without this zip code in order to see how robust our results are to the inclusion of this unusual observation. Instructions A basic linear regression fit of population on sales has already been fit in the object model_blr. Re-fit this same model to the data, this time omitting Kenosha (observation number 9). Plot these two least squares fitted lines superimposed on the full data set. What is the effect on the distribution of residuals by removing this point? Calculate a normal qq plot with and without Kenosha. Hint. You can extract the residuals from a regression object with the function [residuals()]. eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6IiNMb3QgPC0gcmVhZC5jc3YoXCJDU1ZEYXRhXFxcXFdpc2NfbG90dGVyeS5jc3ZcIiwgaGVhZGVyID0gVFJVRSlcbkxvdCA8LSByZWFkLmNzdihcImh0dHBzOi8vYXNzZXRzLmRhdGFjYW1wLmNvbS9wcm9kdWN0aW9uL3JlcG9zaXRvcmllcy8yNjEwL2RhdGFzZXRzL2E3OTJiMzBmYjMyYjA4OTZkZDY4OTQ1MDFjYmFiMzJiNWQ0OGRmNTEvV2lzY19sb3R0ZXJ5LmNzdlwiLCBoZWFkZXIgPSBUUlVFKSIsInNhbXBsZSI6Im1vZGVsX2JsciA8LWxtKHNhbGVzIH4gcG9wLCBkYXRhID0gTG90KVxuc3VtbWFyeShtb2RlbF9ibHIpXG4jIFJlLWZpdCB0aGlzIG1vZGVsIHRvIHRoZSBkYXRhLCB0aGlzIHRpbWUgb21pdHRpbmcgS2Vub3NoYSAob2JzZXJ2YXRpb24gbnVtYmVyIDkpLlxubW9kZWxfS2Vub3NoYSA8LSBsbShfX18gfiBfX18sIGRhdGEgPSBMb3QsIHN1YnNldCA9IC1jKDkpKVxuc3VtbWFyeShfX18pXG5cbiMgUGxvdCB0aGVzZSB0d28gbGVhc3Qgc3F1YXJlcyBmaXR0ZWQgbGluZXMgc3VwZXJpbXBvc2VkIG9uIHRoZSBmdWxsIGRhdGEgc2V0LlxucGxvdChfX18sIF9fXywgeGxhYiA9IFwicG9wdWxhdGlvblwiLCB5bGFiID0gXCJzYWxlc1wiKVxudGV4dCg1MDAwLCAyNDAwMCwgXCJLZW5vc2hhXCIpXG5hYmxpbmUobW9kZWxfYmxyLCBjb2w9XCJibHVlXCIpXG5hYmxpbmUoX19fLCBjb2w9XCJyZWRcIilcblxuIyBDYWxjdWxhdGUgYSBub3JtYWwgcXEgcGxvdCB3aXRoIGFuZCB3aXRob3V0IEtlbm9zaGEuXG5wYXIobWZyb3cgPSBjKDEsIDIpKVxucXFub3JtKHJlc2lkdWFscyhfX18pLCBtYWluID0gXCJcIilcbnFxbGluZShyZXNpZHVhbHMoX19fKSkpXG5xcW5vcm0ocmVzaWR1YWxzKF9fXykpLCBtYWluID0gXCJcIilcbnFxbGluZShyZXNpZHVhbHMoX19fKSkpIiwic29sdXRpb24iOiJtb2RlbF9ibHIgPC1sbShzYWxlcyB+IHBvcCwgZGF0YSA9IExvdClcbnN1bW1hcnkobW9kZWxfYmxyKVxubW9kZWxfS2Vub3NoYSA8LSBsbShzYWxlcyB+IHBvcCwgZGF0YSA9IExvdCwgc3Vic2V0ID0gLWMoOSkpXG5zdW1tYXJ5KG1vZGVsX0tlbm9zaGEpXG5cbnBsb3QoTG90JHBvcCwgTG90JHNhbGVzLCB4bGFiID0gXCJwb3B1bGF0aW9uXCIsIHlsYWIgPSBcInNhbGVzXCIpXG50ZXh0KDUwMDAsIDI0MDAwLCBcIktlbm9zaGFcIilcbmFibGluZShtb2RlbF9ibHIsIGNvbD1cImJsdWVcIilcbmFibGluZShtb2RlbF9LZW5vc2hhLCBjb2w9XCJyZWRcIilcblxucGFyKG1mcm93ID0gYygxLCAyKSlcbnFxbm9ybShyZXNpZHVhbHMobW9kZWxfYmxyKSwgbWFpbiA9IFwiXCIpXG5xcWxpbmUocmVzaWR1YWxzKG1vZGVsX2JscikpXG5xcW5vcm0ocmVzaWR1YWxzKG1vZGVsX0tlbm9zaGEpLCBtYWluID0gXCJcIilcbnFxbGluZShyZXNpZHVhbHMobW9kZWxfS2Vub3NoYSkpIiwic2N0IjoiZXgoKSAlPiUgY2hlY2tfb2JqZWN0KFwibW9kZWxfYmxyXCIpICU+JSBjaGVja19lcXVhbCgpXG5leCgpICU+JSBjaGVja19mdW5jdGlvbihcImxtXCIsaW5kZXg9MSkgJT4lIHtcbiAgY2hlY2tfYXJnKC4sIFwiZm9ybXVsYVwiKSAlPiUgY2hlY2tfZXF1YWwoKVxuICBjaGVja19hcmcoLiwgXCJkYXRhXCIpICU+JSBjaGVja19lcXVhbCgpXG59XG5leCgpICU+JSBjaGVja19mdW5jdGlvbihcInN1bW1hcnlcIiwgaW5kZXg9MSkgJT4lIGNoZWNrX2FyZyguLCBcIm9iamVjdFwiKSAlPiUgY2hlY2tfZXF1YWwoKVxuZXgoKSAlPiUgY2hlY2tfZnVuY3Rpb24oXCJzdW1tYXJ5XCIsIGluZGV4PTEpICU+JSBjaGVja19yZXN1bHQoKSAlPiUgY2hlY2tfZXF1YWwoKVxuZXgoKSAlPiUgY2hlY2tfb2JqZWN0KFwibW9kZWxfS2Vub3NoYVwiKSAlPiUgY2hlY2tfZXF1YWwoKVxuZXgoKSAlPiUgY2hlY2tfZnVuY3Rpb24oXCJsbVwiLCBpbmRleD0yKSAlPiUge1xuICBjaGVja19hcmcoLiwgXCJmb3JtdWxhXCIpICU+JSBjaGVja19lcXVhbCgpXG4gIGNoZWNrX2FyZyguLCBcImRhdGFcIikgJT4lIGNoZWNrX2VxdWFsKClcbiAgY2hlY2tfYXJnKC4sIFwic3Vic2V0XCIpICU+JSBjaGVja19lcXVhbCgpXG59XG5leCgpICU+JSBjaGVja19mdW5jdGlvbihcInN1bW1hcnlcIiwgaW5kZXg9MikgJT4lIGNoZWNrX2FyZyguLCBcIm9iamVjdFwiKSAlPiUgY2hlY2tfZXF1YWwoKVxuZXgoKSAlPiUgY2hlY2tfZnVuY3Rpb24oXCJzdW1tYXJ5XCIsIGluZGV4PTIpICU+JSBjaGVja19yZXN1bHQoKSAlPiUgY2hlY2tfZXF1YWwoKVxuZXgoKSAlPiUgY2hlY2tfZnVuY3Rpb24oXCJwbG90XCIpICU+JSB7XG4gIGNoZWNrX2FyZyguLCBcInhcIikgJT4lIGNoZWNrX2VxdWFsKClcbiAgY2hlY2tfYXJnKC4sIFwieVwiKSAlPiUgY2hlY2tfZXF1YWwoKVxufVxuZXgoKSAlPiUgY2hlY2tfZnVuY3Rpb24oXCJhYmxpbmVcIixpbmRleD0xKSAlPiUgY2hlY2tfYXJnKC4sIFwicmVnXCIpICU+JSBjaGVja19lcXVhbCgpXG5leCgpICU+JSBjaGVja19mdW5jdGlvbihcImFibGluZVwiLGluZGV4PTIpICU+JSBjaGVja19hcmcoLiwgXCJyZWdcIikgJT4lIGNoZWNrX2VxdWFsKClcbmV4KCkgJT4lIGNoZWNrX2Z1bmN0aW9uKFwicGFyXCIpICU+JSBjaGVja19hcmcoLiwgXCJtZnJvd1wiKSAlPiUgY2hlY2tfZXF1YWwoKVxuZXgoKSAlPiUgY2hlY2tfZnVuY3Rpb24oXCJxcW5vcm1cIixpbmRleD0xKSAlPiUgY2hlY2tfYXJnKC4sIFwieVwiKSAlPiUgY2hlY2tfZXF1YWwoKVxuZXgoKSAlPiUgY2hlY2tfZnVuY3Rpb24oXCJxcWxpbmVcIixpbmRleD0xKSAlPiUgY2hlY2tfYXJnKC4sIFwieVwiKSAlPiUgY2hlY2tfZXF1YWwoKVxuZXgoKSAlPiUgY2hlY2tfZnVuY3Rpb24oXCJxcW5vcm1cIixpbmRleD0yKSAlPiUgY2hlY2tfYXJnKC4sIFwieVwiKSAlPiUgY2hlY2tfZXF1YWwoKVxuZXgoKSAlPiUgY2hlY2tfZnVuY3Rpb24oXCJxcWxpbmVcIixpbmRleD0yKSAlPiUgY2hlY2tfYXJnKC4sIFwieVwiKSAlPiUgY2hlY2tfZXF1YWwoKVxuc3VjY2Vzc19tc2coXCJDb25ncmF0dWxhdGlvbnMhIEp1c3QgYmVjYXVzZSBhbiBvYnNlcnZhdGlvbiBpcyB1bnVzdWFsIGRvZXMgbm90IG1ha2UgaXQgYmFkIG9yIG5vbmluZm9ybWF0aXZlLiBLZW5vc2hhIGlzIGNsb3NlIHRvIHRoZSBJbGxpbm9pcyBib3JkZXI7IHJlc2lkZW50cyBmcm9tIElsbGlub2lzIHByb2JhYmx5IHBhcnRpY2lwYXRlIGluIHRoZSBXaXNjb25zaW4gbG90dGVyeSB0aHVzIGVmZmVjdGl2ZWx5IGluY3JlYXNpbmcgdGhlIHBvdGVudGlhbCBwb29sIG9mIHNhbGVzIGluIEtlbm9zaGEuIEFsdGhvdWdoIHVudXN1YWwsIHRoZXJlIGlzIGludGVyZXN0aW5nIGluZm9ybWF0aW9uIHRvIGJlIGxlYXJuZWQgZnJvbSB0aGlzIG9ic2VydmF0aW9uLlwiKSJ9 "],
["multiple-linear-regression.html", "Chapter 3 Multiple Linear Regression Term Life Data 3.1 Method of least squares 3.2 Statistical inference and multiple linear regresson 3.3 Binary variables 3.4 Categorical variables 3.5 General linear hypothesis", " Chapter 3 Multiple Linear Regression Chapter description This chapter introduces linear regression in the case of several explanatory variables, known as multiple linear regression (MLR). Many basic linear regression concepts extend directly, including goodness of fit measures such as the coefficient of determination and inference using t-statistics. Multiple linear regression models provide a framework for summarizing highly complex, multivariate data. Because this framework requires only linearity in the parameters, we are able to fit models that are nonlinear functions of the explanatory variables, thus providing a wide scope of potential applications. Term Life Data Video Overhead Details A Details. Demand for term life insurance B Details. Term life insurance summary statistics C Details. Summary statistics D Details. Scatter plots of income versus face in original and logarithmic units Hide A Details. Demand for term life insurance “Who buys insurance and how much do they buy?” Companies have data on current customers How do get info on potential (new) customers? To understand demand, consider the Survey of Consumer Finances (SCF) This is a nationally representative sample that contains extensive information on potential U.S. customers. We study a random sample of 500 of the 4,519 households with positive income that were interviewed in the 2004 survey. We now focus on n = 275 households that purchased term life insurance Hide B Details. Term life insurance summary statistics We study y = face, the amount that the company will pay in the event of the death of the named insured. We focus on k = 3 explanatory variables - annual income, - the number of years of education of the survey respondent and - the number of household members, numhh. The data suggest that income and face are skewed so we also introduce logarithmic versions. Hide C Details. Summary statistics #Term &lt;- read.csv(&quot;CSVData\\\\term_life.csv&quot;, header = TRUE) Term &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/efc64bc2d78cf6b48ad2c3f5e31800cb773de261/term_life.csv&quot;, header = TRUE) # PICK THE SUBSET OF THE DATA CORRESPONDING TO TERM PURCHASE Term1 &lt;- subset(Term, subset = face &gt; 0) str(Term1) head(Term1) library(psych) Term2 &lt;- Term1[, c(&quot;education&quot;, &quot;face&quot;, &quot;income&quot;, &quot;logface&quot;, &quot;logincome&quot;, &quot;numhh&quot;)] #options(scipen = 100, digits = 4) head(Term2) describe(Term2)[,c(3,4,8,5,9,2)] &#39;data.frame&#39;: 275 obs. of 7 variables: $ education: int 16 9 16 17 11 16 17 16 14 12 ... $ face : int 20000 130000 1500000 50000 220000 600000 100000 2500000 250000 50000 ... $ income : int 43000 12000 120000 40000 28000 100000 112000 15000 32000 25000 ... $ logface : num 9.9 11.8 14.2 10.8 12.3 ... $ logincome: num 10.67 9.39 11.7 10.6 10.24 ... $ numhh : int 3 3 5 4 4 3 2 4 1 2 ... $ marstat : int 1 1 1 1 2 1 1 1 0 1 ... education face income logface logincome numhh marstat 1 16 20000 43000 9.903488 10.668955 3 1 2 9 130000 12000 11.775290 9.392662 3 1 3 16 1500000 120000 14.220976 11.695247 5 1 4 17 50000 40000 10.819778 10.596635 4 1 6 11 220000 28000 12.301383 10.239960 4 2 8 16 600000 100000 13.304685 11.512925 3 1 education face income logface logincome numhh 1 16 20000 43000 9.903488 10.668955 3 2 9 130000 12000 11.775290 9.392662 3 3 16 1500000 120000 14.220976 11.695247 5 4 17 50000 40000 10.819778 10.596635 4 6 11 220000 28000 12.301383 10.239960 4 8 16 600000 100000 13.304685 11.512925 3 mean sd min median max n education 14.52 2.55 2.00 16.00 1.700e+01 275 face 747581.45 1674362.43 800.00 150000.00 1.400e+07 275 income 208974.62 824009.77 260.00 65000.00 1.000e+07 275 logface 11.99 1.87 6.68 11.92 1.645e+01 275 logincome 11.15 1.30 5.56 11.08 1.612e+01 275 numhh 2.96 1.49 1.00 3.00 9.000e+00 275 Hide D Details. Scatter plots of income versus face in original and logarithmic units par(mfrow = c(1, 2)) plot(Term2$income, Term2$face, xlab = &quot;income&quot;, ylab = &quot;face&quot;) plot(Term2$logincome, Term2$logface, xlab = &quot;log&quot;, ylab = &quot;log face&quot;) 3.1 Method of least squares In this section, you learn how to: Interpret correlation coefficients by visualizing a scatterplot matrix Fit a plane to data using the method of least squares Predict an observation using a least squares fitted plane 3.1.1 Video Video Overhead Details A Details. Correlation table B Details. Scatterplot matrix C Details. Visualizing a regression plane D Details. Method of least squares E Details. Fit a multiple linear regression model Hide A Details. Correlation table round(cor(Term2), digits=3) education face income logface logincome numhh education 1.000 0.244 0.163 0.383 0.343 -0.064 face 0.244 1.000 0.217 0.656 0.323 0.107 income 0.163 0.217 1.000 0.251 0.518 0.142 logface 0.383 0.656 0.251 1.000 0.482 0.288 logincome 0.343 0.323 0.518 0.482 1.000 0.179 numhh -0.064 0.107 0.142 0.288 0.179 1.000 Hide B Details. Scatterplot matrix Term3 &lt;- Term1[,c(&quot;numhh&quot;, &quot;education&quot;, &quot;logincome&quot;, &quot;logface&quot;)] pairs(Term3, upper.panel = NULL, gap = 0, cex.labels = 1.25) Hide C Details. Visualizing a regression plane education &lt;- seq(3, 16, length = 15) logincome &lt;- seq(5, 15, length = 15) f &lt;- function(education,logincome){ r &lt;- 5 + 0.221*education + 0.354*logincome } logface &lt;- outer(education, logincome, f) persp(education, logincome, logface, theta = 30, phi = 30, expand = 0.5, ticktype = &quot;detailed&quot;) rm(education,logincome,logface) education &lt;- seq(3, 16, length = 15) logincome &lt;- seq(5, 15, length = 15) f &lt;- function(education,logincome){ r &lt;- 5 + 0.221*education + 0.354*logincome } logface &lt;- outer(education, logincome, f) persp(education, logincome, logface, theta = 30, phi = 30, expand = 0.5, ticktype = &quot;simple&quot;, #ticktype = &quot;detailed&quot;, # xlab = &quot;x1&quot;, ylab=&quot;x2&quot;,zlab=&quot;y&quot;, nticks = 1) rm(education,logincome,logface) Hide D Details. Method of least squares For observation \\(\\{(y, x_1, \\ldots, x_k)\\}\\), the height of the regression plane is \\[b_0 + b_1 x_1 + \\cdots + b_k x_k .\\] Thus, \\(y - (b_0 + b_1 x_1 + \\cdots + b_k x_k)\\) represents the deviation. The sum of squared deviations is \\[SS(b_0, \\ldots, b_k) = \\sum (y - (b_0 + b_1 x_1 + \\cdots + b_k x_k))^2 .\\] The method of least squares – determine values of \\(b_0, \\ldots, b_k\\) that minimize \\(SS\\). Hide E Details. Fit a multiple linear regression model Term_mlr &lt;- lm(logface ~ education + numhh + logincome, data = Term2) round(coefficients(Term_mlr), digits=4) newdata &lt;- data.frame(logincome = log(60000), education = 12, numhh = 3) exp(predict(Term_mlr, newdata)) (Intercept) education numhh logincome 2.5841 0.2064 0.3060 0.4935 1 90135.86 3.1.2 Exercise. Least squares and term life data Assignment Text The prior video introduced the Survey of Consumer Finances (SCF) term life data. A subset consisting of only those who purchased term life insurance, has already been read into a dataframe Term2. Suppose that you wish to predict the amount of term life insurance that someone will purchase but are uneasy about the education variable. The SCF education variable is the number of completed years of schooling and so 12 corresponds to completing high school in the US. Your sense is that, for purposes of purchasing life insurance, high school graduates and those that attend college should be treated the same. So, in this exercise, your will create a new variable, education1, that is equal to years of education for those with education less than or equal to 12 and is equal to 12 otherwise. Instructions Use the pmin() function to create the education1 variable as part of the Term2 dataframe. Check your work by examining summary statistics for the revised Term2 dataframe. Examine correlations for the revised dataframe. Using the method of least squares and the function lm(), fit a MLR model using logface as the dependent variables and using education, numhh, and logincome as explanatory variables. With this fitted model and the function predict(), predict the face amount of insurance that someone with income of 40,000, 11 years of education, and 4 people in the household would purchase. Hint. Remember that your prediction is in log dollars so you need to exponentiate it to get the results in the original dollar units eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6IiNUZXJtIDwtIHJlYWQuY3N2KFwiQ1NWRGF0YVxcXFx0ZXJtX2xpZmUuY3N2XCIsIGhlYWRlciA9IFRSVUUpXG5UZXJtIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9hc3NldHMuZGF0YWNhbXAuY29tL3Byb2R1Y3Rpb24vcmVwb3NpdG9yaWVzLzI2MTAvZGF0YXNldHMvZWZjNjRiYzJkNzhjZjZiNDhhZDJjM2Y1ZTMxODAwY2I3NzNkZTI2MS90ZXJtX2xpZmUuY3N2XCIsIGhlYWRlciA9IFRSVUUpXG5UZXJtMSA8LSBzdWJzZXQoVGVybSwgc3Vic2V0ID0gZmFjZSA+IDApXG5UZXJtMiA8LSBUZXJtMVssIGMoXCJlZHVjYXRpb25cIiwgXCJmYWNlXCIsIFwiaW5jb21lXCIsIFwibG9nZmFjZVwiLCBcImxvZ2luY29tZVwiLCBcIm51bWhoXCIpXSIsInNhbXBsZSI6IiMgQ3JlYXRlIHRoZSBgZWR1Y2F0aW9uMWAgdmFyaWFibGUgYXMgcGFydCBvZiB0aGUgYFRlcm0yYCBkYXRhZnJhbWUuXG5UZXJtMiRlZHVjYXRpb24xIDwtIHBtaW4oMTIsIFRlcm0yJGVkdWNhdGlvbilcblxuIyBDaGVjayB5b3VyIHdvcmsgYnkgZXhhbWluaW5nIHN1bW1hcnkgc3RhdGlzdGljcyBmb3IgdGhlIHJldmlzZWQgYFRlcm0yYCBkYXRhZnJhbWUuXG5zdW1tYXJ5KF9fXylcblxuIyBFeGFtaW5lIGNvcnJlbGF0aW9ucyBmb3IgdGhlIHJldmlzZWQgZGF0YWZyYW1lLlxucm91bmQoY29yKF9fXyksIGRpZ2l0cz0zKVxuXG4jIEZpdCBhIE1MUiBtb2RlbCB1c2luZyBgbG9nZmFjZWAgYXMgdGhlIGRlcGVuZGVudCB2YXJpYWJsZXMgYW5kIHVzaW5nIGBlZHVjYXRpb25gLCBgbnVtaGhgLCBhbmQgYGxvZ2luY29tZWAgYXMgZXhwbGFuYXRvcnkgdmFyaWFibGVzLlxuVGVybV9tbHIyIDwtIGxtKGxvZ2ZhY2UgfiBfX18gKyBudW1oaCArIGxvZ2luY29tZSwgZGF0YSA9IFRlcm0yKVxuXG4jIFByZWRpY3QgdGhlIGZhY2UgYW1vdW50IG9mIGluc3VyYW5jZSB0aGF0IHNvbWVvbmUgd2l0aCBpbmNvbWUgb2YgNDAsMDAwLCAxMSB5ZWFycyBvZiBlZHVjYXRpb24sIGFuZCA0IHBlb3BsZSBpbiB0aGUgaG91c2Vob2xkIHdvdWxkIHB1cmNoYXNlLlxubmV3ZGF0YSA8LSBkYXRhLmZyYW1lKGxvZ2luY29tZSA9IGxvZyg0MDAwMCksIGVkdWNhdGlvbjEgPSAxMSwgbnVtaGggPSA0KVxuZXhwKHByZWRpY3QoX19fLCBuZXdkYXRhKSkiLCJzb2x1dGlvbiI6IlRlcm0yJGVkdWNhdGlvbjEgPC0gcG1pbigxMiwgVGVybTIkZWR1Y2F0aW9uKVxuc3VtbWFyeShUZXJtMilcbnJvdW5kKGNvcihUZXJtMiksIGRpZ2l0cz0zKVxuVGVybV9tbHIyIDwtIGxtKGxvZ2ZhY2UgfiBlZHVjYXRpb24xICsgbnVtaGggKyBsb2dpbmNvbWUsIGRhdGEgPSBUZXJtMilcbm5ld2RhdGEgPC0gZGF0YS5mcmFtZShsb2dpbmNvbWUgPSBsb2coNDAwMDApLCBlZHVjYXRpb24xID0gMTEsIG51bWhoID0gNClcbmV4cChwcmVkaWN0KFRlcm1fbWxyMiwgbmV3ZGF0YSkpIiwic2N0IjoiZXgoKSAlPiUgY2hlY2tfb2JqZWN0KFwiVGVybTJcIikgJT4lIGNoZWNrX2NvbHVtbihcImVkdWNhdGlvbjFcIikgJT4lIGNoZWNrX2VxdWFsKClcbmV4KCkgJT4lIGNoZWNrX2Z1bmN0aW9uKFwic3VtbWFyeVwiKSAlPiUge1xuICBjaGVja19hcmcoLiwgXCJvYmplY3RcIikgJT4lIGNoZWNrX2VxdWFsKClcbiAgY2hlY2tfcmVzdWx0KCkgJT4lIGNoZWNrX2VxdWFsKClcbn1cbmV4KCkgJT4lIGNoZWNrX2Z1bmN0aW9uKFwicm91bmRcIikgJT4lIHtcbiAgY2hlY2tfYXJnKC4sIFwieFwiKSAlPiUgY2hlY2tfZXF1YWwoKVxuICBjaGVja19hcmcoLiwgXCJkaWdpdHNcIikgJT4lIGNoZWNrX2VxdWFsKClcbn1cbmV4KCkgJT4lIGNoZWNrX29iamVjdChcIlRlcm1fbWxyMlwiKSAlPiUgY2hlY2tfZXF1YWwoKVxuZXgoKSAlPiUgY2hlY2tfZnVuY3Rpb24oXCJsbVwiKSAlPiUge1xuICBjaGVja19hcmcoLiwgXCJmb3JtdWxhXCIpICU+JSBjaGVja19lcXVhbCgpXG4gIGNoZWNrX2FyZyguLCBcImRhdGFcIikgJT4lIGNoZWNrX2VxdWFsKClcbn1cbmV4KCkgJT4lIGNoZWNrX29iamVjdChcIm5ld2RhdGFcIikgJT4lIGNoZWNrX2VxdWFsKClcbmV4KCkgJT4lIGNoZWNrX2Z1bmN0aW9uKFwiZXhwXCIpICU+JSBjaGVja19yZXN1bHQoKSAlPiUgY2hlY2tfZXF1YWwoKVxuZXgoKSAlPiUgY2hlY2tfZnVuY3Rpb24oXCJwcmVkaWN0XCIpICU+JSB7XG4gIGNoZWNrX2FyZyguLCBcIm9iamVjdFwiKSAlPiUgY2hlY2tfZXF1YWwoKVxuICBjaGVja19hcmcoLiwgXCJuZXdkYXRhXCIpICU+JSBjaGVja19lcXVhbCgpXG59XG5zdWNjZXNzX21zZyhcIkNvbmdyYXR1bGF0aW9ucyEgWW91IG5vdyBoYXZlIGV4cGVyaWVuY2UgZml0dGluZyBhIHJlZ3Jlc3Npb24gcGxhbmUgYW5kIHVzaW5nIHRoaXMgcGxhbmUgZm9yIHByZWRpY3Rpb25zLiBQcmVkaWN0aW9uIGlzIG9uZSBvZiB0aGUga2V5IHRhc2tzIG9mICdwcmVkaWN0aXZlIG1vZGVsaW5nLicgV2VsbCBkb25lIVwiKSJ9 3.1.3 Exercise. Interpreting coefficients as proportional changes Assignment Text In a previous exercise, you fit a MLR model using logface as the outcome variable and using education, numhh, and logincome as explanatory variables; the resulting fit is in the object Term_mlr. For this fit, the coefficient associated with education is 0.2064. We now wish to interpret this regression coefficient. The typical interpretation of coefficients in a regression model is as a partial slope. That is, for the coefficient \\(b_1\\) associated with \\(x_1\\), we interpret \\(b_1\\) to be amount that the expected outcome changes per unit change in \\(x_1\\), holding the other explanatory variables fixed. For the term life example, the units of the outcome are in logarithmic dollars. So, for small values of \\(b_1\\), we can interpret this to be a proportional change in dollars. Instructions Determine least square fitted values for several selected values of education, holding other explantory variables fixed. For this part of the demonstration, we used their mean values. Determine the proportional changes. Note the relation between these values from a discrete change approximation to the regression coefficient for education equal to 0.2064. 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 3.1.4 Exercise. Interpreting coefficients as elasticities Assignment Text In a previous exercise, you fit a MLR model using logface as the outcome variable and using education, numhh, and logincome as explanatory variables; the resulting fit is in the object Term_mlr. From this fit, the coefficient associated with logincome is 0.4935. We now wish to interpret this regression coefficient. The typical interpretation of coefficients in a regression model is as a partial slope. When both \\(x_1\\) and \\(y\\) are in logarithmic units, then we can interpret \\(b_1\\) to be ratio of two percentage changes, known as an elasticity in economics. Mathematically, we summarize this as \\[ \\frac{\\partial \\ln y}{\\partial \\ln x} = \\left(\\frac{\\partial y}{y}\\right) ~/ ~\\left(\\frac{\\partial x}{x}\\right) . \\] Instructions For several selected values of logincome, determine the corresponding proportional changes. Determine least square fitted values for several selected values of logincome, holding other explantory variables fixed. Determine the corresponding proportional changes for the fitted values. Calculate the ratio of proportional changes of fitted values to those for income. Note the relation between these values (from a discrete change approximation) to the regression coefficient for logincome equal to 0.4935. Hint. When you calculate the ratio of proportional changes of fitted values to those for income, note the relation between these values (from a discrete change approximation) to the regression coefficient for logincome equal to 0.4935. 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 3.2 Statistical inference and multiple linear regresson In this section, you learn how to: Explain mean square error and residual standard error in terms of degrees of freedom Develop an ANOVA table and use it to derive the coefficient of determination Calculate and interpret the coefficient of determination adjusted for degrees of freedom Conduct a test of a regression coefficient Summarize regression coefficients using point and interval estimators 3.2.1 Video Video Overhead Details A Details. Goodness of fit B Details. Goodness of fit and term life C Details. Statistical inference D Details. Statistical inference and term life Hide A Details. Goodness of fit Summarize deviations \\(s^2\\) \\(R^2\\) \\(R_a^2\\) ANOVA table Hide B Details. Goodness of fit and term life Term_mlr &lt;- lm(logface ~ education + numhh + logincome, data = Term2) summary(Term_mlr) anova(Term_mlr) Call: lm(formula = logface ~ education + numhh + logincome, data = Term2) Residuals: Min 1Q Median 3Q Max -5.7420 -0.8681 0.0549 0.9093 4.7187 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 2.58408 0.84643 3.053 0.00249 ** education 0.20641 0.03883 5.316 2.22e-07 *** numhh 0.30605 0.06333 4.833 2.26e-06 *** logincome 0.49353 0.07754 6.365 8.32e-10 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.525 on 271 degrees of freedom Multiple R-squared: 0.3425, Adjusted R-squared: 0.3353 F-statistic: 47.07 on 3 and 271 DF, p-value: &lt; 2.2e-16 Analysis of Variance Table Response: logface Df Sum Sq Mean Sq F value Pr(&gt;F) education 1 140.55 140.549 60.417 1.601e-13 *** numhh 1 93.68 93.681 40.270 9.251e-10 *** logincome 1 94.24 94.238 40.510 8.316e-10 *** Residuals 271 630.43 2.326 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Hide C Details. Statistical inference hypothesis testing of a regression coefficient confidence intervals Hide D Details. Statistical inference and term life Term_mlr &lt;- lm(logface ~ education + numhh + logincome, data = Term2) model_sum &lt;- summary(Term_mlr) model_sum$coefficients round(confint(Term_mlr, level = .95), digits = 3) round(confint(Term_mlr, level = .95), digits = 3) Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 2.5840786 0.84642972 3.052916 2.491588e-03 education 0.2064139 0.03883186 5.315581 2.223619e-07 numhh 0.3060455 0.06332511 4.832926 2.255708e-06 logincome 0.4935323 0.07754198 6.364711 8.316097e-10 2.5 % 97.5 % (Intercept) 0.918 4.250 education 0.130 0.283 numhh 0.181 0.431 logincome 0.341 0.646 2.5 % 97.5 % (Intercept) 0.918 4.250 education 0.130 0.283 numhh 0.181 0.431 logincome 0.341 0.646 3.2.2 Exercise. Statistical inference and term life Assignment Text In later chapters, we will learn how to specify a model using diagnostics techniques; these techniques were used to specify face in log dollars for the outcome and similarly income in log dollars as an explanatory variable. Just to see how things work, in this exercise we will create new variables face and income that are in the original units and run a regression with these. We have already seen that rescaling by constants do not affect relationships but can be helpful with interpretations, so we define both face and income to be in thousands of dollars. A prior video introduced the term life dataframe Term2. Instructions Create Term2$face by exponentiating logface and dividing by 1000. For convenience, we are storing this variable in the data set Term2. Use the same process to create Term2$income. Run a regression using face as the outcome variable and education, numhh, and income as explanatory variables. Summarize this model and identify the residual standard error (\\(s\\)) as well as the coefficient of determination (\\(R^2\\)) and the version adjusted for degrees of freedom (\\(R_a^2\\)). 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 3.3 Binary variables In this section, you learn how to: Interpret regression coefficients associated with binary variables Use binary variables and interaction terms to create regression models that are nonlinear in the covariates 3.3.1 Video Video Overhead Details A Details. Binary variables B Details. Visualize effect of binary variables C Details. R script for visualization D Details. Interaction Terms E Details. Visualizing binary variables with interactions terms Hide A Details. Binary variables We can define a new variable \\[ single= \\left\\{ \\begin{array}{ll} 0 &amp; \\text{for other respondents} \\\\ 1 &amp; \\text{for single respondents} \\end{array} \\right. \\] The variable single is said to be an indicator, or dummy, variable. To interpret coefficients, we now consider the regression function \\[ \\text{E }logface = \\beta_0 + \\beta_1 logincome + \\beta_2 single \\] - This can be expressed as two lines \\[ \\text{E }logface = \\left\\{ \\begin{array}{ll} \\beta_0 + \\beta_1 logincome &amp; \\textrm{for other respondents} \\\\ \\beta_0 + \\beta_2 + \\beta_1 logincome &amp; \\textrm{for single respondents} \\end{array} \\right. . \\] - The least squares method of calculating the estimators, and the resulting theoretical properties, are the still valid when using binary variables. Hide B Details. Visualize effect of binary variables Hide C Details. R script for visualization Term4 &lt;- Term1[,c(&quot;numhh&quot;, &quot;education&quot;, &quot;logincome&quot;, &quot;logface&quot;, &quot;marstat&quot;)] Term4$marstat &lt;- as.factor(Term4$marstat) table(Term4$marstat) Term4$single &lt;- 1*(Term4$marstat == 0) model_single &lt;- lm(logface ~ logincome + single, data = Term4) summary(model_single) plot(Term4$logincome,Term4$logface,xlab=&quot;logarithmic income&quot;, ylab=&quot;log face&quot;, pch= 1+16*Term4$single, col = c(&quot;red&quot;, &quot;black&quot;, &quot;black&quot;)[Term4$marstat]) Ey1 &lt;- model_single$coefficients[1]+model_single$coefficients[2]*Term4$logincome Ey2 &lt;- Ey1 + model_single$coefficients[3] lines(Term4$logincome, Ey1) lines(Term4$logincome, Ey2, col=&quot;red&quot;) 0 1 2 57 208 10 Call: lm(formula = logface ~ logincome + single, data = Term4) Residuals: Min 1Q Median 3Q Max -6.2828 -0.8785 0.0364 0.9227 5.9573 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.09007 0.88643 5.742 2.49e-08 *** logincome 0.63378 0.07776 8.151 1.33e-14 *** single -0.80006 0.24796 -3.227 0.00141 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.615 on 272 degrees of freedom Multiple R-squared: 0.2605, Adjusted R-squared: 0.255 F-statistic: 47.9 on 2 and 272 DF, p-value: &lt; 2.2e-16 Hide D Details. Interaction Terms Linear regression models are defined in terms of linear combinations of explanatory varibles but we can expand their scope through nonlinear transformations One type of nonlinear transform is the product of two varibles that is used to create what is known as an interaction variable To interpret coefficients, we now consider the regression function \\[ \\text{E }logface = \\beta_0 + \\beta_1 logincome + \\beta_2 single + \\beta_3 single*logincome \\] - This can be expressed as two lines with different slopes \\[ \\text{E }logface = \\left\\{ \\begin{array}{ll} \\beta_0 + \\beta_1 logincome &amp; \\textrm{for other respondents} \\\\ \\beta_0 + \\beta_2 + (\\beta_1 + \\beta_3) logincome &amp; \\textrm{for single respondents} \\end{array} \\right. . \\] Hide E Details. Visualizing binary variables with interactions terms Term4 &lt;- Term1[,c(&quot;numhh&quot;, &quot;education&quot;, &quot;logincome&quot;, &quot;logface&quot;, &quot;marstat&quot;)] Term4$marstat &lt;- as.factor(Term4$marstat) table(Term4$marstat) Term4$single &lt;- 1*(Term4$marstat == 0) model_single_inter &lt;- lm(logface ~ logincome + single + single*logincome, data = Term4) summary(model_single_inter) plot(Term4$logincome,Term4$logface,xlab=&quot;logarithmic income&quot;, ylab=&quot;log face&quot;, pch= 1+16*Term4$single, col = c(&quot;red&quot;, &quot;black&quot;, &quot;black&quot;)[Term4$marstat]) Ey1 &lt;- model_single_inter$coefficients[1]+model_single_inter$coefficients[2]*Term4$logincome Ey2 &lt;- Ey1 + model_single_inter$coefficients[3]+model_single_inter$coefficients[4]*Term4$logincome lines(Term4$logincome, Ey1) lines(Term4$logincome, Ey2, col=&quot;red&quot;) 0 1 2 57 208 10 Call: lm(formula = logface ~ logincome + single + single * logincome, data = Term4) Residuals: Min 1Q Median 3Q Max -6.2149 -0.8287 0.0696 0.9308 5.6070 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.77902 0.92550 6.244 1.64e-09 *** logincome 0.57288 0.08124 7.051 1.47e-11 *** single -7.29211 2.74216 -2.659 0.0083 ** logincome:single 0.61244 0.25764 2.377 0.0181 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.601 on 271 degrees of freedom Multiple R-squared: 0.2756, Adjusted R-squared: 0.2676 F-statistic: 34.36 on 3 and 271 DF, p-value: &lt; 2.2e-16 3.3.2 Exercise. Binary variables and term life Assignment Text In the prior video, we saw how the variable single can be used with logarithmic income to explain logarithmic face amounts of term life insurance that people purchase. The coefficient associated with this variable turns out to be negative which is intuitively appealing; if an individual is single, then that person may not have the strong need to purchase financial security for others in the event of unexpected death. In this exercise, we will extend this by incorporating single into our larger regression model that contains other explanatory varibles, logincome, education and numhh. The data have been pre-loaded into the dataframe Term4. Instructions Calculate a table of correlation coefficients to examine pairwise linear relationships among the variables numhh, education, logincome, single, and logface. Fit a MLR model of logface using explanatory variables numhh, education, logincome, and single. Examine the residual standard deviation \\(s\\), the coefficient of determination \\(R^2\\), and the adjusted version \\(R_a^2\\). Also note the statistical significance of the coefficient associated with single. Repeat the MLR model fit while adding the interaction term single*logincome. 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 3.4 Categorical variables In this section, you learn how to: Represent categorical variables using a set of binary variables Interpret the regression coefficients associated with categorical variables Describe the effect of the reference level choice on the model fit 3.4.1 Video Video Overhead Details A Details. Categorical variables B Details. Term life example C Details. Term life boxplots D Details. Regression with a categorical variable E Details. t-ratios depend on the reference level Hide A Details. Categorical variables Categorical variables provide labels for observations to denote membership in distinct groups, or categories. A binary variable is a special case of a categorical variable. To illustrate, a binary variable may tell us whether or not someone has health insurance. A categorical variable could tell us whether someone has (i) private individual health insurance, (ii) private group insurance, (iii) public insurance or (iv) no health insurance. For categorical variables, there may or may not be an ordering of the groups. For health insurance, it is difficult to say which is ‘larger’, private individual versus public health insurance (such as Medicare). However, for education, we may group individuals from a dataset into ‘low’, ‘intermediate’ and ‘high’ years of education. Factor is another term used for a (unordered) categorical explanatory variable. Hide B Details. Term life example We studied y = logface, the amount that the company will pay in the event of the death of the named insured (in logarithmic dollars), focusing on the explanatory variables logincome, education, and numhh. We now supplement this by including the categorical variable, marstat, that is the marital status of the survey respondent. This may be: 1, for married 2, for living with partner 0, for other (SCF actually breaks this category into separated, divorced, widowed, never married and inapplicable, for persons age 17 or less or no further persons) Hide C Details. Term life boxplots # Pre-exercise code #Term &lt;- read.csv(&quot;CSVData\\\\term_life.csv&quot;, header = TRUE) Term &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/efc64bc2d78cf6b48ad2c3f5e31800cb773de261/term_life.csv&quot;, header = TRUE) Term1 &lt;- subset(Term, subset = face &gt; 0) Term4 &lt;- Term1[,c(&quot;numhh&quot;, &quot;education&quot;, &quot;logincome&quot;, &quot;marstat&quot;, &quot;logface&quot;)] Term4$single &lt;- 1*(Term4$marstat == 0) Term4$marstat&lt;- as.factor(Term4$marstat) boxplot(logface ~ marstat, ylab = &quot;log face&quot;, xlab = &quot;Marital Status&quot;, data = Term4) table(Term4$marstat) # SUMMARY BY LEVEL OF MARSTAT #library(Rcmdr) #numSummary(Term4[, &quot;logface&quot;], groups = Term4$marstat, statistics = c(&quot;mean&quot;, &quot;sd&quot;)) #numSummary(Term4[, &quot;logface&quot;], statistics = c(&quot;mean&quot;, &quot;sd&quot;)) 0 1 2 57 208 10 Hide D Details. Regression with a categorical variable Term4$marstat &lt;- as.factor(Term4$marstat) Term4$marstat &lt;- relevel(Term4$marstat, ref = &quot;2&quot;) summary(lm(logface ~ logincome+education+numhh+marstat, data = Term4)) Call: lm(formula = logface ~ logincome + education + numhh + marstat, data = Term4) Residuals: Min 1Q Median 3Q Max -5.8875 -0.8505 0.1124 0.8468 4.5173 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 2.60536 0.95218 2.736 0.006629 ** logincome 0.45151 0.07872 5.736 2.61e-08 *** education 0.20467 0.03862 5.299 2.42e-07 *** numhh 0.24770 0.06940 3.569 0.000424 *** marstat0 0.23234 0.53283 0.436 0.663155 marstat1 0.78941 0.49532 1.594 0.112169 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.513 on 269 degrees of freedom Multiple R-squared: 0.358, Adjusted R-squared: 0.3461 F-statistic: 30 on 5 and 269 DF, p-value: &lt; 2.2e-16 Hide E Details. t-ratios depend on the reference level \\[ \\begin{array}{l|rr|rr|rr} \\hline &amp; \\text{Model 1}&amp;&amp; \\text{Model 2}&amp;&amp; \\text{Model 3}&amp;\\\\ \\hline \\text{Var}&amp; \\text{Coef} &amp; \\text{t-stat} &amp; \\text{Coef} &amp; \\text{t-stat} &amp;\\text{Coef} &amp; \\text{t-stat} \\\\\\hline logincome &amp; 0.452 &amp; 5.74 &amp; 0.452 &amp; 5.74 &amp; 0.452 &amp; 5.74 \\\\ education &amp;0.205 &amp; 5.30 &amp;0.205 &amp; 5.30&amp;0.205 &amp; 5.30 \\\\ numhh &amp; 0.248 &amp; 3.57 &amp; 0.248 &amp; 3.57 &amp; 0.248 &amp; 3.57 \\\\\\hline \\text{Intercept} &amp; 3.395 &amp; 3.77 &amp; 2.605&amp; 2.74 &amp; 2.838 &amp; 3.34\\\\ \\text{mar=0} &amp; -0.557 &amp; -2.15&amp; 0.232 &amp; 0.44\\\\ \\text{mar=1} &amp; &amp; &amp; 0.789 &amp; 1.59 &amp; 0.557 &amp; 2.15\\\\ \\text{mar=2}&amp; -0.789 &amp; -1.59 &amp; &amp; &amp; -0.232 &amp; -0.44\\\\ \\hline \\end{array} \\] 3.4.2 Exercise. Categorical variables and Wisconsin hospital costs Assignment Text This exercise examines the impact of various predictors on hospital charges. Identifying predictors of hospital charges can provide direction for hospitals, government, insurers and consumers in controlling these variables that in turn leads to better control of hospital costs. The data, from 1989, are aggregated by: drg, diagnostic related groups of costs, payer, type of health care provider (Fee for service, HMO, and other), and hsa, nine major geographic areas in Wisconsin. Some preliminary analysis of the data has already been done. In this exercise, we will analyze logcharge, the logarithm of total hospital charges per number of discharges, in terms of log_numdschg, the logarithm of the number of discharges. In the dataframe Hcost which has been loaded in advance, we restrict consideration to three types of drgs, numbers 209, 391, and 431. Instructions Fit a basic linear regression model using logarithmic number of discharges to predict logarithmic hospital costs and superimposed the fitted regression line on the scatter plot. Produce a scatter plot of logarithmic number of discharges to predict logarithmic hospital costs. Allow plotting symbols and colors to vary by diagnostic related group. Fit a MLR model using logarithmic number of discharges to predict logarithmic hospital costs, allowing intercepts and slopes to vary by diagnostic related groups. Superimpose the fits from the MLR model on the scatter plot of logarithmic number of discharges to predict logarithmic hospital costs. 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 3.5 General linear hypothesis In this section, you learn how to: Jointly test the significance of a set of regression coefficients using the general linear hypothesis Conduct a test of a regression coefficient versus one- or two-side alternatives 3.5.1 Video Video Overhead Details A Details. Testing the significance of a categorical variable B Details. Overview of the general linear hypothesis C Details. Procedure for conducting the general linear hypothesis D Details. The general linear hypothesis for a single variable Hide A Details. Testing the significance of a categorical variable #Term &lt;- read.csv(&quot;CSVData\\\\term_life.csv&quot;, header = TRUE) Term &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/efc64bc2d78cf6b48ad2c3f5e31800cb773de261/term_life.csv&quot;, header = TRUE) Term1 &lt;- subset(Term, subset = face &gt; 0) Term4 &lt;- Term1[,c(&quot;numhh&quot;, &quot;education&quot;, &quot;logincome&quot;, &quot;marstat&quot;, &quot;logface&quot;)] Term_mlr1 &lt;- lm(logface ~ logincome + education + numhh + as.factor(Term4$marstat), data = Term4) anova(Term_mlr1) Term_mlr2 &lt;- lm(logface ~ logincome + education + numhh, data = Term4) Fstat &lt;- (anova(Term_mlr2)$`Sum Sq`[4] - anova(Term_mlr1)$`Sum Sq`[5])/(2*anova(Term_mlr1)$`Mean Sq`[5]) Fstat cat(&quot;p-value is&quot;, 1 - pf(Fstat, df1 = 2 , df2 = anova(Term_mlr1)$Df[5])) Analysis of Variance Table Response: logface Df Sum Sq Mean Sq F value Pr(&gt;F) logincome 1 222.63 222.629 97.280 &lt; 2.2e-16 *** education 1 51.50 51.502 22.504 3.407e-06 *** numhh 1 54.34 54.336 23.743 1.883e-06 *** as.factor(Term4$marstat) 2 14.81 7.406 3.236 0.04085 * Residuals 269 615.62 2.289 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 [1] 3.236048 p-value is 0.04085475 Hide B Details. Overview of the general linear hypothesis The likelihood ratio is a general statistical test procedure that compares a model to a subset The general linear hypothesis test procedure is similar. Start with a (large) linear regression model, examine the fit to a set of data Compare this to smaller model that is a subset of the large model. “Subset” is the sense that regression coefficients from the small model are linear combinations of regression coefficients of the large model (e.g., set them to zero) Although the likelihood ratio test is more generally available, the general linear hypothesis test is more accurate for smaller data sets (for normally distributed data) Hide C Details. Procedure for conducting the general linear hypothesis Run the full regression and get the error sum of squares and mean square error, which we label as \\((Error SS)_{full}\\) and \\(s^2_{full}\\), respectively. Run a reduced regression and get the error sum of squares, labelled \\((Error SS)_{reduced}\\). Using \\(p\\) for the number of linear restrictions, calculate \\[ F-ratio = \\frac{(Error SS)_{reduced}-(Error SS)_{full}}{p s^2_{full}} . \\] The probability value is \\(p-value = \\Pr(F_{p,df} &gt; F-ratio)\\) where \\(F_{p,df}\\) has an F distribution with degrees of freedom p and df, respectively. (Here, df is the degrees of freedom for the full model.) Hide D Details. The general linear hypothesis for a single variable Suppose that you wish to test the hypothesisthat a regression coefficient equals 0. One could use the general linear hypothsis procedure with \\(p=1\\). One could also examine the corresponding \\(t-ratio\\). Which is correct? Both. One can show that \\((t-ratio)^2 = F-ratio\\), so they are equivalent statistics. The general linear hypothesis is useful because it can be extended to multiple coefficients. The t-ratio is useful because it can be used to examine one-sided alternative hypotheses. 3.5.2 Exercise. Hypothesis testing and term life Assignment Text With our Term life data, let us compare a model based on the binary variable that indicates whether a survey respondent is single versus the more complex marital status, marstat. In principle, more detailed information is better. But, it may be that the additional information in marstat, compared to single, does not help fit the data in a significantly better way. As part of the preparatory work, the dataframe Term4 is available that includes the binary variable single and the factor marstat. Moreover, the regression object Term_mlr contains information in a multiple linear regression fit of logface on the base explanatory variables ’logincome,education, andnumhh`. Instructions Fit a MLR model using the base explanatory variables plus single and another model using the base variables plus marstat. Use the F test to decide whether the additional complexity marstat is warranted by calculating the p-value associated with this test. Fit a MLR model using the base explanatory variables plus single interacted with logincome and another model using the base variables plus marstat interacted with logincome. Use the F test to decide whether the additional complexity marstat is warranted by calculating the p-value associated with this test. Hint Here is the code to calculate it by hand Fstat12 &lt;- (anova(Term_mlr1)$`Sum Sq`[5] - anova(Term_mlr2)$`Sum Sq`[5])/(1*anova(Term_mlr2)$`Mean Sq`[5]) Fstat12 cat(&quot;p-value is&quot;, 1 - pf(Fstat12, df1 = 1 , df2 = anova(Term_mlr2)$Df[5])) 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 3.5.3 Exercise. Hypothesis testing and Wisconsin hospital costs Assignment Text In a previous exercise, you were introduced to a dataset with hospital charges aggregated by: drg, diagnostic related groups of costs, payer, type of health care provider (Fee for service, HMO, and other), and hsa, nine major geographic areas. We continue our analysis of the outcome variable logcharge, the logarithm of total hospital charges per number of discharges, in terms of log_numdschg, the logarithm of the number of discharges, as well as the three categorical variables used in the aggregation. As before, we restrict consideration to three types of drgs, numbers 209, 391, and 431 that has been preloaded in the dataframe Hcost1. Instructions Fit a basic linear regression model using logarithmic hospital costs as the outcome variable and explanatory variable logarithmic number of discharges. Fit a MLR model using logarithmic hospital costs as the outcome variable and explanatory variables logarithmic number of discharges and the categorical variable diagnostic related group. Identify the F statistic and p value that test the importance of diagnostic related group. Fit a MLR model using logarithmic hospital costs as the outcome variable and explanatory variable logarithmic number of discharges interacted with diagnostic related group. Identify the F statistic and p value that test the importance of diagnostic related group interaction with logarithmic number of discharges. Calculate a coefficient of determination, \\(R^2\\), for each of these models as well as for a model using logarithmic number of discharges and categorical variable hsa as predictors. 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 3.5.4 Exercise. Hypothesis testing and auto claims Assignment Text As an actuarial analyst, you are working with a large insurance company to help them understand their claims distribution for their private passenger automobile policies. You have available claims data for a recent year, consisting of: state: codes 01 through 17 used, with each code randomly assigned to an actual individual state class: rating class of operator, based on age, gender, marital status, and use of vehicle gender: operator gender age: operator age paid: amount paid to settle and close a claim. You are focusing on older drivers, 50 and higher, for which there are n = 6,773 claims available. Instructions Run a regression of logpaid on age. Is age a statistically significant variable? To respond to this question, use a formal test of hypothesis. State your null and alternative hypotheses, decision-making criterion, and your decision-making rule. Also comment on the goodness of fit of this variable. Consider using class as a single explanatory variable. Use the one factor to estimate the model and respond to the following questions. b (i). What is the point estimate of claims in class C7, drivers 50-69, driving to work or school, less than 30 miles per week with annual mileage under 7500, in natural logarithmic units? b (ii). Determine the corresponding 95% confidence interval of expected claims, in natural logarithmic units. b (iii). Convert the 95% confidence interval of expected claims that you determined in part b(ii) to dollars. Run a regression of logpaid on age, gender and the categorical variables state and class. c (i). Is gender a statistically significant variable? To respond to this question, use a formal test of hypothesis. State your null and alternative hypotheses, decision-making criterion, and your decision-making rule. c (ii). Is class a statistically significant variable? To respond to this question, use a formal test of hypothesis. State your null and alternative hypotheses, decision-making criterion, and your decision-making rule. c (iii). Use the model to provide a point estimate of claims in dollars (not log dollars) for a male age 60 in STATE 2 in class C7. c (iv). Write down the coefficient associated with class C7 and interpret this coefficient. Call: lm(formula = logpaid ~ class, data = AutoC) Residuals: Min 1Q Median 3Q Max -4.7002 -0.6912 -0.0437 0.7128 4.1009 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 6.940938 0.039699 174.838 &lt;2e-16 *** classC11 0.010564 0.050696 0.208 0.8349 classC1A -0.075432 0.128202 -0.588 0.5563 classC1B 0.057420 0.065380 0.878 0.3798 classC1C -0.154707 0.178007 -0.869 0.3848 classC2 -0.139442 0.142595 -0.978 0.3282 classC6 -0.015057 0.053217 -0.283 0.7772 classC7 -0.039775 0.053191 -0.748 0.4546 classC71 0.012730 0.050887 0.250 0.8025 classC72 0.241716 0.122626 1.971 0.0487 * classC7A 0.122755 0.108174 1.135 0.2565 classC7B 0.131512 0.056956 2.309 0.0210 * classC7C 0.302596 0.125307 2.415 0.0158 * classF1 0.062962 0.202561 0.311 0.7559 classF11 -0.136891 0.173726 -0.788 0.4307 classF6 -0.030546 0.094148 -0.324 0.7456 classF7 -0.363874 0.144807 -2.513 0.0120 * classF71 -0.005476 0.117810 -0.046 0.9629 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.07 on 6755 degrees of freedom Multiple R-squared: 0.005048, Adjusted R-squared: 0.002544 F-statistic: 2.016 on 17 and 6755 DF, p-value: 0.00786 Call: lm(formula = logpaid ~ state, data = AutoC) Residuals: Min 1Q Median 3Q Max -4.6555 -0.6847 -0.0415 0.7005 4.0788 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 6.82202 0.08286 82.328 &lt; 2e-16 *** stateSTATE 02 0.11739 0.08878 1.322 0.186133 stateSTATE 03 0.02224 0.10071 0.221 0.825246 stateSTATE 04 0.05195 0.09262 0.561 0.574893 stateSTATE 06 0.30735 0.09327 3.295 0.000988 *** stateSTATE 07 0.10127 0.10537 0.961 0.336562 stateSTATE 10 0.21247 0.10486 2.026 0.042787 * stateSTATE 11 0.17462 0.36540 0.478 0.632751 stateSTATE 12 0.40876 0.10715 3.815 0.000138 *** stateSTATE 13 0.21792 0.11111 1.961 0.049890 * stateSTATE 14 0.06485 0.11667 0.556 0.578297 stateSTATE 15 0.08479 0.08596 0.986 0.323956 stateSTATE 17 0.22406 0.09585 2.338 0.019442 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.068 on 6760 degrees of freedom Multiple R-squared: 0.008109, Adjusted R-squared: 0.006348 F-statistic: 4.606 on 12 and 6760 DF, p-value: 1.749e-07 Analysis of Variance Table Response: logpaid Df Sum Sq Mean Sq F value Pr(&gt;F) state 12 63.0 5.2495 4.6055 1.749e-07 *** Residuals 6760 7705.2 1.1398 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Call: lm(formula = logpaid ~ class + state + age + gender, data = AutoC) Residuals: Min 1Q Median 3Q Max -4.7266 -0.6802 -0.0433 0.7072 4.1809 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 6.974923 0.140144 49.770 &lt; 2e-16 *** classC11 0.055573 0.051695 1.075 0.282410 classC1A -0.110591 0.128238 -0.862 0.388506 classC1B 0.022671 0.066388 0.341 0.732740 classC1C -0.160355 0.178056 -0.901 0.367842 classC2 -0.183935 0.142784 -1.288 0.197719 classC6 0.057560 0.058767 0.979 0.327395 classC7 -0.021854 0.054531 -0.401 0.688605 classC71 0.024622 0.053049 0.464 0.642569 classC72 0.260057 0.123451 2.107 0.035192 * classC7A 0.119315 0.108879 1.096 0.273183 classC7B 0.124196 0.059098 2.102 0.035634 * classC7C 0.299023 0.126300 2.368 0.017934 * classF1 0.130054 0.202380 0.643 0.520491 classF11 -0.058684 0.174394 -0.337 0.736503 classF6 0.068612 0.098270 0.698 0.485079 classF7 -0.309974 0.145271 -2.134 0.032898 * classF71 0.029459 0.118848 0.248 0.804239 stateSTATE 02 0.098538 0.089280 1.104 0.269765 stateSTATE 03 0.006302 0.101050 0.062 0.950272 stateSTATE 04 0.019266 0.093712 0.206 0.837118 stateSTATE 06 0.283853 0.094105 3.016 0.002568 ** stateSTATE 07 0.076593 0.106361 0.720 0.471475 stateSTATE 10 0.181802 0.105448 1.724 0.084739 . stateSTATE 11 0.172272 0.365574 0.471 0.637487 stateSTATE 12 0.388006 0.108333 3.582 0.000344 *** stateSTATE 13 0.188491 0.111980 1.683 0.092371 . stateSTATE 14 0.068141 0.116720 0.584 0.559373 stateSTATE 15 0.065570 0.086624 0.757 0.449110 stateSTATE 17 0.199398 0.096914 2.057 0.039680 * age -0.003021 0.001690 -1.787 0.073914 . genderM 0.038953 0.026907 1.448 0.147747 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.066 on 6741 degrees of freedom Multiple R-squared: 0.01365, Adjusted R-squared: 0.009113 F-statistic: 3.009 on 31 and 6741 DF, p-value: 4.354e-08 Analysis of Variance Table Response: logpaid Df Sum Sq Mean Sq F value Pr(&gt;F) class 17 39.2 2.3066 2.0293 0.007346 ** state 12 61.0 5.0841 4.4728 3.354e-07 *** age 1 3.4 3.4284 3.0162 0.082480 . gender 1 2.4 2.3823 2.0958 0.147747 Residuals 6741 7662.2 1.1367 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Submission Correctness Tests (SCT) success_msg(“Congratulations!”) "],
["variable-selection.html", "Chapter 4 Variable Selection 4.1 An iterative approach to data analysis and modeling 4.2 Automatic variable selection procedures 4.3 Residual analysis 4.4 Unusual observations 4.5 Collinearity 4.6 Selection criteria", " Chapter 4 Variable Selection Chapter description This chapter describes tools and techniques to help you select variables to enter into a linear regression model, beginning with an iterative model selection process. In applications with many potential explanatory variables, automatic variable selection procedures are available that will help you quickly evaluate many models. Nonetheless, automatic procedures have serious limitations including the inability to account properly for nonlinearities such as the impact of unusual points; this chapter expands upon the Chapter 2 discussion of unusual points. It also describes collinearity, a common feature of regression data where explanatory variables are linearly related to one another. Other topics that impact variable selection, including out-of-sample validation, are also introduced. 4.1 An iterative approach to data analysis and modeling In this section, you learn how to: Describe the iterative approach to data analysis and modeling. 4.1.1 Video Video Overhead Details A Details. Iterative approach B Details. Many possible models C Details. Model validation Hide A Details. Iterative approach Model formulation stage Fitting Diagnostic checking - the data and model must be consistent with one another before additional inferences can be made. plot.new() par(mar=c(0,0,0,0), cex=0.9) plot.window(xlim=c(0,18),ylim=c(-5,5)) text(1,3,labels=&quot;DATA&quot;,adj=0, cex=0.8) text(1,0,labels=&quot;PLOTS&quot;,adj=0, cex=0.8) text(1,-3,labels=&quot;THEORY&quot;,adj=0, cex=0.8) text(3.9,0,labels=&quot;MODEL\\nFORMULATION&quot;,adj=0, cex=0.8) text(8.1,0,labels=&quot;FITTING&quot;,adj=0, cex=0.8) text(11,0,labels=&quot;DIAGNOSTIC\\nCHECKING&quot;,adj=0, cex=0.8) text(15,0,labels=&quot;INFERENCE&quot;,adj=0, cex=0.8) text(14.1,0.5,labels=&quot;OK&quot;,adj=0, cex=0.6) rect(0.8,2.0,2.6,4.0) arrows(1.7,2.0,1.7,1.0,code=2,lwd=2,angle=25,length=0.10) rect(0.8,-1.0,2.6,1.0) arrows(1.7,-2.0,1.7,-1.0,code=2,lwd=2,angle=25,length=0.10) rect(0.8,-4.0,2.6,-2.0) arrows(2.6,0,3.2,0,code=2,lwd=2,angle=25,length=0.10) x&lt;-c(5,7.0,5,3.2) y&lt;-c(2,0,-2,0) polygon(x,y) arrows(7.0,0,8.0,0,code=2,lwd=2,angle=25,length=0.10) rect(8.0,-1.0,9.7,1.0) arrows(9.7,0,10.2,0,code=2,lwd=2,angle=25,length=0.10) x1&lt;-c(12,14.0,12,10.2) y1&lt;-c(2,0,-2,0) polygon(x1,y1) arrows(14.0,0,14.8,0,code=2,lwd=2,angle=25,length=0.10) rect(14.8,-1.0,17.5,1.0) arrows(12,-2.0,12,-3,code=2,lwd=2,angle=25,length=0.10) arrows(12,-3.0,5,-3,code=2,lwd=2,angle=25,length=0.10) arrows(5,-3.0,5,-2,code=2,lwd=2,angle=25,length=0.10) Hide B Details. Many possible models \\[ \\begin{array}{l|r} \\hline \\text{E }y = \\beta _{0} &amp; \\text{1 model with no variables } \\\\ \\text{E }y = \\beta _{0}+\\beta_1 x_{i}, &amp; \\text{4 models with one variable} \\\\ \\text{E }y = \\beta _{0}+\\beta_1 x_{i}+\\beta_{2} x_{j}, &amp; \\text{6 models with two variables} \\\\ \\text{E }y = \\beta _{0}+\\beta_{1} x_{1}+\\beta_{2} x_{j} +\\beta_{3} x_{k},&amp; \\text{4 models with three variables} \\\\ \\text{E }y = \\beta_{0} + \\beta_{1} x_{1} + \\beta_{2} x_{2} +\\beta_{3} x_{3}+\\beta_{4} x_{4} &amp; \\text{1 model with all variables} \\\\ \\hline \\end{array} \\] With k explanatory variables, there are \\(2^k\\) possible linear models There are infinitely many nonlinear ones!! Hide C Details. Model validation Model validation is the process of confirming our proposed model. Concern: data-snooping - fitting many models to a single set of data. Response to concern: out-of-sample validation. Divide the data into model development, or training and validation, or test, subsamples. par(mai=c(0,0.1,0,0)) plot.new() plot.window(xlim=c(0,18),ylim=c(-10,10)) rect(1,-1.2,14,1.2) rect(7,4,15,8) rect(1,-8,6,-4) x&lt;-seq(1.5,9,length=6) y&lt;-rep(0,6) text(x,y,labels=c(1:6),cex=1.5) x1&lt;-seq(10.5,11.5,length=3) y1&lt;-rep(0,3) text(x1,y1,labels=rep(&quot;.&quot;,3),cex=3) text(13,0,labels=&quot;n&quot;,cex=1.5) text(15,0,labels=&quot;ORIGINAL\\nSAMPLE\\nSIZE n&quot;,adj=0) text(7.5,6,labels=&quot;MODEL DEVELOPMENT\\nSUBSAMPLE SIZE&quot;,adj=0) text(12.5,5.3, expression(n[1]), adj=0, cex=1.1) text(1.4,-6,labels=&quot;VALIDATION\\nSUBSAMPLE\\nSIZE&quot;,adj=0) text(2.8,-7.2,expression(n[2]),adj=0, cex=1.1) arrows(1.8,0.8,8.3,3.9,code=2,lwd=2,angle=15,length=0.2) arrows(4.8,0.8,9,3.8,code=2,lwd=2,angle=15,length=0.2) arrows(9.1,0.9,9.5,3.8,code=2,lwd=2,angle=15,length=0.2) arrows(12.8,0.8,10,3.8,code=2,lwd=2,angle=15,length=0.2) arrows(2.9,-0.9,2.5,-3.8,code=2,lwd=2,angle=15,length=0.2) arrows(5.9,-0.9,3.1,-3.8,code=2,lwd=2,angle=15,length=0.2) arrows(7.4,-0.9,3.5,-3.8,code=2,lwd=2,angle=15,length=0.2) 4.1.2 MC Exercise. An iterative approach to data modeling Which of the following is not true? A. Diagnostic checking reveals symptoms of mistakes made in previous specifications. B. Diagnostic checking provides ways to correct mistakes made in previous specifications. C. Model formulation is accomplished by using prior knowledge of relationships. D. Understanding theoretical model properties is not really helpful when matching a model to data or inferring general relationships based on the data. 4.2 Automatic variable selection procedures In this section, you learn how to: Identify some examples of automatic variable selection procedures Describe the purpose of automatic variable selection procedures and their limitations Describe “data-snooping” 4.2.1 Video Video Overhead Details A Details. Classic stepwise regression algorithm B Details. Drawbacks of stepwise regression C Details. Data-snooping in stepwise regression D Details. Variants of stepwise regression E Details. Automatic variable selection procedures Hide A Details. Classic stepwise regression algorithm Suppose that the analyst has identified one variable as the outcome, \\(y\\), and \\(k\\) potential explanatory variables, \\(x_1, x_2, \\ldots, x_k\\). (i). Consider all possible regressions using one explanatory variable. Choose the one with the highest t-statistic. (ii). Add a variable to the model from the previous step. The variable to enter is with the highest t-statistic. (iii). Delete a variable to the model from the previous step. Delete the variable with the small t-statistic if the statistic is less than, e.g., 2 in absolute value. (iv). Repeat steps (ii) and (iii) until all possible additions and deletions are performed. Hide B Details. Drawbacks of stepwise regression The procedure “snoops” through a large number of models and may fit the data “too well.” There is no guarantee that the selected model is the best. The algorithm does not consider models that are based on nonlinear combinations of explanatory variables. It ignores the presence of outliers and high leverage points. Hide C Details. Data-snooping in stepwise regression Generate \\(y\\) and \\(x_1 - x_{50}\\) using a random number generator By design, there is no relation between \\(y\\) and \\(x_1 - x_{50}\\). But, through stepwise regression, we “discover” a relationship that explains 14% of the variation!!! Call: lm(formula = y ~ xvar27 + xvar29 + xvar32, data = X) Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -0.04885 0.09531 -0.513 0.6094 xvar27 0.21063 0.09724 2.166 0.0328 * xvar29 0.24887 0.10185 2.443 0.0164 * xvar32 0.25390 0.09823 2.585 0.0112 * Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.9171 on 96 degrees of freedom Multiple R-squared: 0.1401, Adjusted R-squared: 0.1132 F-statistic: 5.212 on 3 and 96 DF, p-value: 0.002233 Hide D Details. Variants of stepwise regression This uses the R function step() The option direction can be used to change how variables enter Forward selection. Add one variable at a time without trying to delete variables. Backwards selection. Start with the full model and delete one variable at a time without trying to add variables. The option scope can be used to specify which variables must be included Hide E Details. Automatic variable selection procedures Stepwise regression is a type of automatic variable selection procedure. These procedures are useful because they can quickly search through several candidate models. They mechanize certain routine tasks and are excellent at discovering patterns in data. They are so good at detecting patterns that they analyst must be wary of overfitting (data-snooping) They can miss certain patterns (nonlinearities, unusual points) A model suggested by automatic variable selection procedures should be subject to the same careful diagnostic checking procedures as a model arrived at by any other means 4.2.2 Exercise. Data-snooping in stepwise regression Assignment Text Automatic variable selection procedures, such as the classic stepwise regression algorithm, are very good at detecting patterns. Sometimes they are too good in the sense that they detect patterns in the sample that are not evident in the population from which the data are drawn. The detect “spurious” patterns. This exercise illustrates this phenomenom by using a simulation, designed so that the outcome variable (y) and the explanatory variables are mutually independent. So, by design, there is no relationship between the outcome and the explanatory variables. As part of the code set-up, we have n = 100 observations generated of the outcome y and 50 explanatory variables, xvar1 through xvar50. As anticipated, collections of explanatory variables are not statistically significant. However, with the step() function, you will find some statistically significant relationships! Instructions Fit a basic linear regression model and MLR model with the first ten explanatory variables. Compare the models via an F test. Fit a multiple linear regression model with all fifty explanatory variables. Compare this model to the one with ten variables via an F test. Use the step function to find the best model starting with the fitted model containing all fifty explanatory variables and summarize the fit. Hint. The code shows stepwise regression using BIC, a criterion that results in simpler models than AIC. For AIC, use the option k=2 in the [step()] function (the default) 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 4.3 Residual analysis In this section, you learn how to: Explain how residual analysis can be used to improve a model specification Use relationships between residuals and potential explanatory variables to improve model specification 4.3.1 Video Video Overhead Details A Details. Residual analysis B Details. Using residuals to select explanatory variables C Details. Detecting relationships between residuals and explanatory variables Hide A Details. Residual analysis Use \\(e_i = y_i - \\hat{y}_i\\) as the ith residual. Later, I will discuss rescaling by, for example, \\(s\\), to get a standardized residual. Role of residuals: If the model formulation is correct, then residuals should be approximately equal to random errors or “white noise.” Method of attack: Look for patterns in the residuals. Use this information to improve the model specification. Hide B Details. Using residuals to select explanatory variables Residual analysis can help identify additional explanatory variables that may be used to improve the formulation of the model. If the model is correct, then residuals should resemble random errors and contain no discernible patterns. Thus, when comparing residuals to explanatory variables, we do not expect any relationships. If we do detect a relationship, then this suggests the need to control for this additional variable. Hide C Details. Detecting relationships between residuals and explanatory variables Calculate summary statistics and display the distribution of residuals to identify outliers. Calculate the correlation between the residuals and additional explanatory variables to search for linear relationships. Create scatter plots between the residuals and additional explanatory variables to search for nonlinear relationships. 4.3.2 Exercise. Residual analysis and risk manager survey Assignment Text This exercise examines data, pre-loaded in the dataframe survey, from a survey on the cost effectiveness of risk management practices. Risk management practices are activities undertaken by a firm to minimize the potential cost of future losses, such as the event of a fire in a warehouse or an accident that injures employees. This exercise develops a model that can be used to make statements about cost of managing risks. A measure of risk management cost effectiveness, logcost, is the outcome variable. This variable is defined as total property and casualty premiums and uninsured losses as a proportion of total assets, in logarithmic units. It is a proxy for annual expenditures associated with insurable events, standardized by company size. Explanatory variables include logsize, the logarithm of total firm assets, and indcost, a measure of the firm’s industry risk. Instructions Fit and summarize a MLR model using logcost as the outcome variable and logsize and indcost as explanatory variables. Plot residuals of the fitted model versus indcost and superimpose a locally fitted line using the R function lowess(). Fit and summarize a MLR model of logcost on logsize, indcost and a squared version of indcost. Plot residuals of the fitted model versus `indcost’ and superimpose a locally fitted line using lowess(). Hint. You can access model residuals using mlr.survey1$residuals or mlr.survey1($residuals) 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 4.3.3 Exercise. Added variable plot and refrigerator prices Assignment Text What characteristics of a refrigerator are important in determining its price (price)? We consider here several characteristics of a refrigerator, including the size of the refrigerator in cubic feet (rsize), the size of the freezer compartment in cubic feet (fsize), the average amount of money spent per year to operate the refrigerator (ecost, for energy cost), the number of shelves in the refrigerator and freezer doors (shelves), and the number of features (features). The features variable includes shelves for cans, see-through crispers, ice makers, egg racks and so on. Both consumers and manufacturers are interested in models of refrigerator prices. Other things equal, consumers generally prefer larger refrigerators with lower energy costs that have more features. Due to forces of supply and demand, we would expect consumers to pay more for these refrigerators. A larger refrigerator with lower energy costs that has more features at the similar price is considered a bargain to the consumer. How much extra would the consumer be willing to pay for this additional space? A model of prices for refrigerators on the market provides some insight to this question. To this end, we analyze data from n = 37 refrigerators. Instructions # Pre-exercise code Refrig &lt;- read.table(&quot;CSVData\\\\Refrig.csv&quot;, header = TRUE, sep = &quot;,&quot;) summary(Refrig) Refrig1 &lt;- Refrig[c(&quot;price&quot;, &quot;ecost&quot;, &quot;rsize&quot;, &quot;fsize&quot;, &quot;shelves&quot;, &quot;s_sq_ft&quot;, &quot;features&quot;)] round(cor(Refrig1), digits = 3) refrig_mlr1 &lt;- lm(price ~ rsize + fsize + shelves + features, data = Refrig) summary(refrig_mlr1) Refrig$residuals1 &lt;- residuals(refrig_mlr1) refrig_mlr2 &lt;- lm(ecost ~ rsize + fsize + shelves + features, data = Refrig) summary(refrig_mlr2) Refrig$residuals2 &lt;- residuals(refrig_mlr2) plot(Refrig$residuals2, Refrig$residuals1) #library(Rcmdr) #refrig_mlr3 &lt;- lm(price ~ rsize + fsize + shelves + features + ecost, data = Refrig) #avPlots(refrig_mlr3, terms = &quot;ecost&quot;) price ecost rsize fsize Min. : 460.0 Min. :60.00 Min. :12.6 Min. :4.100 1st Qu.: 545.0 1st Qu.:66.00 1st Qu.:12.9 1st Qu.:4.400 Median : 590.0 Median :68.00 Median :13.2 Median :5.100 Mean : 626.4 Mean :70.51 Mean :13.4 Mean :5.184 3rd Qu.: 685.0 3rd Qu.:75.00 3rd Qu.:13.9 3rd Qu.:5.700 Max. :1200.0 Max. :94.00 Max. :14.7 Max. :7.400 shelves s_sq_ft features Min. :1.000 Min. :20.60 Min. : 1.000 1st Qu.:2.000 1st Qu.:23.40 1st Qu.: 2.000 Median :2.000 Median :24.00 Median : 3.000 Mean :2.514 Mean :24.53 Mean : 3.459 3rd Qu.:3.000 3rd Qu.:25.50 3rd Qu.: 5.000 Max. :5.000 Max. :30.20 Max. :12.000 price ecost rsize fsize shelves s_sq_ft features price 1.000 0.522 -0.024 0.720 0.400 0.155 0.697 ecost 0.522 1.000 -0.033 0.855 0.188 0.058 0.334 rsize -0.024 -0.033 1.000 -0.235 -0.363 0.401 -0.096 fsize 0.720 0.855 -0.235 1.000 0.251 0.110 0.439 shelves 0.400 0.188 -0.363 0.251 1.000 -0.527 0.160 s_sq_ft 0.155 0.058 0.401 0.110 -0.527 1.000 0.083 features 0.697 0.334 -0.096 0.439 0.160 0.083 1.000 Call: lm(formula = price ~ rsize + fsize + shelves + features, data = Refrig) Residuals: Min 1Q Median 3Q Max -128.200 -34.963 7.081 28.716 155.096 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -698.89 302.60 -2.310 0.02752 * rsize 56.50 20.56 2.748 0.00977 ** fsize 75.40 13.93 5.414 5.96e-06 *** shelves 35.92 11.08 3.243 0.00277 ** features 25.16 5.04 4.992 2.04e-05 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 68.11 on 32 degrees of freedom Multiple R-squared: 0.789, Adjusted R-squared: 0.7626 F-statistic: 29.92 on 4 and 32 DF, p-value: 2.102e-10 Call: lm(formula = ecost ~ rsize + fsize + shelves + features, data = Refrig) Residuals: Min 1Q Median 3Q Max -7.8483 -4.3064 0.5154 2.6324 9.3596 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -14.2165 20.9365 -0.679 0.5020 rsize 2.8744 1.4225 2.021 0.0517 . fsize 8.9085 0.9636 9.245 1.49e-10 *** shelves 0.2895 0.7664 0.378 0.7081 features -0.2006 0.3487 -0.575 0.5692 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 4.712 on 32 degrees of freedom Multiple R-squared: 0.7637, Adjusted R-squared: 0.7342 F-statistic: 25.86 on 4 and 32 DF, p-value: 1.247e-09 4.4 Unusual observations In this section, you learn how to: Compare and contrast three alternative definitions of a standardized residual Evaluate three alternative options for dealing with outliers Assess the impact of a high leverage observation Evaluate options for dealing with high leverage observations Describe the notion of influence and Cook’s Distance for quantifying influence 4.4.1 Video Video Overhead Details A Details. Unusual observations B Details. Standardized residuals C Details. Outlier - an unusal standardized residual D Details. High leverage points E Details. High leverage point graph F Details. Leverage Hide A Details. Unusual observations Regression coefficients can be expressed as (matrix) weighted averages of outcomes Averages, even weighted averages can be strongly influenced by unusual observations Observations may be unusual in the y direction or in the X space For unusual in the y direction, we use a residual \\(e = y - \\hat{y}\\) By subtracting the fitted value \\(\\hat{y}\\), we look to the y distance from the regression plane In this way, we “control” for values of explanatory variables Hide B Details. Standardized residuals We standardize residuals so that we can focus on relationships of interest and achieve carry-over of experience from one data set to another. Three commonly used definitions of standardize residuals are: \\[ \\text{(a) }\\frac{e_i}{s}, \\ \\ \\ \\text{ (b) }\\frac{e_i}{s\\sqrt{1-h_{ii}}}, \\ \\ \\ \\text{(c)}\\frac{e_i}{s_{(i)}\\sqrt{1-h_{ii}}}. \\] First choice is simple Second choice, from theory, \\(\\mathrm{Var}(e_i)=\\sigma ^{2}(1-h_{ii}).\\) Here, \\(h_{ii}\\) is the \\(i\\)th leverage (defined later). Third choice is termed “studentized residuals”. Idea: numerator is independent of the denominator. Hide C Details. Outlier - an unusal standardized residual An outlier is an observation that is not well fit by the model; these are observations where the residual is unusually large. Unusual means what? Many packages mark a point if the |standardized residual| &gt; 2. Options for handling outliers Ignore them in the analysis but be sure to discuss their effects. Delete them from the data set (but be sure to discuss their effects). Create a binary variable to indicator their presence. (This will increase your \\(R^2\\)!) Hide D Details. High leverage points A high leverage point is an observation that is “far away” in the \\(x\\)-space from others. One can get a feel for high leverage observations by looking a summary statistics (mins, maxs) for each explanatory variable. Options for dealing with high leverage points are comparable to outliers, we can ignore their effects, delete them, or mark them with a binary indicator variable. Hide E Details. High leverage point graph library(cluster) #library(MASS) par(mar=c(3.2,5.4,.2,.2)) plot(1,5,type=&quot;p&quot;,pch=19,cex=1.5,xlab=&quot;&quot;,ylab=&quot;&quot;,cex.lab=1.5,xaxt=&quot;n&quot;,yaxt=&quot;n&quot;,xlim=c(-3,5),ylim=c(-12,12)) mtext(expression(x[2]), side=1,line=2, cex=2.0) mtext(expression(x[1]), side=2, line=2, las=2, cex=2.0) arrows(1.5,5,4,5,code=1,lwd=2,angle=15,length=0.25) xycov&lt;-matrix(c(2, -5,-5, 20),nrow=2,ncol=2) xyloc&lt;-matrix(c(0, 0),nrow=1,ncol=2) polygon(ellipsoidPoints(xycov, d2 = 2, loc=xyloc),col=&quot;black&quot;) Hide F Details. Leverage Using matrix algebra, one can express the ith fitted value as a linear combination of observations \\[ \\hat{y}_{i} = h_{i1} y_{1} + \\cdots +h_{ii}y_{i}+\\cdots+h_{in}y_{n}. \\] The term \\(h_{ii}\\) is known as the ith leverage The larger the value of \\(h_{ii}\\), the greater the effect of the ith observation \\(y_i\\) on the ith fitted value \\(\\hat{y}_i\\). Statistical routines have values of the leverage coded, so computing this quantity. The key thing to know is that \\(h_{ii}\\) is based solely on the explanatory variables. If you change the \\(y\\) values, the leverage does not change. As a commonly used rule of thumb, a leverage is deemed to be “unusual” if its value exceeds three times the average (= number of regression coefficients divided by the number of observations.) 4.4.2 Exercise. Outlier example In chapter 2, we consider a fictitious data set of 19 “base” points plus three different types of unusual points. In this exercise, we consider the effect of one unusal point, “C”, this both an outlier (unusual in the “y” direction) and a high leverage point (usual in the x-space). The data have been pre-loaded in the dataframe outlrC. Instructions Fit a basic linear regression model of y on x and store the result in an object. Use the function rstandard() to extract the standardized residuals from the fitted regression model object and summarize them. Use the function hatvalues() to extract the leverages from the model fitted and summarize them. Plot the standardized residuals versus the leverages to see the relationship between these two measures that calibrate how unusual an observation is. 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 4.4.3 Exercise. High leverage and risk manager survey Assignment Text In a prior exercise, we fit a regression model of logcost on logsize, indcost and a squared version of indcost. This model is summarized in the object mlr_survey2. In this exercise, we examine the robustness of the model to unusual observations. Instructions Use the R functions rstandard() and hatvalues() to extract the standardized residuals and leverages from the model fitted. Summarize the distributions graphically. You will see that there are two observations where the leverages are high, numbers 10 and 16. On looking at the dataset, these turn out to be observations in a high risk industry. Create a histogram of the variable indcost to corroborate this. Re-run the regression omitting observations 10 and 16. Summarize this regression and the regression in the object mlr_survey2, noting differences in the coefficients. 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 4.5 Collinearity In this section, you learn how to: Define collinearity and describe its potential impact on regression inference Define a variance inflation factor and describe its effect on a regression coefficients standard error Describe rules of thumb for assessing collinearity and options for model reformulation in the presence of severe collinearity Compare and contrast effects of leverage and collinearity 4.5.1 Video Video Overhead Details A Details. Collinearity B Details. Quantifying collinearity C Details. Options for handling collinearity Hide A Details. Collinearity Collinearity, or multicollinearity, occurs when one explanatory variable is, or nearly is, a linear combination of the other explanatory variables. Useful to think of the explanatory variables as being highly correlated with one another. Collinearity neither precludes us from getting good fits nor from making predictions of new observations. Estimates of error variances and, therefore, tests of model adequacy, are still reliable. In cases of serious collinearity, standard errors of individual regression coefficients can be large. With large standard errors, individual regression coefficients may not be meaningful. Because a large standard error means that the corresponding t-ratio is small, it is difficult to detect the importance of a variable. Hide B Details. Quantifying collinearity A common way to quantify collinearity is through the variance inflation factor (VIF). Suppose that the set of explanatory variables is labeled \\(x_{1},x_{2},\\dots,x_{k}\\). Run the regression using \\(x_{j}\\) as the “outcome” and the other \\(x\\)’s as the explanatory variables. Denote the coefficient of determination from this regression by \\(R_j^2\\). Define the variance inflation factor \\[ VIF_{j}=\\frac{1}{1-R_{j}^{2}},\\ \\ \\ \\text{ for } j = 1,2,\\ldots, k. \\] Hide C Details. Options for handling collinearity Rule of thumb: When \\(VIF_{j}\\) exceeds 10 (which is equivalent to \\(R_{j}^{2}&gt;90\\%\\)), we say that severe collinearity exists. This may signal is a need for action. Recode the variables by “centering” - that is, subtract the mean and divide by the standard deviation. Ignore the collinearity in the analysis but comment on it in the interpretation. Probably the most common approach. Replace one or more variables by auxiliary variables or transformed versions. Remove one or more variables. Easy. Which One? is hard. Use interpretation. Which variable(s) do you feel most comfortable with? Use automatic variable selection procedures to suggest a model. 4.5.2 Exercise. Collinearity and term life Assignment Text We have seen that adding an explanatory variable \\(x^2\\) to a model is sometimes helpful even though it is perfectly related to \\(x\\) (such as through the function \\(f(x)=x^2\\)). But, for some data sets, higher order polynomials and interactions can be approximately linearly related (depending on the range of the data). This exercise returns to our term life data set Term1 (preloaded) and demonstrates that collinearity can be severe when introducing interaction terms. Instructions Fit a MLR model of logface on explantory variables education, numhh and logincome Use the function vif() from the car package (preloaded) to calculate variance inflation factors. Fit and summarize a MLR model of logface on explantory variables education , numhh and logincome with an interaction between numhh and logincome, then extract variance inflation factors. Hint. If the car package is not available to you, then you could calculate vifs using the [lm()] function, treating each variable separately. For example 1/(1-summary(lm(education ~ numhh + logincome, data = Term1))$r.squared) gives the education vif. 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 4.6 Selection criteria In this section, you learn how to: Summarize a regression fit using alternative goodness of fit measures Validate a model using in-sample and out-of-sample data to mitigate issues of data-snooping Compare and contrast SSPE and PRESS statistics for model validation 4.6.1 Video Video Overhead Details A Details. Goodness of fit B Details. Goodness of fit and information criteria C Details. Out of sample validation D Details. Out of sample validation procedure E Details. Cross - validation Hide A Details. Goodness of fit Criteria that measure the proximity of the fitted model and realized data are known as goodness of fit statistics. Basic examples include: the coefficient of determination \\((R^{2})\\), an adjusted version \\((R_{a}^{2})\\), the size of the typical error \\((s)\\), and \\(t\\)-ratios for each regression coefficient. Hide A Details. Goodness of fit A general measure is Akaike’s Information Criterion, defined as \\[ AIC = -2 \\times (fitted~log~likelihood) + 2 \\times (number~of~parameters) \\] For model comparison, the smaller the \\(AIC,\\) the better is the fit. This measures balances the fit (in the first part) with a penalty for complexity (in the second part) It is a general measure - for linear regression, it reduces to \\[ AIC = n \\ln (s^2) + n \\ln (2 \\pi) +n +k + 3 . \\] So, selecting a model to minimize \\(s\\) or \\(s^2\\) is equivalent to model selection based on minimizing \\(AIC\\) (same k). Hide C Details. Out of sample validation When you choose a model to minimize \\(s\\) or \\(AIC\\), it is based on how well the model fits the data at hand, or the model development, or training, data As we have seen, this approach is susceptible to overfitting. A better approach is to validate the model on a model validation, or test data set, held out for this purpose. Hide D Details. Out of sample validation procedure Using the model development subsample, fit a candidate model. Using the Step (ii) model and the explanatory variables from the validation subsample, “predict” the dependent variables in the validation subsample, \\(\\hat{y}_i\\), where \\(i=n_{1}+1,...,n_{1}+n_{2}\\). Calc the *sum of absolute prediction errors** \\[SAPE=\\sum_{i=n_{1}+1}^{n_{1}+n_{2}} |y_{i}-\\hat{y}_{i}| . \\] Repeat Steps (i) through (iii) for each candidate model. Choose the model with the smallest SAPE. Hide E Details. Cross - validation With out-of-sample validation, the statistic depends on a random split between in-sample and out-of-sample data (a problem for data sets that are not large) Alternatively, one may use cross-validation Use a random mechanism to split the data into k subsets, (e.g., 5-10) Use the first k-1 subsamples to estimate model parameters. Then, “predict” the outcomes for the kth subsample and use SAE to summarize the fit Repeat this by holding out each of the k sub-samples, summarizing with a cumulative SAE. Repeat these steps for several candidate models. Choose the model with the lowest cumulative SAE statistic. 4.6.2 Exercise. Cross-validation and term life Assignment Text Here is some sample code to give you a better feel for cross-validation. The first part of the randomly re-orders (“shuffles”) the data. It also identifies explanatory variables explvars. The function starts by pulling out only the needed data into cvdata. Then, for each subsample, a model is fit based on all the data except for the subsample, in train_mlr with the subsample in test. This is repeated for each subsample, then results are summarized. Show Code # Randomly re-order data - &quot;shuffle it&quot; n &lt;- nrow(Term1) set.seed(12347) shuffled_Term1 &lt;- Term1[sample(n), ] explvars &lt;- c(&quot;education&quot;, &quot;numhh&quot;, &quot;logincome&quot;) ## Cross - Validation crossvalfct &lt;- function(explvars){ cvdata &lt;- shuffled_Term1[, c(&quot;logface&quot;, explvars)] crossval &lt;- 0 k &lt;- 5 for (i in 1:k) { indices &lt;- (((i-1) * round((1/k)*nrow(cvdata))) + 1):((i*round((1/k) * nrow(cvdata)))) # Exclude them from the train set train_mlr &lt;- lm(logface ~ ., data = cvdata[-indices,]) # Include them in the test set test &lt;- data.frame(cvdata[indices, explvars]) names(test) &lt;- explvars predict_test &lt;- exp(predict(train_mlr, test)) # Compare predicted to held-out and summarize predict_err &lt;- exp(cvdata[indices, &quot;logface&quot;]) - predict_test crossval &lt;- crossval + sum(abs(predict_err)) } crossval/1000 } crossvalfct(explvars) Instructions Calculate the cross-validation statistic using only logarithmic income, logincome. Calculate the cross-validation statistic using logincome, education and numhh. Calculate the cross-validation statistic using logincome, education, numhh and marstat. The best model has the lowest cross-validation statistic. Hint. The function [sample()] is for taking random samples. We use it without replacement so it results in a re-ordering of data. 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 "],
["interpreting-regression-results.html", "Chapter 5 Interpreting Regression Results 5.1 Case study: MEPS health expenditures 5.2 What the modeling procedure tells us 5.3 The importance of variable selection", " Chapter 5 Interpreting Regression Results Chapter description A case study, on determining an individual’s characteristics that influence its health expenditures, illustrates the regression modeling process from start to finish. Subsequently, the chapter summarizes what we learn from the modeling process, underscoring the importance of variable selection. 5.1 Case study: MEPS health expenditures 5.1.1 Video Video Overhead Details A Details. MEPS health expenditures B Details. Overhead MEPS health expenditures C Details. Outcome variable D Details. Explanatory variables E Details. Case study outline Hide A Details. MEPS health expenditures This exercise considers data from the Medical Expenditure Panel Survey (MEPS), conducted by the U.S. Agency of Health Research and Quality. MEPS is a probability survey that provides nationally representative estimates of health care use, expenditures, sources of payment, and insurance coverage for the U.S. civilian population. This survey collects detailed information on individuals of each medical care episode by type of services including physician office visits, hospital emergency room visits, hospital outpatient visits, hospital inpatient stays, all other medical provider visits, and use of prescribed medicines. This detailed information allows one to develop models of health care utilization to predict future expenditures. You can learn more about MEPS at http://www.meps.ahrq.gov/mepsweb/. We consider MEPS data from the panels 7 and 8 of 2003 that consists of 18,735 individuals between ages 18 and 65. From this sample, we took a random sample of 2,000 individuals that appear in the file HealthExpend. From this sample, there are 1,352 that had positive outpatient expenditures. Our dependent variable is the amount of expenditures for outpatient visits, expendop. For MEPS, outpatient events include hospital outpatient department visits, office-based provider visits and emergency room visits excluding dental services. (Dental services, compared to other types of health care services, are more predictable and occur in a more regular basis.) Hospital stays with the same date of admission and discharge, known as “zero-night stays,” were included in outpatient counts and expenditures. (Payments associated with emergency room visits that immediately preceded an inpatient stay were included in the inpatient expenditures. Prescribed medicines that can be linked to hospital admissions were included in inpatient expenditures, not in outpatient utilization.) Hide B Details. Overhead MEPS health expenditures Data from the Medical Expenditure Panel Survey (MEPS), conducted by the U.S. Agency of Health Research and Quality (AHRQ). A probability survey that provides nationally representative estimates of health care use, expenditures, sources of payment, and insurance coverage for the U.S. civilian population. Collects detailed information on individuals of each medical care episode by type of services including physician office visits, hospital emergency room visits, hospital outpatient visits, hospital inpatient stays, all other medical provider visits, and use of prescribed medicines. This detailed information allows one to develop models of health care utilization to predict future expenditures. We consider MEPS data from the first panel of 2003 and take a random sample of n = 2, 000 individuals between ages 18 and 65. Hide C Details. Outcome variable Our dependent variable is expenditures for outpatient admissions. For MEPS, inpatient admissions include persons who were admitted to a hospital and stayed overnight. In contrast, outpatient events include hospital outpatient department visits, office-based provider visits and emergency room visits excluding dental services. Hospital stays with the same date of admission and discharge, known as “zero-night stays,” were included in outpatient counts and expenditures. Payments associated with emergency room visits that immediately preceded an inpatient stay were included in the inpatient expenditures. Prescribed medicines that can be linked to hospital admissions were included in inpatient expenditures, not in outpatient utilization. Hide D Details. Explanatory variables 9 variables in the database. Here 13 most relevant. \\[ {\\small \\begin{array}{ll} expendop &amp; \\text{Amounts of expenditures for outpatient visits} \\\\ gender &amp; \\text{Indicate gender of patient (=1 if female, =0 if male)} \\\\ age &amp; \\text{Age in years between 18 and 65 }\\\\ race &amp; \\text{Race of patient described as Asian, Black, Native, White and other} \\\\ region &amp; \\text{Region of patient described as WEST, NORTHEAST, MIDWEST and SOUTH} \\\\ educ &amp; \\text{Level of education received described by words (LHIGHSC, HIGHSCH and COLLEGE)} \\\\ phstat &amp; \\text{Self-rated physical health status described as EXCE, VGOO, GOOD, FAIR and POOR} \\\\ mpoor &amp; \\text{Self-rated mental health (=1 if poor or fair, =0 if good to excellent mental health)} \\\\ anylimit &amp; \\text{Any activity limitation (=1 if any functional/activity limitation, =0 if otherwise)} \\\\ income &amp; \\text{Income compared to poverty line described as POOR, NPOOR, LINCOME, MINCOME and HINCOME} \\\\ insure &amp; \\text{Insurance coverage (=1 if covered by public/private health insurance in any month of 1996, =0 otherwise)} \\\\ usc &amp; \\text{1 if dissatisfied with one&#39;s usual source of care} \\\\ unemploy &amp; \\text{Employment status of patients} \\\\ managedcare &amp; \\text{1 if enrolled in an HMO or gatekeeper plan} \\\\ \\end{array}} \\] Hide E Details. Case study outline The next series of exercises leads you through an analysis of the steps for understanding a complex data set. Because of the complexity of the data, in each step only a sample of procedures will be executed. The outline consists of: Summary statistics Splitting the data into training and testing portions with initial model fits Selecting variables to be included in the model 5.1.2 Exercise. Summarizing data Assignment Text With a complex dataset, you will probably want to take a look at the structure of the data. You are already familiar with taking a [summary()] of a dataframe which provides summary statistics for many variables. You will see that several variables in this dataframe are categorical, or factor, variables. We can use the table() function to summarize them. After getting a sense of the distributions of explanatory variables, we want to take a deeper dive into the distribution of the outcome variable, expendop. We will do this by comparing the histograms of the variable to that of its logarithmic version. To examine relationships of the outcome variable visually, we look to scatterplots for continuous variables (such as age) and boxplots for categorical variables (such as phstat). Instructions Examine the structure of the meps dataframe using the str() function. Also, get a [summary()] of the dataframe. Examine the distribution of the race variable using the table() function. Compare the expenditures distribution to its logarithmic version visually via histograms plotted next to another. par(mfrow = c(1, 2)) is used to organize the plots you create. Examine the distribution of logarithmic expenditures in terms of levels of phstat visually using the boxplot() function. Examine the relationship of age versus logarithmic expenditures using a scatter plot. Superimpose a local fitting line using the lines() and lowess() functions. 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 5.1.3 Exercise. Fit a benchmark multiple linear regression model Assignment Text As part of the pre-processing for the model fitting, we will split the data into training and test subsamples. For this exercise, we use a 75/25 split although other choices are certainly suitable. Some analysts prefer to do this splitting before looking at the data. Another approach, adopted here, is that the final report typically contains summary statistcs of the entire data set and so it makes sense to do so when examining summary statistics. We start by fitting a benchmark model. It is common to use all available explanatory variables with the outcome on the original scale and so we use this as our benchmark model. This exercise shows that when you plot() a fitted linear regression model in R, the result provides four graphs that you have seen before. These can be useful for identifying an appropriate model. Instructions Randomly split the data into a training and a testing data sets. Use 75% for the training, 25% for the testing. Fit a full model using expendop as the outcome and all explanatory variables. Summarize the results of this model fitting. You can plot() the fitted model to view several diagnostic plots. These plots provide evidence that expenditures may not be the best scale for linear regression. Fit a full model using logexpend as the outcome and all explanatory variables and summarize the fit. Use the plot() function for evidence that this variable is more suited for linear regression methods than expenditures on the original scale. Hint. A plot of a regression object such as plot(mlr) provides four diagnostic plots. These can be organized as a 2 by 2 array using par(mfrow = c(2, 2)). eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6IiNtZXBzIDwtIHJlYWQuY3N2KFwiQ1NWRGF0YVxcXFxIZWFsdGhNZXBzLmNzdlwiLCBoZWFkZXIgPSBUUlVFKVxubWVwcyA8LSByZWFkLmNzdihcImh0dHBzOi8vYXNzZXRzLmRhdGFjYW1wLmNvbS9wcm9kdWN0aW9uL3JlcG9zaXRvcmllcy8yNjEwL2RhdGFzZXRzLzdiN2RhYjZkMGM1MjhlNGNkMmY4ZDBlMGZjNzgyNGEyNTQ0MjliZjgvSGVhbHRoTWVwcy5jc3ZcIiwgaGVhZGVyID0gVFJVRSlcbm1lcHMkbG9nZXhwZW5kIDwtIGxvZyhtZXBzJGV4cGVuZG9wKSIsInNhbXBsZSI6IiMgUmFuZG9tbHkgc3BsaXQgdGhlIGRhdGEgaW50byBhIHRyYWluaW5nIGFuZCBhIHRlc3RpbmcgZGF0YSBzZXRzLiBVc2UgNzVcXCUgZm9yIHRoZSB0cmFpbmluZywgMjVcXCUgZm9yIHRoZSB0ZXN0aW5nLlxubiA8LSBucm93KG1lcHMpXG5zZXQuc2VlZCgxMjM0NylcbnNodWZmbGVkX21lcHMgPC0gbWVwc1tzYW1wbGUobiksIF1cbnRyYWluX2luZGljZXMgPC0gMTpyb3VuZCgwLjc1ICogbilcbnRyYWluX21lcHMgICAgPC0gc2h1ZmZsZWRfbWVwc1t0cmFpbl9pbmRpY2VzLCBdXG50ZXN0X2luZGljZXMgIDwtIChyb3VuZCgwLjI1ICogbikgKyAxKTpuXG50ZXN0X21lcHMgICAgIDwtIHNodWZmbGVkX21lcHNbdGVzdF9pbmRpY2VzLCBdXG5cbiMgRml0IGEgZnVsbCBtb2RlbCB1c2luZyBgZXhwZW5kb3BgIGFzIHRoZSBvdXRjb21lIGFuZCBhbGwgZXhwbGFuYXRvcnkgdmFyaWFibGVzLiBTdW1tYXJpemUgdGhlIHJlc3VsdHMgb2YgdGhpcyBtb2RlbCBmaXR0aW5nLlxubWVwc19tbHIxIDwtIGxtKF9fXyB+IGdlbmRlciArIGFnZSArIHJhY2UgKyByZWdpb24gKyBlZHVjICsgcGhzdGF0ICsgbXBvb3IgKyBhbnlsaW1pdCArIGluY29tZSArIGluc3VyZSArIHVzYyArIHVuZW1wbG95ICsgbWFuYWdlZGNhcmUsIGRhdGEgPSBfX18pXG5zdW1tYXJ5KG1lcHNfbWxyMSlcblxuIyBQcm92aWRlIGRpYWdub3N0aWMgcGxvdHMgb2YgdGhlIGZpdHRlZCBtb2RlbC4gXG5wYXIobWZyb3cgPSBjKDIsIDIpKVxucGxvdChfX18pXG5cbiMgRml0IGEgZnVsbCBtb2RlbCB1c2luZyBgbG9nZXhwZW5kYCBhcyB0aGUgb3V0Y29tZSBhbmQgYWxsIGV4cGxhbmF0b3J5IHZhcmlhYmxlcy4gU3VtbWFyaXplIHRoZSBmaXQgYW5kIGV4YW1pbmUgZGlhZ25vc3RpYyBwbG90cyBvZiB0aGUgZml0dGVkIG1vZGVsLiBcbm1lcHNfbWxyMiA8LSBsbShfX18gfiBnZW5kZXIgKyBhZ2UgKyByYWNlICsgcmVnaW9uICsgZWR1YyArIHBoc3RhdCArIG1wb29yICsgYW55bGltaXQgKyBpbmNvbWUgKyBpbnN1cmUgKyB1c2MgKyB1bmVtcGxveSArIG1hbmFnZWRjYXJlLCBkYXRhID0gX19fKVxuc3VtbWFyeShtZXBzX21scjIpXG5wbG90KG1lcHNfbWxyMikiLCJzb2x1dGlvbiI6IiMgU3BsaXQgdGhlIHNhbXBsZSBpbnRvIGEgYHRyYWluaW5nYCBhbmQgYHRlc3RgIGRhdGFcbm4gPC0gbnJvdyhtZXBzKVxuc2V0LnNlZWQoMTIzNDcpXG5zaHVmZmxlZF9tZXBzIDwtIG1lcHNbc2FtcGxlKG4pLCBdXG50cmFpbl9pbmRpY2VzIDwtIDE6cm91bmQoMC43NSAqIG4pXG50cmFpbl9tZXBzICAgIDwtIHNodWZmbGVkX21lcHNbdHJhaW5faW5kaWNlcywgXVxudGVzdF9pbmRpY2VzICA8LSAocm91bmQoMC4yNSAqIG4pICsgMSk6blxudGVzdF9tZXBzICAgICA8LSBzaHVmZmxlZF9tZXBzW3Rlc3RfaW5kaWNlcywgXVxuXG5tZXBzX21scjEgPC0gbG0oZXhwZW5kb3AgfiBnZW5kZXIgKyBhZ2UgKyByYWNlICsgcmVnaW9uICsgZWR1YyArIHBoc3RhdCArIG1wb29yICsgYW55bGltaXQgKyBpbmNvbWUgKyBpbnN1cmUgKyB1c2MgKyB1bmVtcGxveSArIG1hbmFnZWRjYXJlLCBkYXRhID0gdHJhaW5fbWVwcylcbnN1bW1hcnkobWVwc19tbHIxKVxucGFyKG1mcm93ID0gYygyLCAyKSlcbnBsb3QoeD1tZXBzX21scjEpXG5cbm1lcHNfbWxyMiA8LSBsbShsb2dleHBlbmQgfiBnZW5kZXIgKyBhZ2UgKyByYWNlICsgcmVnaW9uICsgZWR1YyArIHBoc3RhdCArIG1wb29yICsgYW55bGltaXQgKyBpbmNvbWUgKyBpbnN1cmUgKyB1c2MgKyB1bmVtcGxveSArIG1hbmFnZWRjYXJlLCBkYXRhID0gdHJhaW5fbWVwcylcbnN1bW1hcnkobWVwc19tbHIyKVxucGxvdChtZXBzX21scjIpIiwic2N0IjoiZXgoKSAlPiUgY2hlY2tfb2JqZWN0KG1lcHNfbWxyMSkgJT4lIGNoZWNrX2VxdWFsKClcbmV4KCkgJT4lIGNoZWNrX2Z1bmN0aW9uKFwic3VtbWFyeVwiLGluZGV4PTEpICU+JSBjaGVja19hcmcoLiwgXCJvYmplY3RcIikgJT4lIGNoZWNrX2VxdWFsKClcbmV4KCkgJT4lIGNoZWNrX2Z1bmN0aW9uKFwicGFyXCIpICU+JSBjaGVja19hcmcoLiwgXCJtZnJvd1wiKSAlPiUgY2hlY2tfZXF1YWwoKVxuZXgoKSAlPiUgY2hlY2tfZnVuY3Rpb24oXCJwbG90XCIsaW5kZXg9MSkgJT4lIGNoZWNrX2FyZyguLCBcInhcIikgJT4lIGNoZWNrX2VxdWFsKClcbmV4KCkgJT4lIGNoZWNrX29iamVjdChcIm1lcHNfbWxyMlwiKSAlPiUgY2hlY2tfZXF1YWwoKVxuZXgoKSAlPiUgY2hlY2tfZnVuY3Rpb24oXCJzdW1tYXJ5XCIsaW5kZXg9MSkgJT4lIGNoZWNrX2FyZyguLCBcIm9iamVjdFwiKSAlPiUgY2hlY2tfZXF1YWwoKVxuZXgoKSAlPiUgY2hlY2tfZnVuY3Rpb24oXCJwbG90XCIsaW5kZXg9MikgJT4lIGNoZWNrX2FyZyguLCBcInhcIikgJT4lIGNoZWNrX2VxdWFsKClcbnN1Y2Nlc3NfbXNnKFwiRXhjZWxsZW50ISBZb3UgbWF5IGhhdmUgY29tcGFyZWQgdGhlIGZvdXIgZGlhZ25vc3RpYyBncmFwaHMgZnJvbSB0aGUgTUxSIG1vZGVsIGZpdCBvZiAnZXhwZW5kJyB0byB0aG9zZSBjcmVhdGVkIHVzaW5nIHRoZSBzYW1lIHByb2NlZHVyZSBidXQgd2l0aCBsb2dhcml0aG1pYyBleHBlbmRpdHVyZXMgYXMgdGhlIG91dGNvbWUuIFRoaXMgcHJvdmlkZXMgYW5vdGhlciBwaWVjZSBvZiBldmlkZW5jZSB0aGF0IGxvZyBleHBlbmRpdHVyZXMgYXJlIG1vcmUgc3VpdGFibGUgZm9yIHJlZ3Jlc3Npb24gbW9kZWxpbmcuIFVzaW5nIGxvZ2FyaXRobWljIG91dGNvbWVzIGlzIGEgY29tbW9uIGZlYXR1cmUgb2YgYWN0dWFyaWFsIGFwcGxpY2F0aW9ucyBidXQgY2FuIGJlIGRpZmZpY3VsdCB0byBkaWFnbm9zZSBhbmQgaW50ZXJwcmV0IHdpdGhvdXQgcHJhY3RpY2UuXCIpIn0= 5.1.4 Exercise. Variable selection Assignment Text Modeling building can be approached using a “ground-up” strategy, where the analyst introduces a variable, examines residuls from a regression fit, and then seeks to understand the relationship between these residuals and other available variables so that these variables might be added to the model. Another approach is a “top-down” strategy where all available variables are entered into a model and unnecessary variables are pruned from the model. Both approaches are helpful when using data to specify models. This exercise illustrates the latter approach, using the [step()] function to help narrow our search for the best fitting model. Instructions From our prior work, the training dataframe train_meps has already been loaded in. A multiple linear regression model fit object meps_mlr2 is available that summarizes a fit of logexpend as the outcome variable using all 13 explanatory variables. Use the step() function function to drop unnecessary variables from the full fitted model summarized in the object meps_mlr2 and summarize this recommended model. As an alternative, use the explanatory variables in the recommended model and add the varibles phstat. Summarize the fit and note that statistical significance of the new variable. You have been reminded by your boss that use of the variable gender is unsuitable for actuarial pricing purposes. As an another alternative, drop gender from the recommended model (still keeping phstat). Note the statistical significance of the variable uscwith this fitted model. Hint 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 5.1.5 Exercise. Model comparisons using cross-validation Assignment Text To compare alternative models, you decide to utilize cross-validation. For this exercise, you split the training sample into six subsamples of approximately equal size. In the sample code, the cross-validation procedure has been summarized into a function that you can call. The input to the function is a list of variables that you select as your model explanatory variables. With this function, you can readily test several candidate models. Instructions Run the cross validation (crossvalfct) function using the explanatory variables suggested by the stepwise function. Run the function again but adding the mpoor variable Run the function again but omitting the gender variable Note which model is suggested by the cross validation function. Hint. The cross validation function of this is very similar to the one we did earlier. Different number of subsamples, different test/training data and a different outcome variable. Except for these minor changes, it is the same function that we worked with earlier. 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 5.1.6 Exercise. Out of sample validation Assignment Text From our prior work, the training train_meps and test test_meps dataframes have already been loaded in. We think our best model is based on logarithmic expenditures as the outcome and the following explanatory variables: explvars3 &lt;- c(&quot;gender&quot;, &quot;age&quot;, &quot;phstat&quot;, &quot;anylimit&quot;, &quot;insure&quot;, &quot;mpoor&quot;) We will compare this to a benchmark model that is based on expenditures as the outcome and all 13 explanatory variables explvars4 &lt;- c(explvars3, &quot;race&quot;, &quot;income&quot;, &quot;region&quot;, &quot;educ&quot;, &quot;unemploy&quot;, &quot;managedcare&quot;, &quot;usc&quot;) The comparisons will be based on expenditures in dollars using the held-out validation sample. Instructions Use the training sample to fit a linear model with logexpend and explanatory variables listed in explvars3 Predict expenditures (not logged) for the test data and summarize the fit using the sum of absolute prediction errors. Use the training sample to fit a benchmark linear model with expendop and explanatory variables listed in explvars4 Predict expenditures for the test data and summarize the fit for the benchmark model using the sum of absolute prediction errors. Compare the predictions of the models graphically. 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 5.2 What the modeling procedure tells us In this section, you learn how to: Interpret individual effects, based on their substantive and statistical significance Describe other purposes of regression modeling, including regression function for pricing, benchmarking studies, and predicting future observations. 5.2.1 Video Video Overhead Details A Details. Interpreting individual effects B Details. Other Interpretations Hide A Details. Interpreting individual effects Substantive Effect Does a 1 unit change in \\(x\\) imply an economically meaningful change in \\(y\\)? Example: Looking at urban and rural claims experience, is there a big enough difference to warrant differentiating prices by location? Statistical Significance We have standards for deciding whether or not a variable is statistically significant. A “statistically significant effect” is the result of a regression coefficient that is large relative to its standard error. Statistical significance is driven by precision of \\(s\\), collinearity (\\(VIF\\)) and sample size Causal Effects If we change \\(x\\), would \\(y\\) change? Hide B Details. Other Interpretations Regression function and pricing The regression function is \\(\\mathrm{E~}y = \\beta_0 + \\beta_1 x_1 + \\cdots +\\beta _k x_k\\). Think about expected claims as our baseline price for short-term insurance coverages. Benchmarking studies In studies of CEO’s salaries, who is making a lot (or a little), controlled for industry, years of experience and so forth? In studies of medical claims, who are the high-cost patients? Prediction A new patient comes in with a given set of characteristics, what can I say about his or her future medical claims? MC Exercise. Which of the following are not important when interpreting the effects of individual variables? Substantive significance Statistical significance The amount of effort that it took to gather the data and do the analysis Role of causality Submit Answer MC Exercise. Which of the following is not a potential explanation for the lack of statistical significance of an explanatory variable? Large variation of the disturbance term High collinearity, so that the variable may be confounded with other variables The coefficient of determination, \\(R^2\\), is not sufficiently large Submit Answer MC Exercise. Which of the following is not an important purpose of regression modeling? Pricing of risks such as insurance contracts Benchmarking studies, to compare an observation to others Prediction Keeping a computer occupied with work Submit Answer 5.3 The importance of variable selection In this section, you learn how to: Describe the bias that can occur when omitting important variables Describe the principle of parsimony and reasons for adopting this approach 5.3.1 Video Video Overhead Details A Details. The importance of variable selection B Details. Example. Regression using one explanatory variable Hide A Details. The importance of variable selection With too many or too few variables, \\(s\\) is too large an estimate of \\(\\sigma\\). Prediction intervals are too large Standard errors for the partial slopes are too large With too few or incorrect variables, we produce biased estimates of the slopes \\(\\beta\\). Thus, our predictions are biased and hence inaccurate. Hide B Details. Example. Regression using one explanatory variable Too Many Variables The “true” model is \\(y_i = \\beta_0+ \\varepsilon_i\\) We mistakenly use \\(y_i = \\beta_0+ \\beta_1 x_i^* + \\varepsilon_i\\) The prediction at a generic level \\(x\\) is \\(b_0^* + b_1^* x\\). It is not to hard to confirm that \\(Bias = \\mathrm{E} (b_0^* + b_1^* x) - \\mathrm{E } y= 0\\). Too Few Variables The “true” model is \\(y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\\). We mistakenly use \\(y_i = \\beta_0^* \\varepsilon_i\\). Under the true model, \\(\\overline{y} = \\beta_0 + \\beta_1 \\overline{x} + \\overline{\\varepsilon}\\) Thus, the bias is \\[ Bias = \\text{E }\\bar{y} - \\text{E }(\\beta_0 + \\beta_1 x + \\varepsilon) \\\\ = \\text{E }(\\beta_0 + \\beta_1 \\bar{x}+\\bar{\\varepsilon})-(\\beta_0+\\beta_1 x)=\\beta_1 (\\bar{x}-x). \\] There is a persistent, long-term error in omitting the explanatory variable \\(x\\). Hide C Details. Principle of parsimony The principle of parsimony, also known as Occam’s Razor, states that when there are several possible explanations for a phenomenon, use the simplest. A simpler explanation is easier to interpret. Simpler models, also known as ``more parsimonious’’ models, often do well on fitting out-of-sample data Extraneous variables can cause problems of collinearity, leading to difficulty in interpreting individual coefficients. In contrast, in a quote often attributed to Albert Einstein, we should use “the simplest model possible, but no simpler.” Omitting important variables can lead to biased results, a potentially serious error. Including extraneous variables decreases the degrees of freedom and increases the estimate of variability, typically of less concern in actuarial applications. MC Exercise. Which of the following is true about under- and over-fitting a model? When we over-fit a model, estimates of regression coefficients are over-biased as is \\(s^2\\), the estimate of model variance \\(\\sigma^2\\). When we over-fit a model, estimates of regression coefficients remain unbiased whereas \\(s^2\\), the estimate of model variance \\(\\sigma^2\\), is over-biased. When we over-fit a model, estimates of regression coefficients remain under-biased as is \\(s^2\\), the estimate of model variance \\(\\sigma^2\\). When we under-fit a model, estimates of regression coefficients remain unbiased whereas \\(s^2\\), the estimate of model variance \\(\\sigma^2\\), is over-biased. Submit Answer MC Exercise. Which of the following is not true of Occam’s Razor? When there are several possible explanations for a phenomenon, use the simplest one. Simpler models are easier to interpret. Variables can be statistically significant but practically unimportant. Simpler models often do better for predicting out-of-sample data Submit Answer "]
]
