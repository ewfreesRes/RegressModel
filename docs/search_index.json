[
["index.html", "Online Tutorial on Regression Modeling with Actuarial and Financial Applications Preface", " Online Tutorial on Regression Modeling with Actuarial and Financial Applications Edward W. (Jed) Frees, University of Wisconsin-Madison Preface Date: 23 October 2018 About Regression Modeling Statistical techniques can be used to address new situations. This is important in a rapidly evolving risk management world. Analysts with a strong analytical background understand that a large data set can represent a treasure trove of information to be mined and can yield a strong competitive advantage. This book and online tutorial provides budding analysts with a foundation in multiple reression. Viewers will learn about these statistical techniques using data on the demand for insurance, healthcare expenditures, and other applications. Although no specific knowledge of actuarial or risk management is presumed, the approach introduces applications in which statistical techniques can be used to analyze real data of interest. Resources This tutorial is based on the book Regression Modeling with Actuarial and Financial Applications. For resources associated with the book, please visit the Regression Modeling book web site. For advanced regression applications in insurance, you may be interested in the series, Predictive Modeling Applications in Actuarial Science. Sample code and data for the series are available at series website. An earlier version of this tutorial, a Short Course constructed for Indonesian actuaries, uses the Datacamp learning platform. Tutorial Description This online tutorial is designed to guide you through the foundations of regession with applications in actuarial science. Anticipated completion time is approximately six hours. The tutorial assumes that you are familiar with the foundations in the statistical software R, such as Datacamp’s Introduction to R. General Layout. There are five chapters in this tutorial that summarize the foundations of multiple linear regression. Each chapter is subdivided into several sections. At the beginning of each section is a short video, typically 4-8 minutes, that summarizes the section key learning outcomes. Following the video, you can see more details about the underlying R code for the analysis presented in the video. Role of Exercises. Following each video, there are one or two exercises that allow you to practice skills to make sure that you fully grasp the learning outcomes. The exercises are implented using an online learning platfor provided by Datacamp so that you need not install R. Feedback is programmed into the exercises so that you will learn a lot by making mistakes! You will be pacing yourself, so always feel free to reveal the answers by hitting the Solution tab. Remember, going through quickly is not equivalent to learning deeply. Use this tool to enhance your understanding of one of the foundations of data science, regression analysis. Welcome to the Tutorial Video In this video, you learn how to: Describe regression briefly, i.e., in a nutshell Explain Galton’s height example as a regression application Video Overhead Show Overhead A. Galton’s 1885 Regression Data \\[ \\small{\\begin{array}{l|ccccccccccc|c} \\hline \\text{Height of }&amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\\\ \\text{adult child }&amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\\\ \\text{in inches }&amp; &lt;64.0 &amp; 64.5 &amp; 65.5 &amp; 66.5 &amp; 67.5 &amp; 68.5 &amp; 69.5 &amp; 70.5 &amp; 71.5 &amp; 72.5 &amp; &gt;73.0 &amp; \\text{Totals} \\\\ \\hline &gt;73.7 &amp; - &amp; - &amp; - &amp; - &amp; - &amp; - &amp; 5 &amp; 3 &amp; 2 &amp; 4 &amp; - &amp; 14 \\\\ 73.2 &amp; - &amp; - &amp; - &amp; - &amp; - &amp; 3 &amp; 4 &amp; 3 &amp; 2 &amp; 2 &amp; 3 &amp; 17 \\\\ 72.2 &amp; - &amp; - &amp; 1 &amp; - &amp; 4 &amp; 4 &amp; 11 &amp; 4 &amp; 9 &amp; 7 &amp; 1 &amp; 41 \\\\ 71.2 &amp; - &amp; - &amp; 2 &amp; - &amp; 11 &amp; 18 &amp; 20 &amp; 7 &amp; 4 &amp; 2 &amp; - &amp; 64 \\\\ 70.2 &amp; - &amp; - &amp; 5 &amp; 4 &amp; 19 &amp; 21 &amp; 25 &amp; 14 &amp; 10 &amp; 1 &amp; - &amp; 99 \\\\ 69.2 &amp; 1 &amp; 2 &amp; 7 &amp; 13 &amp; 38 &amp; 48 &amp; 33 &amp; 18 &amp; 5 &amp; 2 &amp; - &amp; 167 \\\\ 68.2 &amp; 1 &amp; - &amp; 7 &amp; 14 &amp; 28 &amp; 34 &amp; 20 &amp; 12 &amp; 3 &amp; 1 &amp; - &amp; 120 \\\\ 67.2 &amp; 2 &amp; 5 &amp; 11 &amp; 17 &amp; 38 &amp; 31 &amp; 27 &amp; 3 &amp; 4 &amp; - &amp; - &amp; 138 \\\\ 66.2 &amp; 2 &amp; 5 &amp; 11 &amp; 17 &amp; 36 &amp; 25 &amp; 17 &amp; 1 &amp; 3 &amp; - &amp; - &amp; 117 \\\\ 65.2 &amp; 1 &amp; 1 &amp; 7 &amp; 2 &amp; 15 &amp; 16 &amp; 4 &amp; 1 &amp; 1 &amp; - &amp; - &amp; 48 \\\\ 64.2 &amp; 4 &amp; 4 &amp; 5 &amp; 5 &amp; 14 &amp; 11 &amp; 16 &amp; - &amp; - &amp; - &amp; - &amp; 59 \\\\ 63.2 &amp; 2 &amp; 4 &amp; 9 &amp; 3 &amp; 5 &amp; 7 &amp; 1 &amp; 1 &amp; - &amp; - &amp; - &amp; 32 \\\\ 62.2 &amp; - &amp; 1 &amp; - &amp; 3 &amp; 3 &amp; - &amp; - &amp; - &amp; - &amp; - &amp; - &amp; 7 \\\\ &lt;61.2 &amp; 1 &amp; 1 &amp; 1 &amp; - &amp; - &amp; 1 &amp; - &amp; 1 &amp; - &amp; - &amp; - &amp; 5 \\\\ \\hline \\text{Totals }&amp; 14 &amp; 23 &amp; 66 &amp; 78 &amp; 211 &amp; 219 &amp; 183 &amp; 68 &amp; 43 &amp; 19 &amp; 4 &amp; 928 \\\\ \\hline \\end{array}} \\] Show Overhead B. Supporting R Code # Reformat Data Set #heights &lt;- read.csv(&quot;CSVData\\\\GaltonFamily.csv&quot;,header = TRUE) heights &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/c85ede6c205d22049e766bd08956b225c576255b/galton_height.csv&quot;, header = TRUE) str(heights) head(heights) heights$child_ht &lt;- heights$CHILDC heights$parent_ht &lt;- heights$PARENTC heights2 &lt;- heights[c(&quot;child_ht&quot;,&quot;parent_ht&quot;)] #heights &lt;- read.csv(&quot;CSVData\\\\galton_height.csv&quot;,header = TRUE) heights &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/c85ede6c205d22049e766bd08956b225c576255b/galton_height.csv&quot;, header = TRUE) plot(jitter(heights$parent_ht),jitter(heights$child_ht), ylim = c(60,80), xlim = c(60,80), ylab = &quot;height of child&quot;, xlab = &quot;height of parents&quot;) abline(lm(heights$child_ht~heights$parent_ht)) abline(0,1,col = &quot;red&quot;, lty=2) summary(lm(heights$child_ht~heights$parent_ht)) Call: lm(formula = heights$child_ht ~ heights$parent_ht) Residuals: Min 1Q Median 3Q Max -8.258 -1.428 0.132 1.572 5.792 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 25.8486 2.6901 9.61 &lt;0.0000000000000002 *** heights$parent_ht 0.6099 0.0388 15.71 &lt;0.0000000000000002 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 2.26 on 926 degrees of freedom Multiple R-squared: 0.21, Adjusted R-squared: 0.21 F-statistic: 247 on 1 and 926 DF, p-value: &lt;0.0000000000000002 "],
["regression-and-the-normal-distribution.html", "Chapter 1 Regression and the Normal Distribution 1.1 Fitting a normal distribution 1.2 Visualizing distributions 1.3 Summarizing distributions 1.4 Transformations", " Chapter 1 Regression and the Normal Distribution Chapter description Regression analysis is a statistical method that is widely used in many fields of study, with actuarial science being no exception. This chapter introduces the role of the normal distribution in regression and the use of logarithmic transformations in specifying regression relationships. 1.1 Fitting a normal distribution In this section, you learn how to: Calculate and interpret two basic summary statistics Fit a data set to a normal curve Calculate probabilities under a standard normal curve 1.1.1 Video Video Overhead Details Show Overhead A Details. Description of the data To illustrate a data set that can be analyzed using regression methods, we consider some data included in Galton’s 1885 paper. These data include the heights of 928 adult children (child_ht), together with an index of their parents’ height (parent_ht). Here, all female heights were multiplied by 1.08, and the index was created by taking the average of the father’s height and rescaled mother’s height. Galton was aware that the parents’ and the adult child’s height could each be adequately approximated by a normal curve. In developing regression analysis, he provided a single model for the joint distribution of heights. heights &lt;- read.csv(&quot;CSVData\\\\galton_height.csv&quot;, header = TRUE) #heights &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/c85ede6c205d22049e766bd08956b225c576255b/galton_height.csv&quot;, header = TRUE) plot(jitter(heights$parent_ht),jitter(heights$child_ht), ylim = c(60,80), xlim = c(60,80), ylab = &quot;height of child&quot;, xlab = &quot;height of parents&quot;) abline(lm(heights$child_ht~heights$parent_ht)) abline(0,1,col = &quot;red&quot;) Show Overhead B Details. Read and examine data structure The data has already been read into a dataset called heights. Examine the structure of the data with the function str() and use the head() command to looks at the first few records. heights &lt;- read.csv(&quot;CSVData\\\\galton_height.csv&quot;,header = TRUE) #heights &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/c85ede6c205d22049e766bd08956b225c576255b/galton_height.csv&quot;, header = TRUE) str(heights) head(heights) &#39;data.frame&#39;: 928 obs. of 2 variables: $ child_ht : num 72.2 73.2 73.2 73.2 68.2 ... $ parent_ht: num 74.5 74.5 74.5 74.5 73.5 73.5 73.5 73.5 73.5 73.5 ... child_ht parent_ht 1 72.2 74.5 2 73.2 74.5 3 73.2 74.5 4 73.2 74.5 5 68.2 73.5 6 69.2 73.5 Show Overhead C Details. Summary stats for parents’ height Next, examine the distribution of the child’s height and then examine the distribution of the parents height. ht_par &lt;- heights$parent_ht hist(ht_par) mean(ht_par) sd(ht_par) [1] 69.26 [1] 1.912 Show Overhead D. Fit a normal curve to parents’ height details (mparent &lt;- mean(ht_par)) (sdparent &lt;- sd(ht_par)) x &lt;- seq(60, 80,by = 0.1) hist(ht_par, freq = FALSE) lines(x, dnorm(x, mean = mparent, sd = sdparent), col = &quot;blue&quot;) [1] 69.26 [1] 1.912 Show Overhead E Details. Use the normal approximation to determine the probability of the height of tall parents TallHeight &lt;- 72 pnorm(TallHeight, mean = mparent, sd = sdparent) pnorm(72, mean = mean(ht_par), sd = sd(ht_par)) (StdUnitsTallHeight &lt;- (TallHeight - mparent)/sdparent) pnorm(StdUnitsTallHeight, mean = 0, sd = 1) [1] 0.9238 [1] 0.9238 [1] 1.431 [1] 0.9238 1.1.2 Exercise. Fitting Galton’s height data Assignment Text The Galton data has already been read into a dataframe called heights. These data include the heights of 928 adult children child_ht, together with an index of their parents’ height parent_ht. The video explored the distribution of the parents’ height; in this assignment, we investigate the distribution of the heights of the adult children. Instructions Define the height of an adult child as a global variable Use the function mean() to calculate the mean and the function sd() to calculate the standard deviation Use the normal approximation and the function pnorm() determine the probability that an adult child’s height is less than 72 inches Hint. Remember that we can reference a variable, say var, from a data set such as heights, as heights$var. eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6IiNoZWlnaHRzIDwtIHJlYWQuY3N2KFwiQ1NWRGF0YVxcXFxnYWx0b25faGVpZ2h0LmNzdlwiLGhlYWRlciA9IFRSVUUpXG5oZWlnaHRzIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9hc3NldHMuZGF0YWNhbXAuY29tL3Byb2R1Y3Rpb24vcmVwb3NpdG9yaWVzLzI2MTAvZGF0YXNldHMvYzg1ZWRlNmMyMDVkMjIwNDllNzY2YmQwODk1NmIyMjVjNTc2MjU1Yi9nYWx0b25faGVpZ2h0LmNzdlwiLCBoZWFkZXIgPSBUUlVFKSIsInNhbXBsZSI6IiNEZWZpbmUgdGhlIGdsb2JhbCB2YXJpYWJsZVxuaHRfY2hpbGQgPC0gX19fXG5cbiNDYWxjdWxhdGUgdGhlIG1lYW4gaGVpZ2h0XG5tY2hpbGQgPC0gX19fXG5tY2hpbGRcblxuI0NhbGN1bGF0ZSB0aGUgc3RhbmRhcmQgZGV2aWF0aW9uIG9mIGhlaWdodHNcbnNkY2hpbGQgPC0gX19fXG5zZGNoaWxkXG5cbiNEZXRlcm1pbmUgdGhlIHByb2JhYmlsaXR5IHRoYXQgdGhlIGhlaWdodCBpcyBsZXNzIHRoYW4gNzJcbnByPV9fXyg3MiwgbWVhbj1tY2hpbGQsIHNkPXNkY2hpbGQpIiwic29sdXRpb24iOiIjIFNvbHV0aW9uXG5odF9jaGlsZCA8LSBoZWlnaHRzJGNoaWxkX2h0XG5tY2hpbGQgPC0gbWVhbihodF9jaGlsZClcbnNkY2hpbGQgPC0gc2QoaHRfY2hpbGQpXG5wcj1wbm9ybSg3MiwgbWVhbiA9IG1jaGlsZCwgc2QgPSBzZGNoaWxkKSIsInNjdCI6ImV4KCkgJT4lIGNoZWNrX29iamVjdChcImh0X2NoaWxkXCIsdW5kZWZpbmVkX21zZz1cIk1ha2Ugc3VyZSB5b3UgYXNzaWduIHRoZSBjaGlsZHJlbidzIGhpZ2h0IHRvIGh0X2NoaWxkXCIpICU+JSBjaGVja19lcXVhbChpbmNvcnJlY3RfbXNnPVwiUmVtZW1iZXIgdGhhdCBpbiBvcmRlciB0byBjYWxsIGEgc3BlY2lmaWMgY29sdW1uIGZyb20gYSBkYXRhZnJhbWUsIHVzZSB0aGUgJCBvcGVyYXRvclwiKVxuZXgoKSAlPiUgY2hlY2tfb2JqZWN0KFwibWNoaWxkXCIpICAlPiUgY2hlY2tfZXF1YWwoKVxuZXgoKSAlPiUgY2hlY2tfb2JqZWN0KFwic2RjaGlsZFwiKSAlPiUgY2hlY2tfZXF1YWwoKVxuZXgoKSAlPiUgY2hlY2tfb2JqZWN0KFwicHJcIikgJT4lIGNoZWNrX2VxdWFsKClcbnN1Y2Nlc3NfbXNnKFwiRXhjZWxsZW50ISBXaXRoIHRoaXMgcHJvY2VkdXJlLCB5b3UgY2FuIG5vdyBjYWxjdWxhdGUgcHJvYmFiaWxpdGllcyBmb3IgYW55IGRpc3RyaWJ1dGlvbiB1c2luZyBhIG5vcm1hbCBjdXJ2ZSBhcHByb3hpbWF0aW9uLlwiKSJ9 1.1.3 Exercise. Visualizing child’s height distribution Assignment Text As in the prior exercise, from the Galton dataset heights, the heights of 928 adult children have been used to create a global variable called ht_child. We also have basic summary statistics, the mean height mchild and the standard deviation of heights in sdchild. In this exercise, we explore the fit of the normal curve to this distribution. Instructions To visualize the distribution, use the function hist() to calculate the histogram. Use the freq = FALSE option to give a histogram with proportions instead of counts. Use the function seq() to determine a sequence that can be used for plotting. Then, with the function lines(), superimpose a normal curve on the histogram Determine the probability that a child’s height is greater than 72 inches Hint 1. Use the function dnorm() to calculate the normal density, similar to the cumulative probabilites that you calculated using pnorm() Hint 2. To calculate probabilities greater that an amount, simply use 1 minus the cumulative probability Pre-exercise code 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 1.2 Visualizing distributions In this section, you learn how to: Calculate and interpret distributions using histograms Calculate and interpret distributions using density plots 1.2.1 Video Video Overhead Details Show Overhead Details. Data description For our first look at an insurance data set, we consider data from Rempala and Derrig (2005). They considered claims arising from automobile bodily injury insurance coverages. These are amounts incurred for outpatient medical treatments that arise from automobile accidents, typically sprains, broken collarbones and the like. The data consists of a sample of 272 claims from Massachusetts that were closed in 2001 (by “closed,” we mean that the claim is settled and no additional liabilities can arise from the same accident). Rempala and Derrig were interested in developing procedures for handling mixtures of “typical” claims and others from providers who reported claims fraudulently. For this sample, we consider only those typical claims, ignoring the potentially fraudulent ones. # Reformat Data Set injury &lt;- read.csv(&quot;CSVData\\\\MassBodilyInjury.csv&quot;,header = TRUE) str(injury) head(injury) # PICK THE SUBSET OF THE DATA CORRESPONDING TO PROVIDER A injury2 &lt;- subset(injury, providerA ! = 0 ) injury2$claims &lt;- 1000*injury2$claims injury2$logclaims &lt;- log(injury2$claims) injury3 &lt;- injury2[c(&quot;claims&quot;,&quot;logclaims&quot;)] #write.csv(injury3,&quot;CSVData\\\\MassBI.csv&quot;,row.names = FALSE) Show Overhead A Details. Bring in Data, Introduce Logarithmic Claims injury &lt;- read.csv(&quot;CSVData\\\\MassBI.csv&quot;,header = TRUE) # CHECK THE NAMES, DIMENSION IN THE FILE AND LIST THE FIRST 8 OBSERVATIONS ; str(injury) head(injury) attach(injury) The following objects are masked from injury (pos = 3): claims, logclaims The following objects are masked from injury (pos = 4): claims, logclaims The following objects are masked from injury (pos = 13): claims, logclaims The following objects are masked from injury (pos = 14): claims, logclaims claims &lt;- injury$claims par(mfrow = c(1, 2)) hist(claims) hist(logclaims) &#39;data.frame&#39;: 272 obs. of 2 variables: $ claims : int 45 47 70 75 77 92 117 117 140 145 ... $ logclaims: num 3.81 3.85 4.25 4.32 4.34 ... claims logclaims 1 45 3.807 2 47 3.850 3 70 4.248 4 75 4.317 5 77 4.344 6 92 4.522 Show Overhead B Details. Show how to get a finer grid for histograms par(mfrow = c(1, 2)) hist(logclaims) hist(logclaims,breaks = 15) Show Overhead C Details. Introduce the density plot par(mfrow = c(1, 2)) plot(density(logclaims)) hist(logclaims, breaks = 15,freq = FALSE) lines(density(logclaims)) 1.2.2 Exercise. Visualizing bodily injury claims with density plots Assignment Text In the prior video, you learned about the Massachusetts bodily injury dataset. This dataframe, injury, has been read in and the global variable claims has been created. This assignment reviews the hist() function for visualizing distributions and allows you to explore density plotting, a smoothed version of the histogram. Instructions Use the function log() to create the logarithmic version of the claims variable Calculate a histogram of logarithmic with 40 bins using an option in the hist() function, breaks =. Create a density plot of logarithmic claims using the functions plot() and density(). Repeat the density plot, this time using a more refined bandwidth equal to 0.03. Use an option in the density() function, bw =. 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 1.3 Summarizing distributions In this section, you learn how to: Calculate and interpret basic summary statistics Calculate and interpret distributions using boxplots Calculate and interpret distributions using qq plots 1.3.1 Video Video Overhead Details Show Overhead A Details. Summary statistics injury &lt;- read.csv(&quot;CSVData\\\\MassBI.csv&quot;,header = TRUE) #injury &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/8cca19d0503fcf6e9d30d9cb912de5ba95ecb9c1/MassBI.csv&quot;, header = TRUE) attach(injury) # SUMMARY STATISTICS summary(injury) sd(claims);sd(logclaims) length(claims) claims logclaims Min. : 45 Min. : 3.81 1st Qu.: 892 1st Qu.: 6.79 Median : 2210 Median : 7.70 Mean : 2698 Mean : 7.39 3rd Qu.: 3215 3rd Qu.: 8.08 Max. :50000 Max. :10.82 [1] 3944 [1] 1.101 [1] 272 Show Overhead B Details. Boxplot # BASIC BOXPLOT boxplot(logclaims) quantile(logclaims, probs = 0.75) # BOXPLOT WITH ANNOTATION boxplot(logclaims, main = &quot;Boxplot of logclaims&quot;) text(1, 7.6, &quot;median&quot;, cex = 0.7) text(1, 6.55, &quot;25th percentile&quot;, cex = 0.7) text(1, 7.95, &quot;75th percentile&quot;, cex = 0.7) arrows(1.05, 4.9, 1.05, 3.6, col = &quot;blue&quot;, code = 3, angle = 20, length = 0.1) text(1.1, 4.4, &quot;outliers&quot;, cex = 0.7) text(1.1, 10.9, &quot;outlier&quot;, cex = 0.7) 75% 8.076 Show Overhead C Details. QQ Plot summary(injury) quantile(claims, probs = 0.75) quantile(logclaims, probs = 0.75) log(quantile(claims, probs = 0.75)) qnorm(p = 0.75, mean = mean(logclaims), sd = sd(logclaims)) (qnorm(p = 0.75, mean = mean(logclaims), sd = sd(logclaims)) -mean(logclaims)) / sd(logclaims) qnorm(p = 0.75, mean = 0, sd = 1) # QUANTILE - QUANTILE PLOT qqnorm(logclaims) qqline(logclaims) claims logclaims Min. : 45 Min. : 3.81 1st Qu.: 892 1st Qu.: 6.79 Median : 2210 Median : 7.70 Mean : 2698 Mean : 7.39 3rd Qu.: 3215 3rd Qu.: 8.08 Max. :50000 Max. :10.82 75% 3215 75% 8.076 75% 8.076 [1] 8.131 [1] 0.6745 [1] 0.6745 1.3.2 Exercise. Summarizing bodily injury claims with box and qq plots Assignment Text The Massachusetts bodily injury data has already been read and used to create the global variable claims representing bodily injury claims. The previous video showed how to present the distribution of logarithmic claims which appeared to be approximately normally distributed. However, users are not really interested in log dollars but want to know about a unit of measurement that is more intuitive, such as dollars. So this assignment is based on claims, not the logarithmic version. You will use the functions boxplot() and qqnorm() to visualize the distribution through boxplots and quantile-quantile, or qq-, plots. But, because we are working with such a skewed distribution, do not be surprised that it is difficult to interpret these results readily. Instructions Produce a box plot for claims Determine the 25th empirical percentile for claims using the quantile() function. Determine the 25th percentile for claims based on a normal distribution using the qnorm() function. Produce a normal qq plot for claims using the function qqnorm(). The qqline() function is handy for producing a reference line. Hint. Note that qnorm() (one q) is for a normal quantile and qqnorm(). (two q’s!) is for the normal qq plot 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 1.3.3 Exercise. Effects on distributions of removing the largest claim Assignment Text The Massachusetts bodily injury dataframe injury has been read in; our focus is on the claims variable in that dataset. In the previous exercise, we learned that the Massachusetts bodily injury claims distribution was not even close to approximately normal (as evidenced by the box and qq- plots). Non-normality may be induced by skewness (that we will handle via transformations in the next section). But, seeming non-normality can also be induced by one or two very large observations (called an outlier later in the course). So, this exercise examines the effects on the distribution of removing the largest claims. Instructions Use the function tail() to examine the injury dataset and identify the largest claim Use the function subset() to create a subset omitting the largest claim Compare the summary statistics of the omitted claim distribution to the full distribution Compare the two distributions visually via histograms plotted next to another. par(mfrow = c(1, 2)) is used to organize the plots you create. Do not alter this code. Hint. For this data set, the [subset()] argument claims &lt; 25000 will keep all but the largest claim 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 1.4 Transformations In this exercise, you learn how to: Symmetrize a skewed distribution using a logarithmic transformation 1.4.1 Video Video Overhead Details Show Overhead A Details. Simulate a moderately skewed distribution, with transforms # FIGURE 1.7 - SIMULATE CHI-SQUARE, CREATE 3 TRANSFORMATIONS set.seed(1237) # set the seed of the random number generator # allows us to replicate results X1 &lt;- 10000*rchisq(500, df = 2) # generate variables randomly from a skewed distribution X2 &lt;- X1^(0.5) # square root transform, could also use sqrt(X1) X3 &lt;- log(X1) # logarithmic transform X4 &lt;- -1/X1 # negative reciprocal transform Show Overhead B Details. Visualize the distributions par(mfrow = c(2, 2), cex = .75, mar = c(3,5,1.5,0)) hist(X1, freq = FALSE, nclass = 16, main = &quot;&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;, las = 1, yaxt = &quot;n&quot;,xlim = c(0,200000),ylim = c(0,.00005)) axis(2, at = seq(0,.00005,.00001),las = 1, cex = .3, labels = c(&quot;0&quot;, &quot;0.00001&quot;, &quot;0.00002&quot;,&quot;0.00003&quot;, &quot;0.00004&quot;, &quot;0.00005&quot;)) mtext(&quot;Density&quot;, side = 2, at = .000055, las = 1, cex = .75) mtext(&quot;y&quot;, side = 1, cex = .75, line = 2) par(mar = c(3,4,1.5,0.2)) hist(X2, freq = FALSE, nclass = 16, main = &quot;&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;, las = 1,xlim = c(0,400), ylim = c(0,.008)) mtext(&quot;Density&quot;, side = 2, at = .0088, las = 1, cex = .75) mtext(&quot;Square root of y&quot;, side = 1, cex = .75, line = 2) par(mar = c(3.2,5,1,0)) hist(X3, freq = FALSE, nclass = 16, main = &quot;&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;, las = 1, ylim = c(0,.4)) mtext(&quot;Density&quot;, side = 2, at = .44, las = 1, cex = .75) mtext(&quot;Logarithmic y&quot;, side = 1, cex = .75, line = 2) par(mar = c(3.2,4,1,0.2)) hist(X4, freq = FALSE, nclass = 16, main = &quot;&quot;,xlab = &quot;&quot;, ylab = &quot;&quot;, las = 1, ylim = c(0,100)) mtext(&quot;Density&quot;, side = 2, at = 110, las = 1, cex = .75) mtext(&quot;Negative reciprocal of y&quot;, side = 1, cex = .75, line = 2) 1.4.2 Exercise. Distribution of transformed bodily injury claims Assignment Text We have now examined the distributions of bodily injury claims and its logarithmic version. Grudgingly, we have concluded that to fit a normal curve the logarithmic version of claims is a better choice (again, we really do not like log dollars but you’ll get used to it in this course). But, why logarithmic and not some other transformations? A partial response to this question will appear in later chapters when we describe interpretation of regression coefficients. Another partial response is that the log transform seems to work well with skewed insurance data sets, as we demonstrate visually in this exercise. Instructions Use the code par(mfrow = c(2, 2)) so that four graphs appear in a 2 by 2 matrix format for easy comparisons. Plot the density() of claims square root of claims logarithmic claims negative reciprocal of claims Hint. For negative reciprocal claims, use plot(density(-claims^(-1))) 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 "],
["basic-linear-regression.html", "Chapter 2 Basic Linear Regression 2.1 Correlation 2.2 Method of least squares 2.3 Understanding variability 2.4 Statistical inference 2.5 Diagnostics", " Chapter 2 Basic Linear Regression Chapter description This chapter considers regression in the case of only one explanatory variable. Despite this seeming simplicity, many deep ideas of regression can be developed in this framework. By limiting ourselves to the one variable case, we can illustrate the relationships between two variables graphically. Graphical tools prove to be important for developing a link between the data and a predictive model. 2.1 Correlation In this section, you learn how to: Calculate and interpret a correlation coefficient Interpret correlation coefficients by visualizing scatter plots 2.1.1 Video Video Overhead Details Show Overhead A Details. Wisconsin lottery data description Lot &lt;- read.csv(&quot;CSVData\\\\Wisc_lottery.csv&quot;) #Lot &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/a792b30fb32b0896dd6894501cbab32b5d48df51/Wisc_lottery.csv&quot;, header = TRUE) str(Lot) &#39;data.frame&#39;: 50 obs. of 3 variables: $ pop : int 435 4823 2469 2051 13337 17004 38283 9859 4464 20958 ... $ sales : num 1285 3571 2407 1224 15046 ... $ medhome: num 71.3 98 58.7 65.7 96.7 66.4 91 61 91.5 68.8 ... Show Overhead B Details. Summary statistics #options(scipen = 100, digits = 4) #numSummary(Lot[,c(&quot;pop&quot;, &quot;sales&quot;)], statistics = c(&quot;mean&quot;, &quot;sd&quot;, &quot;quantiles&quot;), quantiles = c(0,.5,1)) (as.data.frame(psych::describe(Lot)))[,c(2,3,4,5,8,9)] #Rcmdr::numSummary(Lot[,c(&quot;pop&quot;, &quot;sales&quot;)], statistics = c(&quot;mean&quot;, &quot;sd&quot;, &quot;quantiles&quot;), quantiles = c(0,.5,1)) n mean sd median min max pop 50 9311.04 11098.16 4405.5 280.0 39098 sales 50 6494.83 8103.01 2426.4 189.0 33181 medhome 50 57.09 18.37 53.9 34.5 120 Show Overhead C Details. Visualizing skewed distributions par(mfrow = c(1, 2)) hist(Lot$pop, main = &quot;&quot;, xlab = &quot;population&quot;) hist(Lot$sales, main = &quot;&quot;, xlab = &quot;sales&quot;) Show Overhead D Details. Visualizing relationships with a scatter plot plot(Lot$pop, Lot$sales, xlab = &quot;population&quot;, ylab = &quot;sales&quot;) Show Overhead E Details. Correlation coefficient cor(Lot$pop, Lot$sales) [1] 0.8863 2.1.2 Exercise. Correlations and the Wisconsin lottery Assignment Text The Wisconsin lottery dataset, Wisc_lottery,has already been read into a dataframe Lot. Like insurance, lotteries are uncertain events and so the skills to work with and interpret lottery data are readily applicable to insurance. It is common to report sales and population in thousands of units, so this exercise gives you practice in rescaling data via linear transformations. Instructions From the available population and sales variables, create new variables in the dataframe Lot, pop_1000 and sales_1000 that are in thousands (of people and of dollars, respectively). Create summary statistics for the dataframe that includes these new variables. Plot pop_1000 versus sales_1000. Calculate the correlation between pop_1000 versus sales_1000 using the function cor(). How does this differ between the correlation between population and sales in the original units? Hint. Use the dataframe to refer to pop and sales as Lot$pop and Lot$sales, respectively eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6IiNMb3QgPC0gcmVhZC5jc3YoXCJDU1ZEYXRhXFxcXFdpc2NfbG90dGVyeS5jc3ZcIiwgaGVhZGVyID0gVFJVRSlcbkxvdCA8LSByZWFkLmNzdihcImh0dHBzOi8vYXNzZXRzLmRhdGFjYW1wLmNvbS9wcm9kdWN0aW9uL3JlcG9zaXRvcmllcy8yNjEwL2RhdGFzZXRzL2E3OTJiMzBmYjMyYjA4OTZkZDY4OTQ1MDFjYmFiMzJiNWQ0OGRmNTEvV2lzY19sb3R0ZXJ5LmNzdlwiLCBoZWFkZXIgPSBUUlVFKSIsInNhbXBsZSI6IiMgQ3JlYXRlIG5ldyB2YXJpYWJsZXMsIHNheSwgYHBvcF8xMDAwYCBhbmQgYHNhbGVzXzEwMDBgXG5Mb3QkcG9wXzEwMDAgPC0gX19fXG5fX18gPC0gTG90JHNhbGVzLzEwMDBcblxuIyBDcmVhdGUgc3VtbWFyeSBzdGF0aXN0aWNzIGZvciB0aGUgZGF0YWZyYW1lXG5zdW1tYXJ5KF9fXylcblxuIyBQbG90IGBwb3BfMTAwMGAgdmVyc3VzIGBzYWxlc18xMDAwYC5cbnBsb3QoX19fLCBfX18pXG5cbiMgQ2FsY3VsYXRlIHRoZSBjb3JyZWxhdGlvbiBiZXR3ZWVuIGBwb3BfMTAwMGAgdmVyc3VzIGBzYWxlc18xMDAwYCBcbmNvcihfX18sIF9fXykiLCJzb2x1dGlvbiI6IkxvdCRwb3BfMTAwMCA8LSBMb3QkcG9wLzEwMDBcbkxvdCRzYWxlc18xMDAwIDwtIExvdCRzYWxlcy8xMDAwXG5zdW1tYXJ5KExvdClcbnBsb3QoTG90JHBvcF8xMDAwLCBMb3Qkc2FsZXNfMTAwMClcbmNvcihMb3QkcG9wXzEwMDAsIExvdCRzYWxlc18xMDAwKSIsInNjdCI6InN1Y2Nlc3NfbXNnKFwiQ29uZ3JhdHVsYXRpb25zISBXZSB3aWxsIHJlc2NhbGUgZGF0YSB1c2luZyAnbGluZWFyJyB0cmFuc2Zvcm1hdGlvbnMgcmVndWxhcmx5LiBJbiBwYXJ0IHdlIGRvIHRoaXMgZm9yIGNvbW11bmljYXRpbmcgb3VyIGFuYWx5c2lzIHRvIG90aGVycy4gQWxzbyBpbiBwYXJ0LCB0aGlzIGlzIGZvciBvdXIgb3duIGNvbnZlbmllbmNlIGFzIGl0IGNhbiBhbGxvdyB1cyB0byBzZWUgcGF0dGVybnMgbW9yZSByZWFkaWx5LlwiKSJ9 2.2 Method of least squares In this section, you learn how to: Fit a line to data using the method of least squares Predict an observation using a least squares fitted line 2.2.1 Video 2.2.1.1 Video Overheads Show Overhead A Details. Where to fit the line? model_blr &lt;- lm(sales ~ pop, data = Lot) plot(Lot$pop, Lot$sales,xlab = &quot;population&quot;, ylab = &quot;sales&quot;) abline(model_blr, col=&quot;blue&quot;) abline(0,1, col=&quot;red&quot;) Show Overhead B Details. Method of least squares For observation \\(\\{(y, x)\\}\\), the height of the regression line is \\[b_0 + b_1 x.\\] Thus, \\(y - (b_0 + b_1 x)\\) represents the deviation. The sum of squared deviations is \\[SS(b_0, b_1) = \\sum (y - (b_0 + b_1 x))^2 .\\] The method of least squares – determine values of \\(b_0, b_1\\) that minimize \\(SS\\). Show Overhead C Details. Regression coefficients model_blr &lt;- lm(sales ~ pop, data = Lot) round(coefficients(model_blr), digits=4) plot(Lot$pop, Lot$sales,xlab = &quot;population&quot;, ylab = &quot;sales&quot;) abline(model_blr, col=&quot;blue&quot;) (Intercept) pop 469.7036 0.6471 Show Overhead D Details. Prediction round(coefficients(model_blr), digits=6) coefficients(model_blr)[1] + coefficients(model_blr)[2]*30000 newdata &lt;- data.frame(pop = 30000) predict(model_blr, newdata) (Intercept) pop 469.7036 0.6471 (Intercept) 19883 1 19883 2.2.2 Exercise. Least squares fit using housing prices Assignment Text The prior video analyzed the effect that a zip code’s population has on lottery sales. Instead of population, suppose that you wish to understand the effect that housing prices have on the sale of lottery tickets. The dataframe Lot, read in from the Wisconsin lottery dataset Wisc_lottery, contains the variable medhome which is the median house price for each zip code, in thousands of dollars. In this exercise, you will get a feel for the distribution of this variable by examining summary statistics, examine its relationship with sales graphically and via correlations, fit a basic linear regression model and use this model to predict sales. Instructions Summarize the dataframe Lot that contains medhome and sales. Plot medhome versus sales. Summarize this relationship by calculating the corresponding correlation coefficient using the function cor(). Using the function lm(), regress medhome, the explanatory variable, on sales, the outcome variable. Display the regression coefficients to four significant digits. Use the function predict() and the fitted regression model to predict sales assuming that the median house price for a zip code is 50 (in thousands of dollars). eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6IiNMb3QgPC0gcmVhZC5jc3YoXCJDU1ZEYXRhXFxcXFdpc2NfbG90dGVyeS5jc3ZcIiwgaGVhZGVyID0gVFJVRSlcbkxvdCA8LSByZWFkLmNzdihcImh0dHBzOi8vYXNzZXRzLmRhdGFjYW1wLmNvbS9wcm9kdWN0aW9uL3JlcG9zaXRvcmllcy8yNjEwL2RhdGFzZXRzL2E3OTJiMzBmYjMyYjA4OTZkZDY4OTQ1MDFjYmFiMzJiNWQ0OGRmNTEvV2lzY19sb3R0ZXJ5LmNzdlwiLCBoZWFkZXIgPSBUUlVFKSIsInNhbXBsZSI6IiMgU3VtbWFyaXplIHRoZSBkYXRhZnJhbWUgYExvdGAgdGhhdCBjb250YWlucyBgbWVkaG9tZWAgYW5kIGBzYWxlc2BcbnN1bW1hcnkoTG90KVxuIyBQbG90IGFuZCBjYWxjdWxhdGUgdGhlIGNvcnJlbGF0aW9uIG9mIGBtZWRob21lYCB2ZXJzdXMgYHNhbGVzYC4gXG5jb3IoX19fLCBfX18pXG5wbG90KF9fXywgX19fKVxuXG4jIFJlZ3Jlc3MgYG1lZGhvbWVgICBvbiBgc2FsZXNgLiBEaXNwbGF5IHRoZSByZWdyZXNzaW9uIGNvZWZmaWNpZW50cyB0byBmb3VyIHNpZ25pZmljYW50IGRpZ2l0cy5cbm1vZGVsX2JscjEgPC0gbG0oX19fIH4gX19fLCBkYXRhID0gTG90KVxucm91bmQoY29lZmZpY2llbnRzKG1vZGVsX2JscjEpLCBkaWdpdHM9IC0tLSlcblxuIyBQcmVkaWN0IHNhbGVzIGFzc3VtaW5nIHRoYXQgdGhlIG1lZGlhbiBob3VzZSBwcmljZSBpcyA1MCBcbm5ld2RhdGEgPC0gZGF0YS5mcmFtZShtZWRob21lID0gX19fKVxucHJlZGljdChtb2RlbF9ibHIxLCBuZXdkYXRhKSIsInNvbHV0aW9uIjoic3VtbWFyeShMb3QpXG5jb3IoTG90JG1lZGhvbWUsTG90JHNhbGVzKVxucGxvdChMb3QkbWVkaG9tZSxMb3Qkc2FsZXMpXG5tb2RlbF9ibHIxIDwtIGxtKHNhbGVzIH4gbWVkaG9tZSwgZGF0YSA9IExvdClcbnJvdW5kKGNvZWZmaWNpZW50cyhtb2RlbF9ibHIxKSwgZGlnaXRzPTQpXG5uZXdkYXRhIDwtIGRhdGEuZnJhbWUobWVkaG9tZSA9IDUwKVxucHJlZGljdChtb2RlbF9ibHIxLCBuZXdkYXRhKSIsInNjdCI6InN1Y2Nlc3NfbXNnKFwiQ29uZ3JhdHVsYXRpb25zISBZb3Ugbm93IGhhdmUgZXhwZXJpZW5jZSBmaXR0aW5nIGEgcmVncmVzc2lvbiBsaW5lIGFuZCB1c2luZyB0aGlzIGxpbmUgZm9yIHByZWRpY3Rpb25zLCBqdXN0IGFzIEdhbHRvbiBkaWQgd2hlbiBoZSB1c2VkIHBhcmVudHMnIGhlaWdodHMgdG8gcHJlZGljdCB0aGUgaGVpZ2h0IG9mIGFuIGFkdWx0IGNoaWxkLiBXZWxsIGRvbmUhXCIpIn0= 2.3 Understanding variability In this section, you learn how to: Visualize the ANOVA decomposition of variability Calculate and interpret \\(R^2\\), the coefficient of determination Calculate and interpret \\(s^2\\) the mean square error Explain the components of the ANOVA table 2.3.1 Video Video Overhead Details Show OverheadS A and B Details. Visualizing the uncertainty about a line par(mar=c(2.2,2.1,.2,.2),cex=1.2) x &lt;- seq(-4, 4, len=101) y &lt;- x plot(x, y, type = &quot;l&quot;, xlim=c(-3, 4), xaxt=&quot;n&quot;, yaxt=&quot;n&quot;, xlab=&quot;&quot;, ylab=&quot;&quot;) axis(1, at = c(-1, 1),lab = expression(bar(x), x)) axis(2, at = c(-1, 1, 3),lab = expression(bar(y), hat(y), y), las=1) abline(-1, 0, lty = 2) segments(-4, 1, 1, 1, lty=2) segments(-4, 3, 1, 3, lty = 2) segments(1, -4, 1, 3, lty = 2) segments(-1, -4, -1, -1, lty = 2) points(1, 3, cex=1.5, pch=19) arrows(1.0, 1, 1.0, 3, code = 3, lty = 1, angle=15, length=0.12, lwd=2) text(1.3, 2.2, expression( y-hat(y)),cex=0.8) text(-.3,2.2,&quot;&#39;unexplained&#39; deviation&quot;, cex=.8) arrows(1.0, -1, 1.0, 1, code = 3, lty = 1, angle=15, length=0.12, lwd=2) text(1.85, 0, expression(hat(y)-bar(y) == b[1](x-bar(x)) ), cex=0.8 ) text(2.1, -0.5, &quot; &#39;explained&#39; deviation&quot;, cex=0.8) arrows(-1, -1.0, 1, -1.0, code = 3, lty = 1, angle=15, length=0.12, lwd = 2) text(0, -1.3, expression( x-bar(x)), cex=0.8 ) text(3.5, 2.7, expression( hat(y)== b[0]+ b[1]*x), cex=0.8 ) Show Overheads C, D and E Details. ANOVA Table model_blr &lt;- lm(sales ~ pop, data = Lot) anova(model_blr) sqrt(anova(model_blr)$Mean[2]) summary(model_blr)$r.squared Analysis of Variance Table Response: sales Df Sum Sq Mean Sq F value Pr(&gt;F) pop 1 2527165015 2527165015 176 &lt;0.0000000000000002 *** Residuals 48 690116755 14377432 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 [1] 3792 [1] 0.7855 2.3.2 Exercise. Summarizing measures of uncertainty Assignment Text In a previous exercise, you developed a regression line to fit the variable medhome, the median house price for each zip code, as a predictor of lottery sales. The regression of medhome on sales has been summarized in the R object model_blr. How reliable is the regression line? In this excercise, you will compute some of the standard measures that are used to summarize the goodness of this fit. Instructions Summarize the fitted regression model in an ANOVA table. Determine the size of the typical residual, \\(s\\). Determine the coefficient of determination, \\(R^2\\). Hint. Learn more about possibilities through the Rdocumentation site. If you have not done so already, check out the function anova() eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6IiNMb3QgPC0gcmVhZC5jc3YoXCJDU1ZEYXRhXFxcXFdpc2NfbG90dGVyeS5jc3ZcIiwgaGVhZGVyID0gVFJVRSlcbkxvdCA8LSByZWFkLmNzdihcImh0dHBzOi8vYXNzZXRzLmRhdGFjYW1wLmNvbS9wcm9kdWN0aW9uL3JlcG9zaXRvcmllcy8yNjEwL2RhdGFzZXRzL2E3OTJiMzBmYjMyYjA4OTZkZDY4OTQ1MDFjYmFiMzJiNWQ0OGRmNTEvV2lzY19sb3R0ZXJ5LmNzdlwiLCBoZWFkZXIgPSBUUlVFKSIsInNhbXBsZSI6Im1vZGVsX2JsciA8LSBsbShzYWxlcyAgfiBtZWRob21lLCBkYXRhID0gTG90KVxuXG4jIFN1bW1hcml6ZSB0aGUgZml0dGVkIHJlZ3Jlc3Npb24gbW9kZWwgaW4gYW4gQU5PVkEgdGFibGUuXG5hbm92YShfX18pXG5cbiMgRGV0ZXJtaW5lIHRoZSBzaXplIG9mIHRoZSB0eXBpY2FsIHJlc2lkdWFsLCAkcyQuXG5zcXJ0KGFub3ZhKF9fXykkTWVhblsyXSlcblxuIyBEZXRlcm1pbmUgdGhlIGNvZWZmaWNpZW50IG9mIGRldGVybWluYXRpb24sICRSXjIkLiBcbnN1bW1hcnkoX19fKSRyLnNxdWFyZWQiLCJzb2x1dGlvbiI6Im1vZGVsX2JsciA8LSBsbShzYWxlcyAgfiBtZWRob21lLCBkYXRhID0gTG90KVxuYW5vdmEobW9kZWxfYmxyKVxuc3FydChhbm92YShtb2RlbF9ibHIpJE1lYW5bMl0pXG5zdW1tYXJ5KG1vZGVsX2Jscikkci5zcXVhcmVkIiwic2N0Ijoic3VjY2Vzc19tc2coXCJDb25ncmF0dWxhdGlvbnMhIEl0IHdpbGwgYmUgaGVscGZ1bCBpZiB5b3UgY29tcGFyZSB0aGUgcmVzdWx0cyBvZiB0aGlzIGV4ZXJjaXNlIHRvIHRoZSByZWdyZXNzaW9uIG9mIGBwb3BgIG9uIGBzYWxlc2AgZnJvbSB0aGUgcHJpb3IgdmlkZW8uIFdlIGhhdmUgc2VlbiB0aGF0IGBwb3BgIGlzIG1vcmUgaGlnaGx5IGNvcnJlbGF0ZWQgd2l0aCBgc2FsZXNgIHRoYW4gYG1lZGhvbWVgLCBzbyB3ZSBhcmUgZXhwZWN0aW5nIGdyZWF0ZXIgdW5jZXJ0YWludHkgaW4gdGhpcyByZWdyZXNzaW9uIGZpdC5cIikifQ== 2.3.3 Exercise. Effects of linear transforms on measures of uncertainty Assignment Text Let us see how rescaling, a linear transformation, affects our measures of uncertainty. As before, the Wisconsin lottery dataset Wisc_lottery has been read into a dataframe Lot that also contains sales_1000, sales in thousands of dollars, and pop_1000, zip code population in thousands. How do measures of uncertainty change when going from the original units to thousands of those units? Instructions Run a regression of pop on sales_1000 and summarize this in an ANOVA table. For this regression, determine the \\(s\\) and the coefficient of determination, \\(R^2\\). Run a regression of pop_1000 on sales_1000 and summarize this in an ANOVA table. For this regression, determine the \\(s\\) and the coefficient of determination, \\(R^2\\). Hint. The residual standard error is also available as summary(model_blr1)$sigma. The coefficient of determination is also available as summary(model_blr1)$r.squared. eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6IiNMb3QgPC0gcmVhZC5jc3YoXCJDU1ZEYXRhXFxcXFdpc2NfbG90dGVyeS5jc3ZcIiwgaGVhZGVyID0gVFJVRSlcbkxvdCA8LSByZWFkLmNzdihcImh0dHBzOi8vYXNzZXRzLmRhdGFjYW1wLmNvbS9wcm9kdWN0aW9uL3JlcG9zaXRvcmllcy8yNjEwL2RhdGFzZXRzL2E3OTJiMzBmYjMyYjA4OTZkZDY4OTQ1MDFjYmFiMzJiNWQ0OGRmNTEvV2lzY19sb3R0ZXJ5LmNzdlwiLCBoZWFkZXIgPSBUUlVFKVxuTG90JHBvcF8xMDAwIDwtIExvdCRwb3AvMTAwMFxuTG90JHNhbGVzXzEwMDAgPC0gTG90JHNhbGVzLzEwMDAiLCJzYW1wbGUiOiIjIFJ1biBhIHJlZ3Jlc3Npb24gb2YgYHBvcGAgb24gYHNhbGVzXzEwMDBgIGFuZCBzdW1tYXJpemUgdGhpcyBpbiBhbiBBTk9WQSB0YWJsZS5cbm1vZGVsX2JscjEgPC0gbG0oc2FsZXNfMTAwMCAgfiBwb3AsIGRhdGEgPSBMb3QpXG5hbm92YShfX18pXG5cbiMgRGV0ZXJtaW5lIHRoZSAkcyQgYW5kIHRoZSBjb2VmZmljaWVudCBvZiBkZXRlcm1pbmF0aW9uLCAkUl4yJC4gIFxuc3FydChhbm92YShfX18pJE1lYW5bMl0pXG5zdW1tYXJ5KF9fXykkci5zcXVhcmVkXG5cbiMgUnVuIGEgcmVncmVzc2lvbiBvZiBgcG9wXzEwMDBgIG9uIGBzYWxlc18xMDAwYCBhbmQgc3VtbWFyaXplIHRoaXMgaW4gYW4gQU5PVkEgdGFibGUuXG5tb2RlbF9ibHIyIDwtIGxtKF9fXyAgfiBfX18sIGRhdGEgPSBMb3QpXG5hbm92YShtb2RlbF9ibHIyKVxuXG4jIERldGVybWluZSB0aGUgJHMkIGFuZCB0aGUgY29lZmZpY2llbnQgb2YgZGV0ZXJtaW5hdGlvbiwgJFJeMiQuIFxuX19fXG5fX18iLCJzb2x1dGlvbiI6Im1vZGVsX2JscjEgPC0gbG0oc2FsZXNfMTAwMCAgfiBwb3AsIGRhdGEgPSBMb3QpXG5hbm92YShtb2RlbF9ibHIxKVxuc3FydChhbm92YShtb2RlbF9ibHIxKSRNZWFuWzJdKVxuc3VtbWFyeShtb2RlbF9ibHIxKSRyLnNxdWFyZWRcbm1vZGVsX2JscjIgPC0gbG0oc2FsZXNfMTAwMCAgfiBwb3BfMTAwMCAsIGRhdGEgPSBMb3QpXG5hbm92YShtb2RlbF9ibHIyKVxuc3FydChhbm92YShtb2RlbF9ibHIyKSRNZWFuWzJdKVxuc3VtbWFyeShtb2RlbF9ibHIyKSRyLnNxdWFyZWQiLCJzY3QiOiJzdWNjZXNzX21zZyhcIkNvbmdyYXR1bGF0aW9ucyEgSW4gdGhpcyBleGVyY2lzZSwgeW91IGhhdmUgc2VlbiB0aGF0IHJlc2NhbGluZyBkb2VzIG5vdCBhZmZlY3Qgb3VyIG1lYXN1cmVzIG9mIGdvb2RuZXNzIG9mIGZpdCBpbiBhbnkgbWVhbmluZ2Z1bCB3YXkuIEZvciBleGFtcGxlLCB0aGUgY29lZmZpY2llbnQgb2YgZGV0ZXJtaW5hdGlvbnMgYXJlIGNvbXBsZXRlbHkgdW5hZmZlY3RlZC4gVGhpcyBpcyBoZWxwZnVsIGJlY2F1c2Ugd2Ugd2lsbCByZXNjYWxlIHZhcmlhYmxlcyBleHRlbnNpdmVseSBpbiBvdXIgc2VhcmNoIGZvciBwYXR0ZXJucyBpbiB0aGUgZGF0YS5cIikifQ== 2.4 Statistical inference In this section, you learn how to: Conduct a hypothesis test for a regression coefficient using either a rejection/acceptance procedure or a p-value Calculate and interpret a confidence interval for a regression coefficient Calculate and interpret a prediction interval at a specific value of a predictor variable 2.4.1 Video Video Overhead Details Show Overhead A Details. Summary of basic linear regression model Introduce the output in the summary of the basic linear regression model. Lot &lt;- read.csv(&quot;CSVData\\\\Wisc_lottery.csv&quot;, header = TRUE) #Lot &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/a792b30fb32b0896dd6894501cbab32b5d48df51/Wisc_lottery.csv&quot;, header = TRUE) #options(scipen = 8, digits = 4) model_blr &lt;- lm(sales ~ pop, data = Lot) summary(model_blr) Call: lm(formula = sales ~ pop, data = Lot) Residuals: Min 1Q Median 3Q Max -6047 -1461 -670 486 18229 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 469.7036 702.9062 0.67 0.51 pop 0.6471 0.0488 13.26 &lt;0.0000000000000002 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 3790 on 48 degrees of freedom Multiple R-squared: 0.785, Adjusted R-squared: 0.781 F-statistic: 176 on 1 and 48 DF, p-value: &lt;0.0000000000000002 Show Overhead B Details. Hypothesis testing &gt; summary(model_blr)$coefficients Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 469.7036 702.90619 0.6682 5.072e-01 pop 0.6471 0.04881 13.2579 1.158e-17 Show Overhead C Details. Confidence intervals Rcmdr::Confint(model_blr, level = .90) Rcmdr::Confint(model_blr, level = .95) Estimate 5 % 95 % (Intercept) 469.7036 -709.2277 1648.635 pop 0.6471 0.5652 0.729 Estimate 2.5 % 97.5 % (Intercept) 469.7036 -943.584 1882.9912 pop 0.6471 0.549 0.7452 Show Overhead D Details. Confidence intervals check # Just for checking summary(model_blr)$coefficients[2,1] summary(model_blr)$coefficients[2,2] qt(.975, 48) summary(model_blr)$coefficients[2,1] - summary(model_blr)$coefficients[2,2]*qt(.975, 48) Rcmdr::Confint(model_blr, level = .95) confint(model_blr, level = .95) [1] 0.6471 [1] 0.04881 [1] 2.011 [1] 0.549 Estimate 2.5 % 97.5 % (Intercept) 469.7036 -943.584 1882.9912 pop 0.6471 0.549 0.7452 2.5 % 97.5 % (Intercept) -943.584 1882.9912 pop 0.549 0.7452 Show Overhead E Details. Prediction intervals NewData &lt;- data.frame(pop = 10000) predict(model_blr, NewData, interval = &quot;prediction&quot;, level = .90) predict(model_blr, NewData, interval = &quot;prediction&quot;, level = .99) fit lwr upr 1 6941 517.5 13364 fit lwr upr 1 6941 -3331 17213 2.4.2 Exercise. Statistical inference and Wisconsin lottery Assignment Text In a previous exercise, you developed a regression line with the variable medhome, the median house price for each zip code, as a predictor of lottery sales. The regression of medhome on sales has been summarized in the R object model_blr. This exercise allows you to practice the standard inferential tasks: hypothesis testing, confidence intervals, and prediction. Instructions Summarize the regression model and identify the t-statistic for testing the importance of the regression coefficient associated with medhome. Use the function confint() to provide a 95% confidence interval for the regression coefficient associated with medhome. Consider a zip code with a median housing price equal to 50 (in thousands of dollars). Use the function predict() to provide a point prediction and a 95% prediction interval for sales. Hint. Taking a [summary()] of a regression object produces a new objeect. You can use the [str()] structure command to learn more about the new object. Try out a command such as str(summary(model_blr)) eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6IiNMb3QgPC0gcmVhZC5jc3YoXCJDU1ZEYXRhXFxcXFdpc2NfbG90dGVyeS5jc3ZcIiwgaGVhZGVyID0gVFJVRSlcbkxvdCA8LSByZWFkLmNzdihcImh0dHBzOi8vYXNzZXRzLmRhdGFjYW1wLmNvbS9wcm9kdWN0aW9uL3JlcG9zaXRvcmllcy8yNjEwL2RhdGFzZXRzL2E3OTJiMzBmYjMyYjA4OTZkZDY4OTQ1MDFjYmFiMzJiNWQ0OGRmNTEvV2lzY19sb3R0ZXJ5LmNzdlwiLCBoZWFkZXIgPSBUUlVFKSIsInNhbXBsZSI6Im1vZGVsX2JscjEgPC0gbG0oc2FsZXMgfiBtZWRob21lLCBkYXRhID0gTG90KVxuIyBTdW1tYXJpemUgdGhlIHJlZ3Jlc3Npb24gbW9kZWwgYW5kIGlkZW50aWZ5IHRoZSAkdCQtc3RhdGlzdGljIGZvciB0ZXN0aW5nIHRoZSBpbXBvcnRhbmNlIG9mIHRoZSByZWdyZXNzaW9uIGNvZWZmaWNpZW50IGFzc29jaWF0ZWQgd2l0aCBgbWVkaG9tZWAuXG5zdW1tYXJ5KF9fXylcbnN1bW1hcnkoX19fKSRjb2VmZmljaWVudHNcbnN1bW1hcnkoX19fKSRjb2VmZmljaWVudHNbLDNdXG5cbiMgUHJvdmlkZSBhIDk1XFwlIGNvbmZpZGVuY2UgaW50ZXJ2YWwgZm9yIHRoZSByZWdyZXNzaW9uIGNvZWZmaWNpZW50IGFzc29jaWF0ZWQgd2l0aCBgbWVkaG9tZWAuXG5jb25maW50KF9fXywgbGV2ZWwgPSBfX18pXG5cbiMgUHJvdmlkZSBhIHBvaW50IHByZWRpY3Rpb24gYW5kIGEgOTVcXCUgcHJlZGljdGlvbiBpbnRlcnZhbCBmb3Igc2FsZXMuXG5OZXdEYXRhMSA8LSBkYXRhLmZyYW1lKG1lZGhvbWUgPSA1MClcbnByZWRpY3QoX19fLCBOZXdEYXRhMSwgaW50ZXJ2YWwgPSBcInByZWRpY3Rpb25cIiwgbGV2ZWwgPSBfX18pIiwic29sdXRpb24iOiJtb2RlbF9ibHIxIDwtIGxtKHNhbGVzIH4gbWVkaG9tZSwgZGF0YSA9IExvdClcbnN1bW1hcnkobW9kZWxfYmxyMSlcbnN1bW1hcnkobW9kZWxfYmxyMSkkY29lZmZpY2llbnRzXG5zdW1tYXJ5KG1vZGVsX2JscjEpJGNvZWZmaWNpZW50c1ssM11cblxuI1JjbWRyOjpDb25maW50KG1vZGVsX2JscjEsIGxldmVsID0gLjk1KVxuY29uZmludChtb2RlbF9ibHIxLCBsZXZlbCA9IC45NSlcblxuTmV3RGF0YTEgPC0gZGF0YS5mcmFtZShtZWRob21lID0gNTApXG5wcmVkaWN0KG1vZGVsX2JscjEsIE5ld0RhdGExLCBpbnRlcnZhbCA9IFwicHJlZGljdGlvblwiLCBsZXZlbCA9IC45NSkiLCJzY3QiOiJzdWNjZXNzX21zZyhcIkNvbmdyYXR1bGF0aW9ucyEgTXVjaCBvZiB3aGF0IHdlIGxlYXJuIGZyb20gYSBkYXRhIG1vZGVsaW5nIGV4ZXJjaXNlIGNhbiBiZSBzdW1tYXJpemVkIHVzaW5nIHN0YW5kYXJkIGluZmVyZW50aWFsIHRvb2xzOiBoeXBvdGhlc2lzIHRlc3RpbmcsIGNvbmZpZGVuY2UgaW50ZXJ2YWxzLCBhbmQgcHJlZGljdGlvbi5cIikifQ== 2.5 Diagnostics In this section, you learn how to: Describe how diagnostic checking and residual analysis are used in a statistical analysis Describe several model misspecifications commonly encountered in a regression analysis 2.5.1 Video Video Overhead Details Show Overhead A Details. Unusual observations in regression We have defined regression estimates as minimizers of a least squares objective function. An appealing intuitive feature of linear regressions is that regression estimates can be expressed as weighted averages of outcomes. The weights vary by observation, some observations are more important than others. “Unusual” observations are far from the majority of the data set: Unusual in the vertical direction is called an outlier. Unusual in the horizontal directional is called a high leverage point. Show Overhead B Details. Example. Outliers and High Leverage Points outlr &lt;- read.csv(&quot;CSVData\\\\Outlier.csv&quot;, header = TRUE) # FIGURE 2.7 plot(outlr$x, outlr$y, xlim = c(0, 10), ylim = c(2, 9), xlab = &quot;x&quot;, ylab = &quot;y&quot;) text(4.5, 8.0, &quot;A&quot;) text(9.8, 8.0, &quot;B&quot;) text(9.8, 2.5, &quot;C&quot;) Show Overhead C Details. Regression fit with 19 base observations model_outlr0 &lt;- lm(y ~ x, data = outlr, subset = -c(20,21,22)) summary(model_outlr0) plot(outlr$x[1:19], outlr$y[1:19], xlab = &quot;x&quot;, ylab = &quot;y&quot;, xlim = c(0, 10), ylim = c(2, 9)) abline(model_outlr0) Call: lm(formula = y ~ x, data = outlr, subset = -c(20, 21, 22)) Residuals: Min 1Q Median 3Q Max -0.4791 -0.2709 0.0711 0.2263 0.4094 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 1.8687 0.1958 9.54 0.0000000306 *** x 0.6109 0.0522 11.71 0.0000000015 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.288 on 17 degrees of freedom Multiple R-squared: 0.89, Adjusted R-squared: 0.883 F-statistic: 137 on 1 and 17 DF, p-value: 0.00000000147 Show Overhead D Details. Regression fit with 19 base observations plus C model_outlrC &lt;- lm(y ~ x, data = outlr, subset = -c(20,21)) summary(model_outlrC) plot(outlr$x[c(1:19,22)], outlr$y[c(1:19,22)], xlab = &quot;x&quot;, ylab = &quot;y&quot;, xlim = c(0, 10), ylim = c(2, 9)) text(9.8, 2.5, &quot;C&quot;, col = &quot;blue&quot;) abline(model_outlrC) Call: lm(formula = y ~ x, data = outlr, subset = -c(20, 21)) Residuals: Min 1Q Median 3Q Max -2.3295 -0.5782 0.0977 0.6724 1.0910 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 3.356 0.456 7.36 0.00000079 *** x 0.155 0.108 1.44 0.17 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.865 on 18 degrees of freedom Multiple R-squared: 0.103, Adjusted R-squared: 0.0533 F-statistic: 2.07 on 1 and 18 DF, p-value: 0.167 Show Overhead E Details. R code model_outlr0 &lt;- lm(y ~ x, data = outlr, subset = -c(20,21,22)) summary(model_outlr0) model_outlrA &lt;- lm(y ~ x, data = outlr, subset = -c(21,22)) summary(model_outlrA) model_outlrB &lt;- lm(y ~ x, data = outlr, subset = -c(20,22)) summary(model_outlrB) model_outlrC &lt;- lm(y ~ x, data = outlr, subset = -c(20,21)) summary(model_outlrC) Call: lm(formula = y ~ x, data = outlr, subset = -c(20, 21, 22)) Residuals: Min 1Q Median 3Q Max -0.4791 -0.2709 0.0711 0.2263 0.4094 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 1.8687 0.1958 9.54 0.0000000306 *** x 0.6109 0.0522 11.71 0.0000000015 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.288 on 17 degrees of freedom Multiple R-squared: 0.89, Adjusted R-squared: 0.883 F-statistic: 137 on 1 and 17 DF, p-value: 0.00000000147 Call: lm(formula = y ~ x, data = outlr, subset = -c(21, 22)) Residuals: Min 1Q Median 3Q Max -0.739 -0.393 -0.180 0.122 3.269 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 1.750 0.574 3.05 0.00688 ** x 0.693 0.152 4.57 0.00024 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.845 on 18 degrees of freedom Multiple R-squared: 0.537, Adjusted R-squared: 0.511 F-statistic: 20.9 on 1 and 18 DF, p-value: 0.000237 Call: lm(formula = y ~ x, data = outlr, subset = -c(20, 22)) Residuals: Min 1Q Median 3Q Max -0.5176 -0.2809 0.0345 0.2359 0.4458 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 1.7746 0.1502 11.8 0.00000000064762 *** x 0.6398 0.0355 18.0 0.00000000000058 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.285 on 18 degrees of freedom Multiple R-squared: 0.947, Adjusted R-squared: 0.945 F-statistic: 325 on 1 and 18 DF, p-value: 0.000000000000581 Call: lm(formula = y ~ x, data = outlr, subset = -c(20, 21)) Residuals: Min 1Q Median 3Q Max -2.3295 -0.5782 0.0977 0.6724 1.0910 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 3.356 0.456 7.36 0.00000079 *** x 0.155 0.108 1.44 0.17 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.865 on 18 degrees of freedom Multiple R-squared: 0.103, Adjusted R-squared: 0.0533 F-statistic: 2.07 on 1 and 18 DF, p-value: 0.167 Show Overhead F Details. Visualizing four regression fits plot(outlr$x, outlr$y, xlim = c(0, 10), ylim = c(2, 9), xlab = &quot;x&quot;, ylab = &quot;y&quot;) text(4.5, 8.0, &quot;A&quot;, col = &quot;red&quot;) text(9.8, 8.0, &quot;B&quot;, col = &quot;green&quot;) text(9.8, 2.5, &quot;C&quot;, col = &quot;blue&quot;) abline(model_outlr0) abline(model_outlrA, col = &quot;red&quot;) abline(model_outlrB, col = &quot;green&quot;) abline(model_outlrC, col = &quot;blue&quot;) Show Overhead G Details. Results from four regression models \\[\\begin{matrix} \\begin{array}{c} \\text{Results from Four Regressions} \\end{array}\\\\\\scriptsize \\begin{array}{l|rrrrr} \\hline \\text{Data} &amp; b_0 &amp; b_1 &amp; s &amp; R^2(\\%) &amp; t(b_1) \\\\ \\hline \\text{19 Base Points} &amp; 1.869 &amp; 0.611 &amp; 0.288 &amp; 89.0 &amp; 11.71 \\\\ \\text{19 Base Points} ~+~ A &amp; 1.750 &amp; 0.693 &amp; 0.846 &amp; 53.7 &amp; 4.57 \\\\ \\text{19 Base Points} ~+~ B &amp; 1.775 &amp; 0.640 &amp; 0.285 &amp; 94.7 &amp; 18.01 \\\\ \\text{19 Base Points} ~+~ C &amp; 3.356 &amp; 0.155 &amp; 0.865 &amp; 10.3 &amp; 1.44 \\\\ \\hline \\end{array} \\end{matrix}\\] 2.5.2 Exercise. Assessing outliers in lottery sales Assignment Text In an earlier video, we made a scatter plot of population versus sales. This plot exhibits an outlier; the point in the upper left-hand side of the plot represents a zip code that includes Kenosha, Wisconsin. Sales for this zip code are unusually high given its population. This exercise summarizes the regression fit both with and without this zip code in order to see how robust our results are to the inclusion of this unusual observation. Instructions A basic linear regression fit of population on sales has already been fit in the object model_blr. Re-fit this same model to the data, this time omitting Kenosha (observation number 9). Plot these two least squares fitted lines superimposed on the full data set. What is the effect on the distribution of residuals by removing this point? Calculate a normal qq plot with and without Kenosha. Hint. You can extract the residuals from a regression object with the function [residuals()]. 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 "],
["multiple-linear-regression.html", "Chapter 3 Multiple Linear Regression Term Life Data 3.1 Method of least squares 3.2 Statistical inference and multiple linear regresson 3.3 Binary variables 3.4 Categorical variables 3.5 General linear hypothesis", " Chapter 3 Multiple Linear Regression Chapter description This chapter introduces linear regression in the case of several explanatory variables, known as multiple linear regression (MLR). Many basic linear regression concepts extend directly, including goodness of fit measures such as the coefficient of determination and inference using t-statistics. Multiple linear regression models provide a framework for summarizing highly complex, multivariate data. Because this framework requires only linearity in the parameters, we are able to fit models that are nonlinear functions of the explanatory variables, thus providing a wide scope of potential applications. Term Life Data Video Overhead Details Show Overhead A Details. Demand for term life insurance “Who buys insurance and how much do they buy?” Companies have data on current customers How do get info on potential (new) customers? To understand demand, consider the Survey of Consumer Finances (SCF) This is a nationally representative sample that contains extensive information on potential U.S. customers. We study a random sample of 500 of the 4,519 households with positive income that were interviewed in the 2004 survey. We now focus on n = 275 households that purchased term life insurance Show Overhead B Details. Term life insurance summary statistics We study y = face, the amount that the company will pay in the event of the death of the named insured. We focus on k = 3 explanatory variables - annual income, - the number of years of education of the survey respondent and - the number of household members, numhh. The data suggest that income and face are skewed so we also introduce logarithmic versions. Show Overhead C Details. Summary statistics Term &lt;- read.csv(&quot;CSVData\\\\term_life.csv&quot;, header = TRUE) # PICK THE SUBSET OF THE DATA CORRESPONDING TO TERM PURCHASE Term1 &lt;- subset(Term, subset = face &gt; 0) str(Term1) head(Term1) library(Rcmdr) Term2 &lt;- Term1[, c(&quot;education&quot;, &quot;face&quot;, &quot;income&quot;, &quot;logface&quot;, &quot;logincome&quot;, &quot;numhh&quot;)] options(scipen = 100, digits = 4) summvar &lt;- numSummary(Term2, statistic = c(&quot;mean&quot;, &quot;sd&quot;, &quot;quantiles&quot;), quantiles = c(0, .5, 1)) summvar &#39;data.frame&#39;: 275 obs. of 7 variables: $ education: int 16 9 16 17 11 16 17 16 14 12 ... $ face : int 20000 130000 1500000 50000 220000 600000 100000 2500000 250000 50000 ... $ income : int 43000 12000 120000 40000 28000 100000 112000 15000 32000 25000 ... $ logface : num 9.9 11.8 14.2 10.8 12.3 ... $ logincome: num 10.67 9.39 11.7 10.6 10.24 ... $ numhh : int 3 3 5 4 4 3 2 4 1 2 ... $ marstat : int 1 1 1 1 2 1 1 1 0 1 ... education face income logface logincome numhh marstat 1 16 20000 43000 9.903 10.669 3 1 2 9 130000 12000 11.775 9.393 3 1 3 16 1500000 120000 14.221 11.695 5 1 4 17 50000 40000 10.820 10.597 4 1 6 11 220000 28000 12.301 10.240 4 2 8 16 600000 100000 13.305 11.513 3 1 mean sd 0% 50% 100% n education 14.52 2.549 2.000 16.00 17.00 275 face 747581.45 1674362.433 800.000 150000.00 14000000.00 275 income 208974.62 824009.775 260.000 65000.00 10000000.00 275 logface 11.99 1.871 6.685 11.92 16.45 275 logincome 11.15 1.295 5.561 11.08 16.12 275 numhh 2.96 1.493 1.000 3.00 9.00 275 Show Overhead D Details. Scatter plots of income versus face in original and logarithmic units par(mfrow = c(1, 2)) plot(Term2$income, Term2$face, xlab = &quot;income&quot;, ylab = &quot;face&quot;) plot(Term2$logincome, Term2$logface, xlab = &quot;log&quot;, ylab = &quot;log face&quot;) 3.1 Method of least squares In this section, you learn how to: Interpret correlation coefficients by visualizing a scatterplot matrix Fit a plane to data using the method of least squares Predict an observation using a least squares fitted plane 3.1.1 Video Video Overhead Details Show Overhead A Details. Correlation table round(cor(Term2), digits=3) education face income logface logincome numhh education 1.000 0.244 0.163 0.383 0.343 -0.064 face 0.244 1.000 0.217 0.656 0.323 0.107 income 0.163 0.217 1.000 0.251 0.518 0.142 logface 0.383 0.656 0.251 1.000 0.482 0.288 logincome 0.343 0.323 0.518 0.482 1.000 0.179 numhh -0.064 0.107 0.142 0.288 0.179 1.000 Show Overhead B Details. Scatterplot matrix Term3 &lt;- Term1[,c(&quot;numhh&quot;, &quot;education&quot;, &quot;logincome&quot;, &quot;logface&quot;)] pairs(Term3, upper.panel = NULL, gap = 0, cex.labels = 1.25) Show Overhead C Details. Visualizing a regression plane education &lt;- seq(3, 16, length = 15) logincome &lt;- seq(5, 15, length = 15) f &lt;- function(education,logincome){ r &lt;- 5 + 0.221*education + 0.354*logincome } logface &lt;- outer(education, logincome, f) persp(education, logincome, logface, theta = 30, phi = 30, expand = 0.5, ticktype = &quot;detailed&quot;) rm(education,logincome,logface) education &lt;- seq(3, 16, length = 15) logincome &lt;- seq(5, 15, length = 15) f &lt;- function(education,logincome){ r &lt;- 5 + 0.221*education + 0.354*logincome } logface &lt;- outer(education, logincome, f) persp(education, logincome, logface, theta = 30, phi = 30, expand = 0.5, ticktype = &quot;simple&quot;, #ticktype = &quot;detailed&quot;, # xlab = &quot;x1&quot;, ylab=&quot;x2&quot;,zlab=&quot;y&quot;, nticks = 1) rm(education,logincome,logface) Show Overhead D Details. Method of least squares For observation \\(\\{(y, x_1, \\ldots, x_k)\\}\\), the height of the regression plane is \\[b_0 + b_1 x_1 + \\cdots + b_k x_k .\\] Thus, \\(y - (b_0 + b_1 x_1 + \\cdots + b_k x_k)\\) represents the deviation. The sum of squared deviations is \\[SS(b_0, \\ldots, b_k) = \\sum (y - (b_0 + b_1 x_1 + \\cdots + b_k x_k))^2 .\\] The method of least squares – determine values of \\(b_0, \\ldots, b_k\\) that minimize \\(SS\\). Show Overhead E Details. Fit a multiple linear regression model Term_mlr &lt;- lm(logface ~ education + numhh + logincome, data = Term2) round(coefficients(Term_mlr), digits=4) newdata &lt;- data.frame(logincome = log(60000), education = 12, numhh = 3) exp(predict(Term_mlr, newdata)) (Intercept) education numhh logincome 2.5841 0.2064 0.3060 0.4935 1 90136 3.1.2 Exercise. Least squares and term life data Assignment Text The prior video introduced the Survey of Consumer Finances (SCF) term life data. A subset consisting of only those who purchased term life insurance, has already been read into a dataframe Term2. Suppose that you wish to predict the amount of term life insurance that someone will purchase but are uneasy about the education variable. The SCF education variable is the number of completed years of schooling and so 12 corresponds to completing high school in the US. Your sense is that, for purposes of purchasing life insurance, high school graduates and those that attend college should be treated the same. So, in this exercise, your will create a new variable, education1, that is equal to years of education for those with education less than or equal to 12 and is equal to 12 otherwise. Instructions Use the pmin() function to create the education1 variable as part of the Term2 dataframe. Check your work by examining summary statistics for the revised Term2 dataframe. Examine correlations for the revised dataframe. Using the method of least squares and the function lm(), fit a MLR model using logface as the dependent variables and using education, numhh, and logincome as explanatory variables. With this fitted model and the function predict(), predict the face amount of insurance that someone with income of 40,000, 11 years of education, and 4 people in the household would purchase. Hint. Remember that your prediction is in log dollars so you need to exponentiate it to get the results in the original dollar units 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 3.1.3 Exercise. Interpreting coefficients as proportional changes Assignment Text In a previous exercise, you fit a MLR model using logface as the outcome variable and using education, numhh, and logincome as explanatory variables; the resulting fit is in the object Term_mlr. For this fit, the coefficient associated with education is 0.2064. We now wish to interpret this regression coefficient. The typical interpretation of coefficients in a regression model is as a partial slope. That is, for the coefficient \\(b_1\\) associated with \\(x_1\\), we interpret \\(b_1\\) to be amount that the expected outcome changes per unit change in \\(x_1\\), holding the other explanatory variables fixed. For the term life example, the units of the outcome are in logarithmic dollars. So, for small values of \\(b_1\\), we can interpret this to be a proportional change in dollars. Instructions Determine least square fitted values for several selected values of education, holding other explantory variables fixed. For this part of the demonstration, we used their mean values. Determine the proportional changes. Note the relation between these values from a discrete change approximation to the regression coefficient for education equal to 0.2064. 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 3.1.4 Exercise. Interpreting coefficients as elasticities Assignment Text In a previous exercise, you fit a MLR model using logface as the outcome variable and using education, numhh, and logincome as explanatory variables; the resulting fit is in the object Term_mlr. From this fit, the coefficient associated with logincome is 0.4935. We now wish to interpret this regression coefficient. The typical interpretation of coefficients in a regression model is as a partial slope. When both \\(x_1\\) and \\(y\\) are in logarithmic units, then we can interpret \\(b_1\\) to be ratio of two percentage changes, known as an elasticity in economics. Mathematically, we summarize this as \\[ \\frac{\\partial \\ln y}{\\partial \\ln x} = \\left(\\frac{\\partial y}{y}\\right) ~/ ~\\left(\\frac{\\partial x}{x}\\right) . \\] Instructions For several selected values of logincome, determine the corresponding proportional changes. Determine least square fitted values for several selected values of logincome, holding other explantory variables fixed. Determine the corresponding proportional changes for the fitted values. Calculate the ratio of proportional changes of fitted values to those for income. Note the relation between these values (from a discrete change approximation) to the regression coefficient for logincome equal to 0.4935. Hint. When you calculate the ratio of proportional changes of fitted values to those for income, note the relation between these values (from a discrete change approximation) to the regression coefficient for logincome equal to 0.4935. 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 3.2 Statistical inference and multiple linear regresson In this section, you learn how to: Explain mean square error and residual standard error in terms of degrees of freedom Develop an ANOVA table and use it to derive the coefficient of determination Calculate and interpret the coefficient of determination adjusted for degrees of freedom Conduct a test of a regression coefficient Summarize regression coefficients using point and interval estimators 3.2.1 Video Video Overhead Details Show Overhead A Details. Goodness of fit Summarize deviations \\(s^2\\) \\(R^2\\) \\(R_a^2\\) ANOVA table Show Overhead B Details. Goodness of fit and term life Term_mlr &lt;- lm(logface ~ education + numhh + logincome, data = Term2) summary(Term_mlr) anova(Term_mlr) Call: lm(formula = logface ~ education + numhh + logincome, data = Term2) Residuals: Min 1Q Median 3Q Max -5.742 -0.868 0.055 0.909 4.719 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 2.5841 0.8464 3.05 0.0025 ** education 0.2064 0.0388 5.32 0.00000022236 *** numhh 0.3060 0.0633 4.83 0.00000225571 *** logincome 0.4935 0.0775 6.36 0.00000000083 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.53 on 271 degrees of freedom Multiple R-squared: 0.343, Adjusted R-squared: 0.335 F-statistic: 47.1 on 3 and 271 DF, p-value: &lt;0.0000000000000002 Analysis of Variance Table Response: logface Df Sum Sq Mean Sq F value Pr(&gt;F) education 1 141 140.5 60.4 0.00000000000016 *** numhh 1 94 93.7 40.3 0.00000000092509 *** logincome 1 94 94.2 40.5 0.00000000083161 *** Residuals 271 630 2.3 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Show Overhead C Details. Statistical inference hypothesis testing of a regression coefficient confidence intervals Show Overhead D Details. Statistical inference and term life Term_mlr &lt;- lm(logface ~ education + numhh + logincome, data = Term2) model_sum &lt;- summary(Term_mlr) model_sum$coefficients round(Rcmdr::Confint(Term_mlr, level = .95), digits = 3) round(confint(Term_mlr, level = .95), digits = 3) Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 2.5841 0.84643 3.053 0.0024915877562 education 0.2064 0.03883 5.316 0.0000002223619 numhh 0.3060 0.06333 4.833 0.0000022557077 logincome 0.4935 0.07754 6.365 0.0000000008316 Estimate 2.5 % 97.5 % (Intercept) 2.584 0.918 4.250 education 0.206 0.130 0.283 numhh 0.306 0.181 0.431 logincome 0.494 0.341 0.646 2.5 % 97.5 % (Intercept) 0.918 4.250 education 0.130 0.283 numhh 0.181 0.431 logincome 0.341 0.646 3.2.2 Exercise. Statistical inference and term life Assignment Text In later chapters, we will learn how to specify a model using diagnostics techniques; these techniques were used to specify face in log dollars for the outcome and similarly income in log dollars as an explanatory variable. Just to see how things work, in this exercise we will create new variables face and income that are in the original units and run a regression with these. We have already seen that rescaling by constants do not affect relationships but can be helpful with interpretations, so we define both face and income to be in thousands of dollars. A prior video introduced the term life dataframe Term2. Instructions Create Term2$face by exponentiating logface and dividing by 1000. For convenience, we are storing this variable in the data set Term2. Use the same process to create Term2$income. Run a regression using face as the outcome variable and education, numhh, and income as explanatory variables. Summarize this model and identify the residual standard error (\\(s\\)) as well as the coefficient of determination (\\(R^2\\)) and the version adjusted for degrees of freedom (\\(R_a^2\\)). eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6IiNUZXJtIDwtIHJlYWQuY3N2KFwiQ1NWRGF0YVxcXFx0ZXJtX2xpZmUuY3N2XCIsIGhlYWRlciA9IFRSVUUpXG5UZXJtIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9hc3NldHMuZGF0YWNhbXAuY29tL3Byb2R1Y3Rpb24vcmVwb3NpdG9yaWVzLzI2MTAvZGF0YXNldHMvZWZjNjRiYzJkNzhjZjZiNDhhZDJjM2Y1ZTMxODAwY2I3NzNkZTI2MS90ZXJtX2xpZmUuY3N2XCIsIGhlYWRlciA9IFRSVUUpXG5UZXJtMSA8LSBzdWJzZXQoVGVybSwgc3Vic2V0ID0gZmFjZSA+IDApXG5UZXJtMiA8LSBUZXJtMVssIGMoXCJlZHVjYXRpb25cIiwgXCJmYWNlXCIsIFwiaW5jb21lXCIsIFwibG9nZmFjZVwiLCBcImxvZ2luY29tZVwiLCBcIm51bWhoXCIpXSIsInNhbXBsZSI6IiMgQ3JlYXRlIGBUZXJtMiRmYWNlYCBhbmQgIGBUZXJtMiRpbmNvbWVgXG5UZXJtMiRmYWNlIDwtIGV4cChfX18pL19fX1xuVGVybTIkaW5jb21lIDwtIGV4cChfX18pL19fX1xuXG4jIFJ1biBhIHJlZ3Jlc3Npb24gdXNpbmcgYGZhY2VgIGFzIHRoZSBvdXRjb21lIHZhcmlhYmxlIGFuZCBgZWR1Y2F0aW9uYCwgYG51bWhoYCwgYW5kIGBpbmNvbWVgIGFzIGV4cGxhbmF0b3J5IHZhcmlhYmxlcy5cblRlcm1fbWxyMSA8LSBsbShmYWNlIH4gX19fLCBkYXRhID0gVGVybTIpXG5cbiMgU3VtbWFyaXplIHRoaXMgbW9kZWxcbnN1bW1hcnkoVGVybV9tbHIxKSIsInNvbHV0aW9uIjoiVGVybTIkZmFjZSA8LSBleHAoVGVybTIkbG9nZmFjZSkvMTAwMFxuVGVybTIkaW5jb21lIDwtIGV4cChUZXJtMiRsb2dpbmNvbWUpLzEwMDBcblRlcm1fbWxyMSA8LSBsbShmYWNlIH4gZWR1Y2F0aW9uICsgbnVtaGggKyBpbmNvbWUsIGRhdGEgPSBUZXJtMilcbnN1bW1hcnkoVGVybV9tbHIxKSIsInNjdCI6InN1Y2Nlc3NfbXNnKFwiQ29uZ3JhdHVsYXRpb25zISBDb21wYXJlIHRoZXNlIGdvb2RuZXNzIG9mIGZpdCBtZWFzdXJlcyB0byB0aG9zZSB3aGVyZSBpbmNvbWUgYW5kIGZhY2UgYXJlIGluIGxvZ2FyaXRobWljIHVuaXRzLiBBbHRob3VnaCBub3QgdGhlIG9ubHkgaW5kaWNhdG9ycywgeW91IHdpbGwgc2VlIHRoYXQgdGhlIHByb3BvcnRpb24gb2YgdmFyaWFiaWxpdHkgZXhwbGFpbmVkIChSIHNxdWFyZSkgYW5kIHRoZSBzdGF0aXN0aWNhbCBzaWduaWZpY2FuY2Ugb2YgY29lZmZpY2llbnRzIGFyZSBzdHJpa2luZ2x5IGhpZ2hlciBpbiB0aGUgbW9kZWwgd2l0aCB2YXJpYWJsZXMgaW4gbG9nZ2VkIHVuaXRzLlwiKSJ9 3.3 Binary variables In this section, you learn how to: Interpret regression coefficients associated with binary variables Use binary variables and interaction terms to create regression models that are nonlinear in the covariates 3.3.1 Video Video Overhead Details Show Overhead A Details. Binary variables We can define a new variable \\[ single= \\left\\{ \\begin{array}{ll} 0 &amp; \\text{for other respondents} \\\\ 1 &amp; \\text{for single respondents} \\end{array} \\right. \\] The variable single is said to be an indicator, or dummy, variable. To interpret coefficients, we now consider the regression function \\[ \\text{E }logface = \\beta_0 + \\beta_1 logincome + \\beta_2 single \\] - This can be expressed as two lines \\[ \\text{E }logface = \\left\\{ \\begin{array}{ll} \\beta_0 + \\beta_1 logincome &amp; \\textrm{for other respondents} \\\\ \\beta_0 + \\beta_2 + \\beta_1 logincome &amp; \\textrm{for single respondents} \\end{array} \\right. . \\] - The least squares method of calculating the estimators, and the resulting theoretical properties, are the still valid when using binary variables. Show Overhead B Details. Visualize effect of binary variables Show Overhead C Details. R script for visualization Term4 &lt;- Term1[,c(&quot;numhh&quot;, &quot;education&quot;, &quot;logincome&quot;, &quot;logface&quot;, &quot;marstat&quot;)] Term4$marstat &lt;- as.factor(Term4$marstat) table(Term4$marstat) Term4$single &lt;- 1*(Term4$marstat == 0) model_single &lt;- lm(logface ~ logincome + single, data = Term4) summary(model_single) plot(Term4$logincome,Term4$logface,xlab=&quot;logarithmic income&quot;, ylab=&quot;log face&quot;, pch= 1+16*Term4$single, col = c(&quot;red&quot;, &quot;black&quot;, &quot;black&quot;)[Term4$marstat]) Ey1 &lt;- model_single$coefficients[1]+model_single$coefficients[2]*Term4$logincome Ey2 &lt;- Ey1 + model_single$coefficients[3] lines(Term4$logincome, Ey1) lines(Term4$logincome, Ey2, col=&quot;red&quot;) 0 1 2 57 208 10 Call: lm(formula = logface ~ logincome + single, data = Term4) Residuals: Min 1Q Median 3Q Max -6.283 -0.878 0.036 0.923 5.957 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.0901 0.8864 5.74 0.000000024936927 *** logincome 0.6338 0.0778 8.15 0.000000000000013 *** single -0.8001 0.2480 -3.23 0.0014 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.61 on 272 degrees of freedom Multiple R-squared: 0.26, Adjusted R-squared: 0.255 F-statistic: 47.9 on 2 and 272 DF, p-value: &lt;0.0000000000000002 Show Overhead D Details. Interaction Terms Linear regression models are defined in terms of linear combinations of explanatory varibles but we can expand their scope through nonlinear transformations One type of nonlinear transform is the product of two varibles that is used to create what is known as an interaction variable To interpret coefficients, we now consider the regression function \\[ \\text{E }logface = \\beta_0 + \\beta_1 logincome + \\beta_2 single + \\beta_3 single*logincome \\] - This can be expressed as two lines with different slopes \\[ \\text{E }logface = \\left\\{ \\begin{array}{ll} \\beta_0 + \\beta_1 logincome &amp; \\textrm{for other respondents} \\\\ \\beta_0 + \\beta_2 + (\\beta_1 + \\beta_3) logincome &amp; \\textrm{for single respondents} \\end{array} \\right. . \\] Show Overhead E Details. Visualizing binary variables with interactions terms Term4 &lt;- Term1[,c(&quot;numhh&quot;, &quot;education&quot;, &quot;logincome&quot;, &quot;logface&quot;, &quot;marstat&quot;)] Term4$marstat &lt;- as.factor(Term4$marstat) table(Term4$marstat) Term4$single &lt;- 1*(Term4$marstat == 0) model_single_inter &lt;- lm(logface ~ logincome + single + single*logincome, data = Term4) summary(model_single_inter) plot(Term4$logincome,Term4$logface,xlab=&quot;logarithmic income&quot;, ylab=&quot;log face&quot;, pch= 1+16*Term4$single, col = c(&quot;red&quot;, &quot;black&quot;, &quot;black&quot;)[Term4$marstat]) Ey1 &lt;- model_single_inter$coefficients[1]+model_single_inter$coefficients[2]*Term4$logincome Ey2 &lt;- Ey1 + model_single_inter$coefficients[3]+model_single_inter$coefficients[4]*Term4$logincome lines(Term4$logincome, Ey1) lines(Term4$logincome, Ey2, col=&quot;red&quot;) 0 1 2 57 208 10 Call: lm(formula = logface ~ logincome + single + single * logincome, data = Term4) Residuals: Min 1Q Median 3Q Max -6.215 -0.829 0.070 0.931 5.607 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.7790 0.9255 6.24 0.000000001639 *** logincome 0.5729 0.0812 7.05 0.000000000015 *** single -7.2921 2.7422 -2.66 0.0083 ** logincome:single 0.6124 0.2576 2.38 0.0181 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.6 on 271 degrees of freedom Multiple R-squared: 0.276, Adjusted R-squared: 0.268 F-statistic: 34.4 on 3 and 271 DF, p-value: &lt;0.0000000000000002 3.3.2 Exercise. Binary variables and term life Assignment Text In the prior video, we saw how the variable single can be used with logarithmic income to explain logarithmic face amounts of term life insurance that people purchase. The coefficient associated with this variable turns out to be negative which is intuitively appealing; if an individual is single, then that person may not have the strong need to purchase financial security for others in the event of unexpected death. In this exercise, we will extend this by incorporating single into our larger regression model that contains other explanatory varibles, logincome, education and numhh. The data have been pre-loaded into the dataframe Term4. Instructions Calculate a table of correlation coefficients to examine pairwise linear relationships among the variables numhh, education, logincome, single, and logface. Fit a MLR model of logface using explanatory variables numhh, education, logincome, and single. Examine the residual standard deviation \\(s\\), the coefficient of determination \\(R^2\\), and the adjusted version \\(R_a^2\\). Also note the statistical significance of the coefficient associated with single. Repeat the MLR model fit while adding the interaction term single*logincome. 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 3.4 Categorical variables In this section, you learn how to: Represent categorical variables using a set of binary variables Interpret the regression coefficients associated with categorical variables Describe the effect of the reference level choice on the model fit 3.4.1 Video Video Overhead Details Show Overhead A Details. Categorical variables Categorical variables provide labels for observations to denote membership in distinct groups, or categories. A binary variable is a special case of a categorical variable. To illustrate, a binary variable may tell us whether or not someone has health insurance. A categorical variable could tell us whether someone has (i) private individual health insurance, (ii) private group insurance, (iii) public insurance or (iv) no health insurance. For categorical variables, there may or may not be an ordering of the groups. For health insurance, it is difficult to say which is ‘larger’, private individual versus public health insurance (such as Medicare). However, for education, we may group individuals from a dataset into ‘low’, ‘intermediate’ and ‘high’ years of education. Factor is another term used for a (unordered) categorical explanatory variable. Show Overhead B Details. Term life example We studied y = logface, the amount that the company will pay in the event of the death of the named insured (in logarithmic dollars), focusing on the explanatory variables logincome, education, and numhh. We now supplement this by including the categorical variable, marstat, that is the marital status of the survey respondent. This may be: 1, for married 2, for living with partner 0, for other (SCF actually breaks this category into separated, divorced, widowed, never married and inapplicable, for persons age 17 or less or no further persons) Show Overhead C Details. Term life boxplots # Pre-exercise code Term &lt;- read.csv(&quot;CSVData\\\\term_life.csv&quot;, header = TRUE) #Term &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/efc64bc2d78cf6b48ad2c3f5e31800cb773de261/term_life.csv&quot;, header = TRUE) Term1 &lt;- subset(Term, subset = face &gt; 0) Term4 &lt;- Term1[,c(&quot;numhh&quot;, &quot;education&quot;, &quot;logincome&quot;, &quot;marstat&quot;, &quot;logface&quot;)] Term4$single &lt;- 1*(Term4$marstat == 0) Term4$marstat&lt;- as.factor(Term4$marstat) boxplot(logface ~ marstat, ylab = &quot;log face&quot;, xlab = &quot;Marital Status&quot;, data = Term4) table(Term4$marstat) # SUMMARY BY LEVEL OF MARSTAT library(Rcmdr) numSummary(Term4[, &quot;logface&quot;], groups = Term4$marstat, statistics = c(&quot;mean&quot;, &quot;sd&quot;)) numSummary(Term4[, &quot;logface&quot;], statistics = c(&quot;mean&quot;, &quot;sd&quot;)) 0 1 2 57 208 10 mean sd data:n 0 10.96 1.566 57 1 12.33 1.822 208 2 10.83 2.001 10 mean sd n 11.99 1.871 275 Show Overhead D Details. Regression with a categorical variable Term4$marstat &lt;- as.factor(Term4$marstat) Term4$marstat &lt;- relevel(Term4$marstat, ref = &quot;2&quot;) summary(lm(logface ~ logincome+education+numhh+marstat, data = Term4)) Call: lm(formula = logface ~ logincome + education + numhh + marstat, data = Term4) Residuals: Min 1Q Median 3Q Max -5.888 -0.851 0.112 0.847 4.517 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 2.6054 0.9522 2.74 0.00663 ** logincome 0.4515 0.0787 5.74 0.000000026 *** education 0.2047 0.0386 5.30 0.000000242 *** numhh 0.2477 0.0694 3.57 0.00042 *** marstat0 0.2323 0.5328 0.44 0.66316 marstat1 0.7894 0.4953 1.59 0.11217 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.51 on 269 degrees of freedom Multiple R-squared: 0.358, Adjusted R-squared: 0.346 F-statistic: 30 on 5 and 269 DF, p-value: &lt;0.0000000000000002 Show Overhead E Details. t-ratios depend on the reference level \\[ \\begin{array}{l|rr|rr|rr} \\hline &amp; \\text{Model 1}&amp;&amp; \\text{Model 2}&amp;&amp; \\text{Model 3}&amp;\\\\ \\hline \\text{Var}&amp; \\text{Coef} &amp; \\text{t-stat} &amp; \\text{Coef} &amp; \\text{t-stat} &amp;\\text{Coef} &amp; \\text{t-stat} \\\\\\hline logincome &amp; 0.452 &amp; 5.74 &amp; 0.452 &amp; 5.74 &amp; 0.452 &amp; 5.74 \\\\ education &amp;0.205 &amp; 5.30 &amp;0.205 &amp; 5.30&amp;0.205 &amp; 5.30 \\\\ numhh &amp; 0.248 &amp; 3.57 &amp; 0.248 &amp; 3.57 &amp; 0.248 &amp; 3.57 \\\\\\hline \\text{Intercept} &amp; 3.395 &amp; 3.77 &amp; 2.605&amp; 2.74 &amp; 2.838 &amp; 3.34\\\\ \\text{mar=0} &amp; -0.557 &amp; -2.15&amp; 0.232 &amp; 0.44\\\\ \\text{mar=1} &amp; &amp; &amp; 0.789 &amp; 1.59 &amp; 0.557 &amp; 2.15\\\\ \\text{mar=2}&amp; -0.789 &amp; -1.59 &amp; &amp; &amp; -0.232 &amp; -0.44\\\\ \\hline \\end{array} \\] 3.4.2 Exercise. Categorical variables and Wisconsin hospital costs Assignment Text This exercise examines the impact of various predictors on hospital charges. Identifying predictors of hospital charges can provide direction for hospitals, government, insurers and consumers in controlling these variables that in turn leads to better control of hospital costs. The data, from 1989, are aggregated by: drg, diagnostic related groups of costs, payer, type of health care provider (Fee for service, HMO, and other), and hsa, nine major geographic areas in Wisconsin. Some preliminary analysis of the data has already been done. In this exercise, we will analyze logcharge, the logarithm of total hospital charges per number of discharges, in terms of log_numdschg, the logarithm of the number of discharges. In the dataframe Hcost which has been loaded in advance, we restrict consideration to three types of drgs, numbers 209, 391, and 431. Instructions Fit a basic linear regression model using logarithmic number of discharges to predict logarithmic hospital costs and superimposed the fitted regression line on the scatter plot. Produce a scatter plot of logarithmic number of discharges to predict logarithmic hospital costs. Allow plotting symbols and colors to vary by diagnostic related group. Fit a MLR model using logarithmic number of discharges to predict logarithmic hospital costs, allowing intercepts and slopes to vary by diagnostic related groups. Superimpose the fits from the MLR model on the scatter plot of logarithmic number of discharges to predict logarithmic hospital costs. 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 3.5 General linear hypothesis In this section, you learn how to: Jointly test the significance of a set of regression coefficients using the general linear hypothesis Conduct a test of a regression coefficient versus one- or two-side alternatives 3.5.1 Video Video Overhead Details Show Overhead A Details. Testing the significance of a categorical variable Term &lt;- read.csv(&quot;CSVData\\\\term_life.csv&quot;, header = TRUE) #Term &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/efc64bc2d78cf6b48ad2c3f5e31800cb773de261/term_life.csv&quot;, header = TRUE) Term1 &lt;- subset(Term, subset = face &gt; 0) Term4 &lt;- Term1[,c(&quot;numhh&quot;, &quot;education&quot;, &quot;logincome&quot;, &quot;marstat&quot;, &quot;logface&quot;)] Term_mlr1 &lt;- lm(logface ~ logincome + education + numhh + as.factor(Term4$marstat), data = Term4) anova(Term_mlr1) Term_mlr2 &lt;- lm(logface ~ logincome + education + numhh, data = Term4) Fstat &lt;- (anova(Term_mlr2)$`Sum Sq`[4] - anova(Term_mlr1)$`Sum Sq`[5])/(2*anova(Term_mlr1)$`Mean Sq`[5]) Fstat cat(&quot;p-value is&quot;, 1 - pf(Fstat, df1 = 2 , df2 = anova(Term_mlr1)$Df[5])) Analysis of Variance Table Response: logface Df Sum Sq Mean Sq F value Pr(&gt;F) logincome 1 223 222.6 97.28 &lt; 0.0000000000000002 education 1 52 51.5 22.50 0.0000034 numhh 1 54 54.3 23.74 0.0000019 as.factor(Term4$marstat) 2 15 7.4 3.24 0.041 Residuals 269 616 2.3 logincome *** education *** numhh *** as.factor(Term4$marstat) * Residuals --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 [1] 3.236 p-value is 0.04085 Show Overhead B Details. Overview of the general linear hypothesis The likelihood ratio is a general statistical test procedure that compares a model to a subset The general linear hypothesis test procedure is similar. Start with a (large) linear regression model, examine the fit to a set of data Compare this to smaller model that is a subset of the large model. “Subset” is the sense that regression coefficients from the small model are linear combinations of regression coefficients of the large model (e.g., set them to zero) Although the likelihood ratio test is more generally available, the general linear hypothesis test is more accurate for smaller data sets (for normally distributed data) Show Overhead C Details. Procedure for conducting the general linear hypothesis Run the full regression and get the error sum of squares and mean square error, which we label as \\((Error SS)_{full}\\) and \\(s^2_{full}\\), respectively. Run a reduced regression and get the error sum of squares, labelled \\((Error SS)_{reduced}\\). Using \\(p\\) for the number of linear restrictions, calculate \\[ F-ratio = \\frac{(Error SS)_{reduced}-(Error SS)_{full}}{p s^2_{full}} . \\] The probability value is \\(p-value = \\Pr(F_{p,df} &gt; F-ratio)\\) where \\(F_{p,df}\\) has an F distribution with degrees of freedom p and df, respectively. (Here, df is the degrees of freedom for the full model.) Show Overhead D Details. The general linear hypothesis for a single variable Suppose that you wish to test the hypothesisthat a regression coefficient equals 0. One could use the general linear hypothsis procedure with \\(p=1\\). One could also examine the corresponding \\(t-ratio\\). Which is correct? Both. One can show that \\((t-ratio)^2 = F-ratio\\), so they are equivalent statistics. The general linear hypothesis is useful because it can be extended to multiple coefficients. The t-ratio is useful because it can be used to examine one-sided alternative hypotheses. 3.5.2 Exercise. Hypothesis testing and term life Assignment Text With our Term life data, let us compare a model based on the binary variable that indicates whether a survey respondent is single versus the more complex marital status, marstat. In principle, more detailed information is better. But, it may be that the additional information in marstat, compared to single, does not help fit the data in a significantly better way. As part of the preparatory work, the dataframe Term4 is available that includes the binary variable single and the factor marstat. Moreover, the regression object Term_mlr contains information in a multiple linear regression fit of logface on the base explanatory variables ’logincome,education, andnumhh`. Instructions Fit a MLR model using the base explanatory variables plus single and another model using the base variables plus marstat. Use the F test to decide whether the additional complexity marstat is warranted by calculating the p-value associated with this test. Fit a MLR model using the base explanatory variables plus single interacted with logincome and another model using the base variables plus marstat interacted with logincome. Use the F test to decide whether the additional complexity marstat is warranted by calculating the p-value associated with this test. Hint Here is the code to calculate it by hand Fstat12 &lt;- (anova(Term_mlr1)$`Sum Sq`[5] - anova(Term_mlr2)$`Sum Sq`[5])/(1*anova(Term_mlr2)$`Mean Sq`[5]) Fstat12 cat(&quot;p-value is&quot;, 1 - pf(Fstat12, df1 = 1 , df2 = anova(Term_mlr2)$Df[5])) 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 3.5.3 Exercise. Hypothesis testing and Wisconsin hospital costs Assignment Text In a previous exercise, you were introduced to a dataset with hospital charges aggregated by: drg, diagnostic related groups of costs, payer, type of health care provider (Fee for service, HMO, and other), and hsa, nine major geographic areas. We continue our analysis of the outcome variable logcharge, the logarithm of total hospital charges per number of discharges, in terms of log_numdschg, the logarithm of the number of discharges, as well as the three categorical variables used in the aggregation. As before, we restrict consideration to three types of drgs, numbers 209, 391, and 431 that has been preloaded in the dataframe Hcost1. Instructions Fit a basic linear regression model using logarithmic hospital costs as the outcome variable and explanatory variable logarithmic number of discharges. Fit a MLR model using logarithmic hospital costs as the outcome variable and explanatory variables logarithmic number of discharges and the categorical variable diagnostic related group. Identify the F statistic and p value that test the importance of diagnostic related group. Fit a MLR model using logarithmic hospital costs as the outcome variable and explanatory variable logarithmic number of discharges interacted with diagnostic related group. Identify the F statistic and p value that test the importance of diagnostic related group interaction with logarithmic number of discharges. Calculate a coefficient of determination, \\(R^2\\), for each of these models as well as for a model using logarithmic number of discharges and categorical variable hsa as predictors. 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 3.5.4 Exercise. Hypothesis testing and auto claims Assignment Text As an actuarial analyst, you are working with a large insurance company to help them understand their claims distribution for their private passenger automobile policies. You have available claims data for a recent year, consisting of: state: codes 01 through 17 used, with each code randomly assigned to an actual individual state class: rating class of operator, based on age, gender, marital status, and use of vehicle gender: operator gender age: operator age paid: amount paid to settle and close a claim. You are focusing on older drivers, 50 and higher, for which there are n = 6,773 claims available. Instructions Run a regression of logpaid on age. Is age a statistically significant variable? To respond to this question, use a formal test of hypothesis. State your null and alternative hypotheses, decision-making criterion, and your decision-making rule. Also comment on the goodness of fit of this variable. Consider using class as a single explanatory variable. Use the one factor to estimate the model and respond to the following questions. b (i). What is the point estimate of claims in class C7, drivers 50-69, driving to work or school, less than 30 miles per week with annual mileage under 7500, in natural logarithmic units? b (ii). Determine the corresponding 95% confidence interval of expected claims, in natural logarithmic units. b (iii). Convert the 95% confidence interval of expected claims that you determined in part b(ii) to dollars. Run a regression of logpaid on age, gender and the categorical variables state and class. c (i). Is gender a statistically significant variable? To respond to this question, use a formal test of hypothesis. State your null and alternative hypotheses, decision-making criterion, and your decision-making rule. c (ii). Is class a statistically significant variable? To respond to this question, use a formal test of hypothesis. State your null and alternative hypotheses, decision-making criterion, and your decision-making rule. c (iii). Use the model to provide a point estimate of claims in dollars (not log dollars) for a male age 60 in STATE 2 in class C7. c (iv). Write down the coefficient associated with class C7 and interpret this coefficient. Variable: logpaid mean sd 0% 50% 100% n C1 6.941 1.0640 3.126 6.855 9.687 726 C11 6.952 1.0739 2.251 6.921 10.183 1151 C1A 6.866 1.0724 4.294 6.830 9.460 77 C1B 6.998 1.0680 3.219 6.934 10.058 424 C1C 6.786 1.1098 3.985 6.909 8.892 38 C2 6.801 0.9479 3.689 6.747 9.091 61 C6 6.926 1.1146 2.708 6.919 10.987 911 C7 6.901 1.0581 2.879 6.865 11.002 913 C71 6.954 1.0380 3.219 6.867 10.171 1129 C72 7.183 0.9884 4.708 7.116 9.982 85 C7A 7.064 1.0208 4.094 7.039 9.526 113 C7B 7.072 1.1028 3.072 7.015 10.261 686 C7C 7.244 0.9436 4.971 7.090 9.610 81 F1 7.004 0.9959 3.912 6.983 8.657 29 F11 6.804 1.2120 3.555 6.652 10.044 40 F6 6.910 1.1932 3.911 7.008 9.405 157 F7 6.577 0.8966 3.912 6.562 8.358 59 F71 6.935 0.9831 4.449 7.020 9.081 93 Variable: paid mean sd 0% 50% 100% n C1 1827 2375.4 22.79 948.9 16100 726 C11 1838 2493.4 9.50 1013.8 26439 1151 C1A 1742 2461.2 73.29 925.5 12841 77 C1B 1919 2580.1 25.00 1026.7 23344 424 C1C 1479 1561.0 53.80 1001.7 7275 38 C2 1356 1454.8 40.00 851.2 8878 61 C6 1873 3131.0 15.00 1011.2 59114 911 C7 1770 2974.2 17.79 957.7 60000 913 C71 1802 2395.3 25.00 960.4 26146 1129 C72 2186 2892.8 110.78 1231.2 21631 85 C7A 1910 2103.5 60.00 1139.9 13715 113 C7B 2123 2889.7 21.58 1113.1 28593 686 C7C 2210 2548.2 144.19 1200.0 14909 81 F1 1630 1418.6 50.00 1078.0 5748 29 F11 1976 3780.1 35.00 774.8 23016 40 F6 1857 2188.6 49.95 1105.0 12152 157 F7 1036 932.5 50.00 707.4 4264 59 F71 1601 1652.0 85.50 1118.7 8785 93 Call: lm(formula = logpaid ~ class, data = AutoC) Residuals: Min 1Q Median 3Q Max -4.700 -0.691 -0.044 0.713 4.101 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 6.94094 0.03970 174.84 &lt;0.0000000000000002 *** classC11 0.01056 0.05070 0.21 0.835 classC1A -0.07543 0.12820 -0.59 0.556 classC1B 0.05742 0.06538 0.88 0.380 classC1C -0.15471 0.17801 -0.87 0.385 classC2 -0.13944 0.14259 -0.98 0.328 classC6 -0.01506 0.05322 -0.28 0.777 classC7 -0.03977 0.05319 -0.75 0.455 classC71 0.01273 0.05089 0.25 0.802 classC72 0.24172 0.12263 1.97 0.049 * classC7A 0.12276 0.10817 1.13 0.257 classC7B 0.13151 0.05696 2.31 0.021 * classC7C 0.30260 0.12531 2.41 0.016 * classF1 0.06296 0.20256 0.31 0.756 classF11 -0.13689 0.17373 -0.79 0.431 classF6 -0.03055 0.09415 -0.32 0.746 classF7 -0.36387 0.14481 -2.51 0.012 * classF71 -0.00548 0.11781 -0.05 0.963 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.07 on 6755 degrees of freedom Multiple R-squared: 0.00505, Adjusted R-squared: 0.00254 F-statistic: 2.02 on 17 and 6755 DF, p-value: 0.00786 Call: lm(formula = logpaid ~ state, data = AutoC) Residuals: Min 1Q Median 3Q Max -4.656 -0.685 -0.042 0.701 4.079 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 6.8220 0.0829 82.33 &lt; 0.0000000000000002 *** stateSTATE 02 0.1174 0.0888 1.32 0.18613 stateSTATE 03 0.0222 0.1007 0.22 0.82525 stateSTATE 04 0.0519 0.0926 0.56 0.57489 stateSTATE 06 0.3074 0.0933 3.30 0.00099 *** stateSTATE 07 0.1013 0.1054 0.96 0.33656 stateSTATE 10 0.2125 0.1049 2.03 0.04279 * stateSTATE 11 0.1746 0.3654 0.48 0.63275 stateSTATE 12 0.4088 0.1072 3.81 0.00014 *** stateSTATE 13 0.2179 0.1111 1.96 0.04989 * stateSTATE 14 0.0649 0.1167 0.56 0.57830 stateSTATE 15 0.0848 0.0860 0.99 0.32396 stateSTATE 17 0.2241 0.0959 2.34 0.01944 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.07 on 6760 degrees of freedom Multiple R-squared: 0.00811, Adjusted R-squared: 0.00635 F-statistic: 4.61 on 12 and 6760 DF, p-value: 0.000000175 Analysis of Variance Table Response: logpaid Df Sum Sq Mean Sq F value Pr(&gt;F) state 12 63 5.25 4.61 0.00000017 *** Residuals 6760 7705 1.14 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Call: lm(formula = logpaid ~ class + state + age + gender, data = AutoC) Residuals: Min 1Q Median 3Q Max -4.727 -0.680 -0.043 0.707 4.181 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 6.97492 0.14014 49.77 &lt; 0.0000000000000002 *** classC11 0.05557 0.05170 1.08 0.28241 classC1A -0.11059 0.12824 -0.86 0.38851 classC1B 0.02267 0.06639 0.34 0.73274 classC1C -0.16035 0.17806 -0.90 0.36784 classC2 -0.18394 0.14278 -1.29 0.19772 classC6 0.05756 0.05877 0.98 0.32739 classC7 -0.02185 0.05453 -0.40 0.68861 classC71 0.02462 0.05305 0.46 0.64257 classC72 0.26006 0.12345 2.11 0.03519 * classC7A 0.11932 0.10888 1.10 0.27318 classC7B 0.12420 0.05910 2.10 0.03563 * classC7C 0.29902 0.12630 2.37 0.01793 * classF1 0.13005 0.20238 0.64 0.52049 classF11 -0.05868 0.17439 -0.34 0.73650 classF6 0.06861 0.09827 0.70 0.48508 classF7 -0.30997 0.14527 -2.13 0.03290 * classF71 0.02946 0.11885 0.25 0.80424 stateSTATE 02 0.09854 0.08928 1.10 0.26976 stateSTATE 03 0.00630 0.10105 0.06 0.95027 stateSTATE 04 0.01927 0.09371 0.21 0.83712 stateSTATE 06 0.28385 0.09410 3.02 0.00257 ** stateSTATE 07 0.07659 0.10636 0.72 0.47147 stateSTATE 10 0.18180 0.10545 1.72 0.08474 . stateSTATE 11 0.17227 0.36557 0.47 0.63749 stateSTATE 12 0.38801 0.10833 3.58 0.00034 *** stateSTATE 13 0.18849 0.11198 1.68 0.09237 . stateSTATE 14 0.06814 0.11672 0.58 0.55937 stateSTATE 15 0.06557 0.08662 0.76 0.44911 stateSTATE 17 0.19940 0.09691 2.06 0.03968 * age -0.00302 0.00169 -1.79 0.07391 . genderM 0.03895 0.02691 1.45 0.14775 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.07 on 6741 degrees of freedom Multiple R-squared: 0.0136, Adjusted R-squared: 0.00911 F-statistic: 3.01 on 31 and 6741 DF, p-value: 0.0000000435 Analysis of Variance Table Response: logpaid Df Sum Sq Mean Sq F value Pr(&gt;F) class 17 39 2.31 2.03 0.0073 ** state 12 61 5.08 4.47 0.00000034 *** age 1 3 3.43 3.02 0.0825 . gender 1 2 2.38 2.10 0.1477 Residuals 6741 7662 1.14 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Submission Correctness Tests (SCT) success_msg(“Congratulations!”) "],
["variable-selection.html", "Chapter 4 Variable Selection 4.1 An iterative approach to data analysis and modeling 4.2 Automatic variable selection procedures 4.3 Residual analysis 4.4 Unusual observations 4.5 Collinearity 4.6 Selection criteria", " Chapter 4 Variable Selection Chapter description This chapter describes tools and techniques to help you select variables to enter into a linear regression model, beginning with an iterative model selection process. In applications with many potential explanatory variables, automatic variable selection procedures are available that will help you quickly evaluate many models. Nonetheless, automatic procedures have serious limitations including the inability to account properly for nonlinearities such as the impact of unusual points; this chapter expands upon the Chapter 2 discussion of unusual points. It also describes collinearity, a common feature of regression data where explanatory variables are linearly related to one another. Other topics that impact variable selection, including out-of-sample validation, are also introduced. 4.1 An iterative approach to data analysis and modeling In this section, you learn how to: Describe the iterative approach to data analysis and modeling. 4.1.1 Video Video Overhead Details Show Overhead A Details. Iterative approach Model formulation stage Fitting Diagnostic checking - the data and model must be consistent with one another before additional inferences can be made. plot.new() par(mar=c(0,0,0,0), cex=0.9) plot.window(xlim=c(0,18),ylim=c(-5,5)) text(1,3,labels=&quot;DATA&quot;,adj=0, cex=0.8) text(1,0,labels=&quot;PLOTS&quot;,adj=0, cex=0.8) text(1,-3,labels=&quot;THEORY&quot;,adj=0, cex=0.8) text(3.9,0,labels=&quot;MODEL\\nFORMULATION&quot;,adj=0, cex=0.8) text(8.1,0,labels=&quot;FITTING&quot;,adj=0, cex=0.8) text(11,0,labels=&quot;DIAGNOSTIC\\nCHECKING&quot;,adj=0, cex=0.8) text(15,0,labels=&quot;INFERENCE&quot;,adj=0, cex=0.8) text(14.1,0.5,labels=&quot;OK&quot;,adj=0, cex=0.6) rect(0.8,2.0,2.6,4.0) arrows(1.7,2.0,1.7,1.0,code=2,lwd=2,angle=25,length=0.10) rect(0.8,-1.0,2.6,1.0) arrows(1.7,-2.0,1.7,-1.0,code=2,lwd=2,angle=25,length=0.10) rect(0.8,-4.0,2.6,-2.0) arrows(2.6,0,3.2,0,code=2,lwd=2,angle=25,length=0.10) x&lt;-c(5,7.0,5,3.2) y&lt;-c(2,0,-2,0) polygon(x,y) arrows(7.0,0,8.0,0,code=2,lwd=2,angle=25,length=0.10) rect(8.0,-1.0,9.7,1.0) arrows(9.7,0,10.2,0,code=2,lwd=2,angle=25,length=0.10) x1&lt;-c(12,14.0,12,10.2) y1&lt;-c(2,0,-2,0) polygon(x1,y1) arrows(14.0,0,14.8,0,code=2,lwd=2,angle=25,length=0.10) rect(14.8,-1.0,17.5,1.0) arrows(12,-2.0,12,-3,code=2,lwd=2,angle=25,length=0.10) arrows(12,-3.0,5,-3,code=2,lwd=2,angle=25,length=0.10) arrows(5,-3.0,5,-2,code=2,lwd=2,angle=25,length=0.10) Show Overhead B Details. Many possible models \\[ \\begin{array}{l|r} \\hline \\text{E }y = \\beta _{0} &amp; \\text{1 model with no variables } \\\\ \\text{E }y = \\beta _{0}+\\beta_1 x_{i}, &amp; \\text{4 models with one variable} \\\\ \\text{E }y = \\beta _{0}+\\beta_1 x_{i}+\\beta_{2} x_{j}, &amp; \\text{6 models with two variables} \\\\ \\text{E }y = \\beta _{0}+\\beta_{1} x_{1}+\\beta_{2} x_{j} +\\beta_{3} x_{k},&amp; \\text{4 models with three variables} \\\\ \\text{E }y = \\beta_{0} + \\beta_{1} x_{1} + \\beta_{2} x_{2} +\\beta_{3} x_{3}+\\beta_{4} x_{4} &amp; \\text{1 model with all variables} \\\\ \\hline \\end{array} \\] With k explanatory variables, there are \\(2^k\\) possible linear models There are infinitely many nonlinear ones!! Show Overhead C Details. Model validation Model validation is the process of confirming our proposed model. Concern: data-snooping - fitting many models to a single set of data. Response to concern: out-of-sample validation. Divide the data into model development, or training and validation, or test, subsamples. par(mai=c(0,0.1,0,0)) plot.new() plot.window(xlim=c(0,18),ylim=c(-10,10)) rect(1,-1.2,14,1.2) rect(7,4,15,8) rect(1,-8,6,-4) x&lt;-seq(1.5,9,length=6) y&lt;-rep(0,6) text(x,y,labels=c(1:6),cex=1.5) x1&lt;-seq(10.5,11.5,length=3) y1&lt;-rep(0,3) text(x1,y1,labels=rep(&quot;.&quot;,3),cex=3) text(13,0,labels=&quot;n&quot;,cex=1.5) text(15,0,labels=&quot;ORIGINAL\\nSAMPLE\\nSIZE n&quot;,adj=0) text(7.5,6,labels=&quot;MODEL DEVELOPMENT\\nSUBSAMPLE SIZE&quot;,adj=0) text(12.5,5.3, expression(n[1]), adj=0, cex=1.1) text(1.4,-6,labels=&quot;VALIDATION\\nSUBSAMPLE\\nSIZE&quot;,adj=0) text(2.8,-7.2,expression(n[2]),adj=0, cex=1.1) arrows(1.8,0.8,8.3,3.9,code=2,lwd=2,angle=15,length=0.2) arrows(4.8,0.8,9,3.8,code=2,lwd=2,angle=15,length=0.2) arrows(9.1,0.9,9.5,3.8,code=2,lwd=2,angle=15,length=0.2) arrows(12.8,0.8,10,3.8,code=2,lwd=2,angle=15,length=0.2) arrows(2.9,-0.9,2.5,-3.8,code=2,lwd=2,angle=15,length=0.2) arrows(5.9,-0.9,3.1,-3.8,code=2,lwd=2,angle=15,length=0.2) arrows(7.4,-0.9,3.5,-3.8,code=2,lwd=2,angle=15,length=0.2) 4.1.2 MC Exercise. An iterative approach to data modeling Which of the following is not true? A. Diagnostic checking reveals symptoms of mistakes made in previous specifications. B. Diagnostic checking provides ways to correct mistakes made in previous specifications. C. Model formulation is accomplished by using prior knowledge of relationships. D. Understanding theoretical model properties is not really helpful when matching a model to data or inferring general relationships based on the data. 4.2 Automatic variable selection procedures In this section, you learn how to: Identify some examples of automatic variable selection procedures Describe the purpose of automatic variable selection procedures and their limitations Describe “data-snooping” 4.2.1 Video Video Overhead Details Show Overhead A Details. Classic stepwise regression algorithm Suppose that the analyst has identified one variable as the outcome, \\(y\\), and \\(k\\) potential explanatory variables, \\(x_1, x_2, \\ldots, x_k\\). (i). Consider all possible regressions using one explanatory variable. Choose the one with the highest t-statistic. (ii). Add a variable to the model from the previous step. The variable to enter is with the highest t-statistic. (iii). Delete a variable to the model from the previous step. Delete the variable with the small t-statistic if the statistic is less than, e.g., 2 in absolute value. (iv). Repeat steps (ii) and (iii) until all possible additions and deletions are performed. Show Overhead B Details. Drawbacks of stepwise regression The procedure “snoops” through a large number of models and may fit the data “too well.” There is no guarantee that the selected model is the best. The algorithm does not consider models that are based on nonlinear combinations of explanatory variables. It ignores the presence of outliers and high leverage points. Show Overhead C Details. Data-snooping in stepwise regression Generate \\(y\\) and \\(x_1 - x_{50}\\) using a random number generator By design, there is no relation between \\(y\\) and \\(x_1 - x_{50}\\). But, through stepwise regression, we “discover” a relationship that explains 14% of the variation!!! Call: lm(formula = y ~ xvar27 + xvar29 + xvar32, data = X) Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -0.04885 0.09531 -0.513 0.6094 xvar27 0.21063 0.09724 2.166 0.0328 * xvar29 0.24887 0.10185 2.443 0.0164 * xvar32 0.25390 0.09823 2.585 0.0112 * Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.9171 on 96 degrees of freedom Multiple R-squared: 0.1401, Adjusted R-squared: 0.1132 F-statistic: 5.212 on 3 and 96 DF, p-value: 0.002233 Show Overhead D Details. Variants of stepwise regression This uses the R function step() The option direction can be used to change how variables enter Forward selection. Add one variable at a time without trying to delete variables. Backwards selection. Start with the full model and delete one variable at a time without trying to add variables. The option scope can be used to specify which variables must be included Show Overhead E Details. Automatic variable selection procedures Stepwise regression is a type of automatic variable selection procedure. These procedures are useful because they can quickly search through several candidate models. They mechanize certain routine tasks and are excellent at discovering patterns in data. They are so good at detecting patterns that they analyst must be wary of overfitting (data-snooping) They can miss certain patterns (nonlinearities, unusual points) A model suggested by automatic variable selection procedures should be subject to the same careful diagnostic checking procedures as a model arrived at by any other means 4.2.2 Exercise. Data-snooping in stepwise regression Assignment Text Automatic variable selection procedures, such as the classic stepwise regression algorithm, are very good at detecting patterns. Sometimes they are too good in the sense that they detect patterns in the sample that are not evident in the population from which the data are drawn. The detect “spurious” patterns. This exercise illustrates this phenomenom by using a simulation, designed so that the outcome variable (y) and the explanatory variables are mutually independent. So, by design, there is no relationship between the outcome and the explanatory variables. As part of the code set-up, we have n = 100 observations generated of the outcome y and 50 explanatory variables, xvar1 through xvar50. As anticipated, collections of explanatory variables are not statistically significant. However, with the step() function, you will find some statistically significant relationships! Instructions Fit a basic linear regression model and MLR model with the first ten explanatory variables. Compare the models via an F test. Fit a multiple linear regression model with all fifty explanatory variables. Compare this model to the one with ten variables via an F test. Use the step function to find the best model starting with the fitted model containing all fifty explanatory variables and summarize the fit. Hint. The code shows stepwise regression using BIC, a criterion that results in simpler models than AIC. For AIC, use the option k=2 in the [step()] function (the default) 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 4.3 Residual analysis In this section, you learn how to: Explain how residual analysis can be used to improve a model specification Use relationships between residuals and potential explanatory variables to improve model specification 4.3.1 Video Video Overhead Details Show Overhead A Details. Residual analysis Use \\(e_i = y_i - \\hat{y}_i\\) as the ith residual. Later, I will discuss rescaling by, for example, \\(s\\), to get a standardized residual. Role of residuals: If the model formulation is correct, then residuals should be approximately equal to random errors or “white noise.” Method of attack: Look for patterns in the residuals. Use this information to improve the model specification. Show Overhead B Details. Using residuals to select explanatory variables Residual analysis can help identify additional explanatory variables that may be used to improve the formulation of the model. If the model is correct, then residuals should resemble random errors and contain no discernible patterns. Thus, when comparing residuals to explanatory variables, we do not expect any relationships. If we do detect a relationship, then this suggests the need to control for this additional variable. Show Overhead C Details. Detecting relationships between residuals and explanatory variables Calculate summary statistics and display the distribution of residuals to identify outliers. Calculate the correlation between the residuals and additional explanatory variables to search for linear relationships. Create scatter plots between the residuals and additional explanatory variables to search for nonlinear relationships. 4.3.2 Exercise. Residual analysis and risk manager survey Assignment Text This exercise examines data, pre-loaded in the dataframe survey, from a survey on the cost effectiveness of risk management practices. Risk management practices are activities undertaken by a firm to minimize the potential cost of future losses, such as the event of a fire in a warehouse or an accident that injures employees. This exercise develops a model that can be used to make statements about cost of managing risks. A measure of risk management cost effectiveness, logcost, is the outcome variable. This variable is defined as total property and casualty premiums and uninsured losses as a proportion of total assets, in logarithmic units. It is a proxy for annual expenditures associated with insurable events, standardized by company size. Explanatory variables include logsize, the logarithm of total firm assets, and indcost, a measure of the firm’s industry risk. Instructions Fit and summarize a MLR model using logcost as the outcome variable and logsize and indcost as explanatory variables. Plot residuals of the fitted model versus indcost and superimpose a locally fitted line using the R function lowess(). Fit and summarize a MLR model of logcost on logsize, indcost and a squared version of indcost. Plot residuals of the fitted model versus `indcost’ and superimpose a locally fitted line using lowess(). Hint. You can access model residuals using mlr.survey1$residuals or mlr.survey1($residuals) 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 4.3.3 Exercise. Added variable plot and refrigerator prices Assignment Text What characteristics of a refrigerator are important in determining its price (price)? We consider here several characteristics of a refrigerator, including the size of the refrigerator in cubic feet (rsize), the size of the freezer compartment in cubic feet (fsize), the average amount of money spent per year to operate the refrigerator (ecost, for energy cost), the number of shelves in the refrigerator and freezer doors (shelves), and the number of features (features). The features variable includes shelves for cans, see-through crispers, ice makers, egg racks and so on. Both consumers and manufacturers are interested in models of refrigerator prices. Other things equal, consumers generally prefer larger refrigerators with lower energy costs that have more features. Due to forces of supply and demand, we would expect consumers to pay more for these refrigerators. A larger refrigerator with lower energy costs that has more features at the similar price is considered a bargain to the consumer. How much extra would the consumer be willing to pay for this additional space? A model of prices for refrigerators on the market provides some insight to this question. To this end, we analyze data from n = 37 refrigerators. Instructions # Pre-exercise code Refrig &lt;- read.table(&quot;CSVData\\\\Refrig.csv&quot;, header = TRUE, sep = &quot;,&quot;) summary(Refrig) Refrig1 &lt;- Refrig[c(&quot;price&quot;, &quot;ecost&quot;, &quot;rsize&quot;, &quot;fsize&quot;, &quot;shelves&quot;, &quot;s_sq_ft&quot;, &quot;features&quot;)] round(cor(Refrig1), digits = 3) refrig_mlr1 &lt;- lm(price ~ rsize + fsize + shelves + features, data = Refrig) summary(refrig_mlr1) Refrig$residuals1 &lt;- residuals(refrig_mlr1) refrig_mlr2 &lt;- lm(ecost ~ rsize + fsize + shelves + features, data = Refrig) summary(refrig_mlr2) Refrig$residuals2 &lt;- residuals(refrig_mlr2) plot(Refrig$residuals2, Refrig$residuals1) library(Rcmdr) refrig_mlr3 &lt;- lm(price ~ rsize + fsize + shelves + features + ecost, data = Refrig) avPlots(refrig_mlr3, terms = &quot;ecost&quot;) price ecost rsize fsize Min. : 460 Min. :60.0 Min. :12.6 Min. :4.10 1st Qu.: 545 1st Qu.:66.0 1st Qu.:12.9 1st Qu.:4.40 Median : 590 Median :68.0 Median :13.2 Median :5.10 Mean : 626 Mean :70.5 Mean :13.4 Mean :5.18 3rd Qu.: 685 3rd Qu.:75.0 3rd Qu.:13.9 3rd Qu.:5.70 Max. :1200 Max. :94.0 Max. :14.7 Max. :7.40 shelves s_sq_ft features Min. :1.00 Min. :20.6 Min. : 1.00 1st Qu.:2.00 1st Qu.:23.4 1st Qu.: 2.00 Median :2.00 Median :24.0 Median : 3.00 Mean :2.51 Mean :24.5 Mean : 3.46 3rd Qu.:3.00 3rd Qu.:25.5 3rd Qu.: 5.00 Max. :5.00 Max. :30.2 Max. :12.00 price ecost rsize fsize shelves s_sq_ft features price 1.000 0.522 -0.024 0.720 0.400 0.155 0.697 ecost 0.522 1.000 -0.033 0.855 0.188 0.058 0.334 rsize -0.024 -0.033 1.000 -0.235 -0.363 0.401 -0.096 fsize 0.720 0.855 -0.235 1.000 0.251 0.110 0.439 shelves 0.400 0.188 -0.363 0.251 1.000 -0.527 0.160 s_sq_ft 0.155 0.058 0.401 0.110 -0.527 1.000 0.083 features 0.697 0.334 -0.096 0.439 0.160 0.083 1.000 Call: lm(formula = price ~ rsize + fsize + shelves + features, data = Refrig) Residuals: Min 1Q Median 3Q Max -128.20 -34.96 7.08 28.72 155.10 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -698.89 302.60 -2.31 0.0275 * rsize 56.50 20.56 2.75 0.0098 ** fsize 75.40 13.93 5.41 0.000006 *** shelves 35.92 11.08 3.24 0.0028 ** features 25.16 5.04 4.99 0.000020 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 68.1 on 32 degrees of freedom Multiple R-squared: 0.789, Adjusted R-squared: 0.763 F-statistic: 29.9 on 4 and 32 DF, p-value: 0.00000000021 Call: lm(formula = ecost ~ rsize + fsize + shelves + features, data = Refrig) Residuals: Min 1Q Median 3Q Max -7.848 -4.306 0.515 2.632 9.360 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -14.217 20.936 -0.68 0.502 rsize 2.874 1.422 2.02 0.052 . fsize 8.909 0.964 9.24 0.00000000015 *** shelves 0.289 0.766 0.38 0.708 features -0.201 0.349 -0.58 0.569 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 4.71 on 32 degrees of freedom Multiple R-squared: 0.764, Adjusted R-squared: 0.734 F-statistic: 25.9 on 4 and 32 DF, p-value: 0.00000000125 4.4 Unusual observations In this section, you learn how to: Compare and contrast three alternative definitions of a standardized residual Evaluate three alternative options for dealing with outliers Assess the impact of a high leverage observation Evaluate options for dealing with high leverage observations Describe the notion of influence and Cook’s Distance for quantifying influence 4.4.1 Video Video Overhead Details Show Overhead A Details. Unusual observationsUnusual observations Regression coefficients can be expressed as (matrix) weighted averages of outcomes Averages, even weighted averages can be strongly influenced by unusual observations Observations may be unusual in the y direction or in the X space For unusual in the y direction, we use a residual \\(e = y - \\hat{y}\\) By subtracting the fitted value \\(\\hat{y}\\), we look to the y distance from the regression plane In this way, we “control” for values of explanatory variables Show Overhead B Details. Standardized residuals We standardize residuals so that we can focus on relationships of interest and achieve carry-over of experience from one data set to another. Three commonly used definitions of standardize residuals are: \\[ \\text{(a) }\\frac{e_i}{s}, \\ \\ \\ \\text{ (b) }\\frac{e_i}{s\\sqrt{1-h_{ii}}}, \\ \\ \\ \\text{(c)}\\frac{e_i}{s_{(i)}\\sqrt{1-h_{ii}}}. \\] First choice is simple Second choice, from theory, \\(\\mathrm{Var}(e_i)=\\sigma ^{2}(1-h_{ii}).\\) Here, \\(h_{ii}\\) is the \\(i\\)th leverage (defined later). Third choice is termed “studentized residuals”. Idea: numerator is independent of the denominator. Show Overhead C Details. Outlier - an unusal standardized residual An outlier is an observation that is not well fit by the model; these are observations where the residual is unusually large. Unusual means what? Many packages mark a point if the |standardized residual| &gt; 2. Options for handling outliers Ignore them in the analysis but be sure to discuss their effects. Delete them from the data set (but be sure to discuss their effects). Create a binary variable to indicator their presence. (This will increase your \\(R^2\\)!) Show Overhead D Details. High leverage points A high leverage point is an observation that is “far away” in the \\(x\\)-space from others. One can get a feel for high leverage observations by looking a summary statistics (mins, maxs) for each explanatory variable. Options for dealing with high leverage points are comparable to outliers, we can ignore their effects, delete them, or mark them with a binary indicator variable. Show Overhead E Details. High leverage point graph library(cluster) #library(MASS) par(mar=c(3.2,5.4,.2,.2)) plot(1,5,type=&quot;p&quot;,pch=19,cex=1.5,xlab=&quot;&quot;,ylab=&quot;&quot;,cex.lab=1.5,xaxt=&quot;n&quot;,yaxt=&quot;n&quot;,xlim=c(-3,5),ylim=c(-12,12)) mtext(expression(x[2]), side=1,line=2, cex=2.0) mtext(expression(x[1]), side=2, line=2, las=2, cex=2.0) arrows(1.5,5,4,5,code=1,lwd=2,angle=15,length=0.25) xycov&lt;-matrix(c(2, -5,-5, 20),nrow=2,ncol=2) xyloc&lt;-matrix(c(0, 0),nrow=1,ncol=2) polygon(ellipsoidPoints(xycov, d2 = 2, loc=xyloc),col=&quot;black&quot;) Show Overhead F Details. Leverage Using matrix algebra, one can express the ith fitted value as a linear combination of observations \\[ \\hat{y}_{i} = h_{i1} y_{1} + \\cdots +h_{ii}y_{i}+\\cdots+h_{in}y_{n}. \\] The term \\(h_{ii}\\) is known as the ith leverage The larger the value of \\(h_{ii}\\), the greater the effect of the ith observation \\(y_i\\) on the ith fitted value \\(\\hat{y}_i\\). Statistical routines have values of the leverage coded, so computing this quantity. The key thing to know is that \\(h_{ii}\\) is based solely on the explanatory variables. If you change the \\(y\\) values, the leverage does not change. As a commonly used rule of thumb, a leverage is deemed to be “unusual” if its value exceeds three times the average (= number of regression coefficients divided by the number of observations.) 4.4.2 Exercise. Outlier example In chapter 2, we consider a fictitious data set of 19 “base” points plus three different types of unusual points. In this exercise, we consider the effect of one unusal point, “C”, this both an outlier (unusual in the “y” direction) and a high leverage point (usual in the x-space). The data have been pre-loaded in the dataframe outlrC. Instructions Fit a basic linear regression model of y on x and store the result in an object. Use the function rstandard() to extract the standardized residuals from the fitted regression model object and summarize them. Use the function hatvalues() to extract the leverages from the model fitted and summarize them. Plot the standardized residuals versus the leverages to see the relationship between these two measures that calibrate how unusual an observation is. eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6IiNvdXRsciA8LSByZWFkLmNzdihcIkNTVkRhdGFcXFxcT3V0bGllci5jc3ZcIiwgaGVhZGVyID0gVFJVRSlcbm91dGxyIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9hc3NldHMuZGF0YWNhbXAuY29tL3Byb2R1Y3Rpb24vcmVwb3NpdG9yaWVzLzI2MTAvZGF0YXNldHMvN2EzODkxMmU1NDRjMzFmYzZmNWZjYTEyYjlhMmViNjQ1ZjJiY2QzMi9PdXRsaWVyLmNzdlwiLCBoZWFkZXIgPSBUUlVFKVxub3V0bHJDIDwtIG91dGxyWy1jKDIwLDIxKSxjKFwieFwiLFwieVwiKV0iLCJzYW1wbGUiOiJvdXRsckMgPC0gb3V0bHJbLWMoMjAsMjEpLGMoXCJ4XCIsXCJ5XCIpXVxuXG4jIEZpdCBhIGJhc2ljIGxpbmVhciByZWdyZXNzaW9uIG1vZGVsIG9mIGB5YCBvbiBgeGAgYW5kIHN0b3JlIHRoZSByZXN1bHQgaW4gYW4gb2JqZWN0LlxubW9kZWxfb3V0bHJDIDwtIGxtKHkgfiB4LCBkYXRhID0gb3V0bHJDKVxuXG4jIEV4dHJhY3QgdGhlIHN0YW5kYXJkaXplZCByZXNpZHVhbHMgZnJvbSB0aGUgZml0dGVkIHJlZ3Jlc3Npb24gbW9kZWwgb2JqZWN0IGFuZCBzdW1tYXJpemUgdGhlbS5cbnJpIDwtIHJzdGFuZGFyZChtb2RlbF9vdXRsckMpXG5zdW1tYXJ5KHJpKVxuXG4jIEV4dHJhY3QgdGhlIGxldmVyYWdlcyBmcm9tIHRoZSBtb2RlbCBmaXR0ZWQgYW5kIHN1bW1hcml6ZSB0aGVtLiBcbmhpaSA8LSBoYXR2YWx1ZXMobW9kZWxfb3V0bHJDKVxuc3VtbWFyeShoaWkpXG5cbiMgUGxvdCB0aGUgc3RhbmRhcmRpemVkIHJlc2lkdWFscyB2ZXJzdXMgdGhlIGxldmVyYWdlc1xucGxvdChoaWkscmkpIiwic29sdXRpb24iOiJwbG90KG91dGxyQylcbm1vZGVsX291dGxyQyA8LSBsbSh5IH4geCwgZGF0YSA9IG91dGxyQylcbnJpIDwtIHJzdGFuZGFyZChtb2RlbF9vdXRsckMpXG5zdW1tYXJ5KHJpKVxuaGlpIDwtIGhhdHZhbHVlcyhtb2RlbF9vdXRsckMpXG5zdW1tYXJ5KGhpaSlcbnBsb3QoaGlpLHJpKSIsInNjdCI6InN1Y2Nlc3NfbXNnKFwiRXhjZWxsZW50ISBXaXRoIG9ubHkgdHdvIHZhcmlhYmxlcywgd2UgY291bGQgYXJndWUgZ3JhcGhpY2FsbHkgdGhhdCBvYnNlcnZhdGlvbnMgd2VyZSB1bnVzdWFsLiBJbiB0aGlzIGV4ZXJjaXNlLCB3ZSBzaG93ZWQgaG93IHN0YXRpc3RpY3MgY291bGQgYWxzbyBiZSB1c2VkIHRvIGlkZW50aWZ5IHVzdWFsIG9ic2VydmF0aW9ucy4gQWx0aG91Z2ggbm90IHJlYWxseSBuZWNlc3NhcnkgaW4gYmFzaWMgbGluZWFyIHJlZ3Jlc3Npb24sIHRoZSBtYWluIGFkdmFudGFnZSBvZiB0aGUgc3RhdGlzdGljcyBpcyB0aGF0IHRoZXkgd29yayByZWFkaWx5IGluIGEgbXVsdGl2YXJpYXRlIHNldHRpbmcuXCIpIn0= 4.4.3 Exercise. High leverage and risk manager survey Assignment Text In a prior exercise, we fit a regression model of logcost on logsize, indcost and a squared version of indcost. This model is summarized in the object mlr_survey2. In this exercise, we examine the robustness of the model to unusual observations. Instructions Use the R functions rstandard() and hatvalues() to extract the standardized residuals and leverages from the model fitted. Summarize the distributions graphically. You will see that there are two observations where the leverages are high, numbers 10 and 16. On looking at the dataset, these turn out to be observations in a high risk industry. Create a histogram of the variable indcost to corroborate this. Re-run the regression omitting observations 10 and 16. Summarize this regression and the regression in the object mlr_survey2, noting differences in the coefficients. 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 4.5 Collinearity In this section, you learn how to: Define collinearity and describe its potential impact on regression inference Define a variance inflation factor and describe its effect on a regression coefficients standard error Describe rules of thumb for assessing collinearity and options for model reformulation in the presence of severe collinearity Compare and contrast effects of leverage and collinearity 4.5.1 Video Video Overhead Details Show Overhead A Details. Collinearity Collinearity, or multicollinearity, occurs when one explanatory variable is, or nearly is, a linear combination of the other explanatory variables. Useful to think of the explanatory variables as being highly correlated with one another. Collinearity neither precludes us from getting good fits nor from making predictions of new observations. Estimates of error variances and, therefore, tests of model adequacy, are still reliable. In cases of serious collinearity, standard errors of individual regression coefficients can be large. With large standard errors, individual regression coefficients may not be meaningful. Because a large standard error means that the corresponding t-ratio is small, it is difficult to detect the importance of a variable. Show Overhead B Details. Quantifying collinearity A common way to quantify collinearity is through the variance inflation factor (VIF). Suppose that the set of explanatory variables is labeled \\(x_{1},x_{2},\\dots,x_{k}\\). Run the regression using \\(x_{j}\\) as the “outcome” and the other \\(x\\)’s as the explanatory variables. Denote the coefficient of determination from this regression by \\(R_j^2\\). Define the variance inflation factor \\[ VIF_{j}=\\frac{1}{1-R_{j}^{2}},\\ \\ \\ \\text{ for } j = 1,2,\\ldots, k. \\] Show Overhead C Details. Options for handling collinearity Rule of thumb: When \\(VIF_{j}\\) exceeds 10 (which is equivalent to \\(R_{j}^{2}&gt;90\\%\\)), we say that severe collinearity exists. This may signal is a need for action. Recode the variables by “centering” - that is, subtract the mean and divide by the standard deviation. Ignore the collinearity in the analysis but comment on it in the interpretation. Probably the most common approach. Replace one or more variables by auxiliary variables or transformed versions. Remove one or more variables. Easy. Which One? is hard. Use interpretation. Which variable(s) do you feel most comfortable with? Use automatic variable selection procedures to suggest a model. 4.5.2 Exercise. Collinearity and term life Assignment Text We have seen that adding an explanatory variable \\(x^2\\) to a model is sometimes helpful even though it is perfectly related to \\(x\\) (such as through the function \\(f(x)=x^2\\)). But, for some data sets, higher order polynomials and interactions can be approximately linearly related (depending on the range of the data). This exercise returns to our term life data set Term1 (preloaded) and demonstrates that collinearity can be severe when introducing interaction terms. Instructions Fit a MLR model of logface on explantory variables education, numhh and logincome Use the function vif() from the car package (preloaded) to calculate variance inflation factors. Fit and summarize a MLR model of logface on explantory variables education , numhh and logincome with an interaction between numhh and logincome, then extract variance inflation factors. Hint. If the car package is not available to you, then you could calculate vifs using the [lm()] function, treating each variable separately. For example 1/(1-summary(lm(education ~ numhh + logincome, data = Term1))$r.squared) gives the education vif. eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6IiNUZXJtIDwtIHJlYWQuY3N2KFwiQ1NWRGF0YVxcXFx0ZXJtX2xpZmUuY3N2XCIsIGhlYWRlciA9IFRSVUUpXG5UZXJtIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9hc3NldHMuZGF0YWNhbXAuY29tL3Byb2R1Y3Rpb24vcmVwb3NpdG9yaWVzLzI2MTAvZGF0YXNldHMvZWZjNjRiYzJkNzhjZjZiNDhhZDJjM2Y1ZTMxODAwY2I3NzNkZTI2MS90ZXJtX2xpZmUuY3N2XCIsIGhlYWRlciA9IFRSVUUpXG5UZXJtMSA8LSBzdWJzZXQoVGVybSwgc3Vic2V0ID0gZmFjZSA+IDApXG4jc3RyKFRlcm0xKSIsInNhbXBsZSI6IiMgRml0IGEgTUxSIG1vZGVsIG9mIGBsb2dmYWNlYCBvbiBleHBsYW50b3J5IHZhcmlhYmxlcyBgZWR1Y2F0aW9uYCwgYG51bWhoYCBhbmQgYGxvZ2luY29tZWBcblRlcm1fbWxyIDwtIGxtKGxvZ2ZhY2UgfiBlZHVjYXRpb24gKyBudW1oaCArIGxvZ2luY29tZSwgZGF0YSA9IFRlcm0xKVxuXG4jIENhbGN1bGF0ZSB0aGUgdmFyaWFuY2UgaW5mbGF0aW9uIGZhY3RvcnMuXG5jYXI6OnZpZihUZXJtX21scilcblxuIyBGaXQgYW5kIHN1bW1hcml6ZSBhIE1MUiBtb2RlbCBvZiBgbG9nZmFjZWAgb24gZXhwbGFudG9yeSB2YXJpYWJsZXMgYGVkdWNhdGlvbmAgLCBgbnVtaGhgIGFuZCBgbG9naW5jb21lYCB3aXRoIGFuIGludGVyYWN0aW9uIGJldHdlZW4gYG51bWhoYCBhbmQgYGxvZ2luY29tZWAsIHRoZW4gZXh0cmFjdCB2YXJpYW5jZSBpbmZsYXRpb24gIGZhY3RvcnMuXG5UZXJtX21scjEgPC0gbG0obG9nZmFjZSB+IGVkdWNhdGlvbiArIG51bWhoKmxvZ2luY29tZSAsIGRhdGEgPSBUZXJtMSlcbnN1bW1hcnkoVGVybV9tbHIxKVxuY2FyOjp2aWYoVGVybV9tbHIxKSIsInNvbHV0aW9uIjoiVGVybV9tbHIgPC0gbG0obG9nZmFjZSB+IGVkdWNhdGlvbiArIG51bWhoICsgbG9naW5jb21lLCBkYXRhID0gVGVybTEpXG5jYXI6OnZpZihUZXJtX21scilcblRlcm1fbWxyMSA8LSBsbShsb2dmYWNlIH4gZWR1Y2F0aW9uICsgbnVtaGgqbG9naW5jb21lICwgZGF0YSA9IFRlcm0xKVxuc3VtbWFyeShUZXJtX21scjEpXG5jYXI6OnZpZihUZXJtX21scjEpIiwic2N0Ijoic3VjY2Vzc19tc2coXCJFeGNlbGxlbnQhIFRoaXMgZXhlcmNpc2UgdW5kZXJzY29yZXMgdGhhdCBjb2xpbmVhcml0eSBhbW9uZyBleHBsYW5hdG9yeSB2YXJpYWJsZXMgY2FuIGJlIGluZHVjZWQgd2hlbiBpbnRyb2R1Y2luZyBoaWdoZXIgb3JkZXIgdGVybXMgc3VjaCBhcyBpbnRlcmFjdGlvbnMuIE5vdGUgdGhhdCBpbiB0aGUgaW50ZXJhY3Rpb24gbW9kZWwgdGhlIHZhcmlhYmxlICdudW1oaCcgZG9lcyBub3QgYXBwZWFyIHRvIGJlIHN0YXRpc3RpY2FsbHkgc2lnbmZpY2FudCBlZmZlY3QuIFRoaXMgaXMgb25lIG9mIHRoZSBiaWcgZGFuZ2VycyBvZiBjb2xsaW5lYXJpdHkgLSBpdCBjYW4gbWFzayBpbXBvcnRhbnQgZWZmZWN0cy5cIikifQ== 4.6 Selection criteria In this section, you learn how to: Summarize a regression fit using alternative goodness of fit measures Validate a model using in-sample and out-of-sample data to mitigate issues of data-snooping Compare and contrast SSPE and PRESS statistics for model validation 4.6.1 Video Video Overhead Details Show Overhead A Details. Goodness of fit Criteria that measure the proximity of the fitted model and realized data are known as goodness of fit statistics. Basic examples include: the coefficient of determination \\((R^{2})\\), an adjusted version \\((R_{a}^{2})\\), the size of the typical error \\((s)\\), and \\(t\\)-ratios for each regression coefficient. Show Overhead B Details. Goodness of fit and information criteria A general measure is Akaike’s Information Criterion, defined as \\[ AIC = -2 \\times (fitted~log~likelihood) + 2 \\times (number~of~parameters) \\] For model comparison, the smaller the \\(AIC,\\) the better is the fit. This measures balances the fit (in the first part) with a penalty for complexity (in the second part) It is a general measure - for linear regression, it reduces to \\[ AIC = n \\ln (s^2) + n \\ln (2 \\pi) +n +k + 3 . \\] So, selecting a model to minimize \\(s\\) or \\(s^2\\) is equivalent to model selection based on minimizing \\(AIC\\) (same k). Show Overhead C Details. Out of sample validation When you choose a model to minimize \\(s\\) or \\(AIC\\), it is based on how well the model fits the data at hand, or the model development, or training, data As we have seen, this approach is susceptible to overfitting. A better approach is to validate the model on a model validation, or test data set, held out for this purpose. Show Overhead D Details. Out of sample validation procedure Using the model development subsample, fit a candidate model. Using the Step (ii) model and the explanatory variables from the validation subsample, “predict” the dependent variables in the validation subsample, \\(\\hat{y}_i\\), where \\(i=n_{1}+1,...,n_{1}+n_{2}\\). Calc the *sum of absolute prediction errors** \\[SAPE=\\sum_{i=n_{1}+1}^{n_{1}+n_{2}} |y_{i}-\\hat{y}_{i}| . \\] Repeat Steps (i) through (iii) for each candidate model. Choose the model with the smallest SAPE. Show Overhead E Details. Cross - validation With out-of-sample validation, the statistic depends on a random split between in-sample and out-of-sample data (a problem for data sets that are not large) Alternatively, one may use cross-validation Use a random mechanism to split the data into k subsets, (e.g., 5-10) Use the first k-1 subsamples to estimate model parameters. Then, “predict” the outcomes for the kth subsample and use SAE to summarize the fit Repeat this by holding out each of the k sub-samples, summarizing with a cumulative SAE. Repeat these steps for several candidate models. Choose the model with the lowest cumulative SAE statistic. 4.6.2 Exercise. Cross-validation and term life Assignment Text Here is some sample code to give you a better feel for cross-validation. The first part of the randomly re-orders (“shuffles”) the data. It also identifies explanatory variables explvars. The function starts by pulling out only the needed data into cvdata. Then, for each subsample, a model is fit based on all the data except for the subsample, in train_mlr with the subsample in test. This is repeated for each subsample, then results are summarized. Show Code # Randomly re-order data - &quot;shuffle it&quot; n &lt;- nrow(Term1) set.seed(12347) shuffled_Term1 &lt;- Term1[sample(n), ] explvars &lt;- c(&quot;education&quot;, &quot;numhh&quot;, &quot;logincome&quot;) ## Cross - Validation crossvalfct &lt;- function(explvars){ cvdata &lt;- shuffled_Term1[, c(&quot;logface&quot;, explvars)] crossval &lt;- 0 k &lt;- 5 for (i in 1:k) { indices &lt;- (((i-1) * round((1/k)*nrow(cvdata))) + 1):((i*round((1/k) * nrow(cvdata)))) # Exclude them from the train set train_mlr &lt;- lm(logface ~ ., data = cvdata[-indices,]) # Include them in the test set test &lt;- data.frame(cvdata[indices, explvars]) names(test) &lt;- explvars predict_test &lt;- exp(predict(train_mlr, test)) # Compare predicted to held-out and summarize predict_err &lt;- exp(cvdata[indices, &quot;logface&quot;]) - predict_test crossval &lt;- crossval + sum(abs(predict_err)) } crossval/1000 } crossvalfct(explvars) Instructions Calculate the cross-validation statistic using only logarithmic income, logincome. Calculate the cross-validation statistic using logincome, education and numhh. Calculate the cross-validation statistic using logincome, education, numhh and marstat. The best model has the lowest cross-validation statistic. Hint. The function [sample()] is for taking random samples. We use it without replacement so it results in a re-ordering of data. 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 "],
["interpreting-regression-results.html", "Chapter 5 Interpreting Regression Results 5.1 Case study: MEPS health expenditures 5.2 What the modeling procedure tells us 5.3 The importance of variable selection", " Chapter 5 Interpreting Regression Results Chapter description A case study, on determining an individual’s characteristics that influence its health expenditures, illustrates the regression modeling process from start to finish. Subsequently, the chapter summarizes what we learn from the modeling process, underscoring the importance of variable selection. 5.1 Case study: MEPS health expenditures 5.1.1 Video Video Overhead Details Show Overhead A Details. MEPS health expenditures This exercise considers data from the Medical Expenditure Panel Survey (MEPS), conducted by the U.S. Agency of Health Research and Quality. MEPS is a probability survey that provides nationally representative estimates of health care use, expenditures, sources of payment, and insurance coverage for the U.S. civilian population. This survey collects detailed information on individuals of each medical care episode by type of services including physician office visits, hospital emergency room visits, hospital outpatient visits, hospital inpatient stays, all other medical provider visits, and use of prescribed medicines. This detailed information allows one to develop models of health care utilization to predict future expenditures. You can learn more about MEPS at http://www.meps.ahrq.gov/mepsweb/. We consider MEPS data from the panels 7 and 8 of 2003 that consists of 18,735 individuals between ages 18 and 65. From this sample, we took a random sample of 2,000 individuals that appear in the file HealthExpend. From this sample, there are 1,352 that had positive outpatient expenditures. Our dependent variable is the amount of expenditures for outpatient visits, expendop. For MEPS, outpatient events include hospital outpatient department visits, office-based provider visits and emergency room visits excluding dental services. (Dental services, compared to other types of health care services, are more predictable and occur in a more regular basis.) Hospital stays with the same date of admission and discharge, known as “zero-night stays,” were included in outpatient counts and expenditures. (Payments associated with emergency room visits that immediately preceded an inpatient stay were included in the inpatient expenditures. Prescribed medicines that can be linked to hospital admissions were included in inpatient expenditures, not in outpatient utilization.) Show Overhead B Details. Overhead MEPS health expenditures Data from the Medical Expenditure Panel Survey (MEPS), conducted by the U.S. Agency of Health Research and Quality (AHRQ). A probability survey that provides nationally representative estimates of health care use, expenditures, sources of payment, and insurance coverage for the U.S. civilian population. Collects detailed information on individuals of each medical care episode by type of services including physician office visits, hospital emergency room visits, hospital outpatient visits, hospital inpatient stays, all other medical provider visits, and use of prescribed medicines. This detailed information allows one to develop models of health care utilization to predict future expenditures. We consider MEPS data from the first panel of 2003 and take a random sample of n = 2, 000 individuals between ages 18 and 65. Show Overhead C Details. Outcome variable Our dependent variable is expenditures for outpatient admissions. For MEPS, inpatient admissions include persons who were admitted to a hospital and stayed overnight. In contrast, outpatient events include hospital outpatient department visits, office-based provider visits and emergency room visits excluding dental services. Hospital stays with the same date of admission and discharge, known as “zero-night stays,” were included in outpatient counts and expenditures. Payments associated with emergency room visits that immediately preceded an inpatient stay were included in the inpatient expenditures. Prescribed medicines that can be linked to hospital admissions were included in inpatient expenditures, not in outpatient utilization. Show Overhead D Details. Explanatory variables 9 variables in the database. Here 13 most relevant. \\[ {\\small \\begin{array}{ll} expendop &amp; \\text{Amounts of expenditures for outpatient visits} \\\\ gender &amp; \\text{Indicate gender of patient (=1 if female, =0 if male)} \\\\ age &amp; \\text{Age in years between 18 and 65 }\\\\ race &amp; \\text{Race of patient described as Asian, Black, Native, White and other} \\\\ region &amp; \\text{Region of patient described as WEST, NORTHEAST, MIDWEST and SOUTH} \\\\ educ &amp; \\text{Level of education received described by words (LHIGHSC, HIGHSCH and COLLEGE)} \\\\ phstat &amp; \\text{Self-rated physical health status described as EXCE, VGOO, GOOD, FAIR and POOR} \\\\ mpoor &amp; \\text{Self-rated mental health (=1 if poor or fair, =0 if good to excellent mental health)} \\\\ anylimit &amp; \\text{Any activity limitation (=1 if any functional/activity limitation, =0 if otherwise)} \\\\ income &amp; \\text{Income compared to poverty line described as POOR, NPOOR, LINCOME, MINCOME and HINCOME} \\\\ insure &amp; \\text{Insurance coverage (=1 if covered by public/private health insurance in any month of 1996, =0 otherwise)} \\\\ usc &amp; \\text{1 if dissatisfied with one&#39;s usual source of care} \\\\ unemploy &amp; \\text{Employment status of patients} \\\\ managedcare &amp; \\text{1 if enrolled in an HMO or gatekeeper plan} \\\\ \\end{array}} \\] Show Overhead E Details. Case study outline The next series of exercises leads you through an analysis of the steps for understanding a complex data set. Because of the complexity of the data, in each step only a sample of procedures will be executed. The outline consists of: Summary statistics Splitting the data into training and testing portions with initial model fits Selecting variables to be included in the model 5.1.2 Exercise. Summarizing data Assignment Text With a complex dataset, you will probably want to take a look at the structure of the data. You are already familiar with taking a [summary()] of a dataframe which provides summary statistics for many variables. You will see that several variables in this dataframe are categorical, or factor, variables. We can use the table() function to summarize them. After getting a sense of the distributions of explanatory variables, we want to take a deeper dive into the distribution of the outcome variable, expendop. We will do this by comparing the histograms of the variable to that of its logarithmic version. To examine relationships of the outcome variable visually, we look to scatterplots for continuous variables (such as age) and boxplots for categorical variables (such as phstat). Instructions Examine the structure of the meps dataframe using the str() function. Also, get a [summary()] of the dataframe. Examine the distribution of the race variable using the table() function. Compare the expenditures distribution to its logarithmic version visually via histograms plotted next to another. par(mfrow = c(1, 2)) is used to organize the plots you create. Examine the distribution of logarithmic expenditures in terms of levels of phstat visually using the boxplot() function. Examine the relationship of age versus logarithmic expenditures using a scatter plot. Superimpose a local fitting line using the lines() and lowess() functions. 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 5.1.3 Exercise. Fit a benchmark multiple linear regression model Assignment Text As part of the pre-processing for the model fitting, we will split the data into training and test subsamples. For this exercise, we use a 75/25 split although other choices are certainly suitable. Some analysts prefer to do this splitting before looking at the data. Another approach, adopted here, is that the final report typically contains summary statistcs of the entire data set and so it makes sense to do so when examining summary statistics. We start by fitting a benchmark model. It is common to use all available explanatory variables with the outcome on the original scale and so we use this as our benchmark model. This exercise shows that when you plot() a fitted linear regression model in R, the result provides four graphs that you have seen before. These can be useful for identifying an appropriate model. Instructions Randomly split the data into a training and a testing data sets. Use 75% for the training, 25% for the testing. Fit a full model using expendop as the outcome and all explanatory variables. Summarize the results of this model fitting. You can plot() the fitted model to view several diagnostic plots. These plots provide evidence that expenditures may not be the best scale for linear regression. Fit a full model using logexpend as the outcome and all explanatory variables and summarize the fit. Use the plot() function for evidence that this variable is more suited for linear regression methods than expenditures on the original scale. Hint. A plot of a regression object such as plot(mlr) provides four diagnostic plots. These can be organized as a 2 by 2 array using par(mfrow = c(2, 2)). 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 5.1.4 Exercise. Variable selection Assignment Text Modeling building can be approached using a “ground-up” strategy, where the analyst introduces a variable, examines residuls from a regression fit, and then seeks to understand the relationship between these residuals and other available variables so that these variables might be added to the model. Another approach is a “top-down” strategy where all available variables are entered into a model and unnecessary variables are pruned from the model. Both approaches are helpful when using data to specify models. This exercise illustrates the latter approach, using the [step()] function to help narrow our search for the best fitting model. Instructions From our prior work, the training dataframe train_meps has already been loaded in. A multiple linear regression model fit object meps_mlr2 is available that summarizes a fit of logexpend as the outcome variable using all 13 explanatory variables. Use the step() function function to drop unnecessary variables from the full fitted model summarized in the object meps_mlr2 and summarize this recommended model. As an alternative, use the explanatory variables in the recommended model and add the varibles phstat. Summarize the fit and note that statistical significance of the new variable. You have been reminded by your boss that use of the variable gender is unsuitable for actuarial pricing purposes. As an another alternative, drop gender from the recommended model (still keeping phstat). Note the statistical significance of the variable uscwith this fitted model. Hint eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6IiNtZXBzIDwtIHJlYWQuY3N2KFwiQ1NWRGF0YVxcXFxIZWFsdGhNZXBzLmNzdlwiLCBoZWFkZXIgPSBUUlVFKVxubWVwcyA8LSByZWFkLmNzdihcImh0dHBzOi8vYXNzZXRzLmRhdGFjYW1wLmNvbS9wcm9kdWN0aW9uL3JlcG9zaXRvcmllcy8yNjEwL2RhdGFzZXRzLzdiN2RhYjZkMGM1MjhlNGNkMmY4ZDBlMGZjNzgyNGEyNTQ0MjliZjgvSGVhbHRoTWVwcy5jc3ZcIiwgaGVhZGVyID0gVFJVRSlcbm1lcHMkbG9nZXhwZW5kIDwtIGxvZyhtZXBzJGV4cGVuZG9wKVxuIyBTcGxpdCB0aGUgc2FtcGxlIGludG8gYSBgdHJhaW5pbmdgIGFuZCBgdGVzdGAgZGF0YVxubiA8LSBucm93KG1lcHMpXG5zZXQuc2VlZCgxMjM0NylcbnNodWZmbGVkX21lcHMgPC0gbWVwc1tzYW1wbGUobiksIF1cbnRyYWluX2luZGljZXMgPC0gMTpyb3VuZCgwLjc1ICogbilcbnRyYWluX21lcHMgICAgPC0gc2h1ZmZsZWRfbWVwc1t0cmFpbl9pbmRpY2VzLCBdXG50ZXN0X2luZGljZXMgIDwtIChyb3VuZCgwLjI1ICogbikgKyAxKTpuXG50ZXN0X21lcHMgICAgIDwtIHNodWZmbGVkX21lcHNbdGVzdF9pbmRpY2VzLCBdIiwic2FtcGxlIjoibWVwc19tbHIyIDwtIGxtKGxvZ2V4cGVuZCB+IGdlbmRlciArIGFnZSArIHJhY2UgKyByZWdpb24gKyBlZHVjICsgcGhzdGF0ICsgbXBvb3IgKyBhbnlsaW1pdCArIGluY29tZSArIGluc3VyZSArIHVzYyArIHVuZW1wbG95ICsgbWFuYWdlZGNhcmUsIGRhdGEgPSB0cmFpbl9tZXBzKVxuIyBVc2UgdGhlIHN0ZXAoKSB0byBkcm9wIHVubmVjZXNzYXJ5IHZhcmlhYmxlcyBmcm9tIHRoZSBmdWxsIGZpdHRlZCBtb2RlbCBzdW1tYXJpemVkIGluIHRoZSBvYmplY3QgYG1lcHNfbWxyMmAgYW5kIHN1bW1hcml6ZSB0aGlzIHJlY29tbWVuZGVkIG1vZGVsLlxubW9kZWxfc3RlcHdpc2UgPC0gc3RlcChtZXBzX21scjIsIGRhdGEgPSBfX18sIGRpcmVjdGlvbj0gXCJib3RoXCIsIGsgPSBsb2cobnJvdyhYKSksIHRyYWNlID0gMCkgXG5zdW1tYXJ5KG1vZGVsX3N0ZXB3aXNlKVxuXG4jIEFzIGFuIGFsdGVybmF0aXZlLCB1c2UgdGhlIGV4cGxhbmF0b3J5IHZhcmlhYmxlcyBpbiB0aGUgcmVjb21tZW5kZWQgbW9kZWwgYW5kIGFkZCB0aGUgdmFyaWJsZXMgYG1wb29yYC4gU3VtbWFyaXplIHRoZSBmaXQgIGFuZCBub3RlIHRoYXQgc3RhdGlzdGljYWwgc2lnbmlmaWNhbmNlIG9mIHRoZSBuZXcgdmFyaWFibGUuXG5tZXBzX21scjQgPC0gbG0oX19fIH4gZ2VuZGVyICsgYWdlICsgcGhzdGF0ICsgYW55bGltaXQgKyBpbnN1cmUgICsgX19fLCBkYXRhID0gdHJhaW5fbWVwcylcbnN1bW1hcnkobWVwc19tbHI0KVxuXG4jIFlvdSBoYXZlIGJlZW4gcmVtaW5kZWQgYnkgeW91ciBib3NzIHRoYXQgdXNlIG9mIHRoZSB2YXJpYWJsZSBgZ2VuZGVyYCBpcyB1bnN1aXRhYmxlIGZvciBhY3R1YXJpYWwgcHJpY2luZyBwdXJwb3Nlcy4gQXMgYW4gYW5vdGhlciBhbHRlcm5hdGl2ZSwgZHJvcCBgZ2VuZGVyYCBmcm9tIHRoZSByZWNvbW1lbmRlZCBtb2RlbCAoc3RpbGwga2VlcGluZyBgbXBvb3JgKS4gTm90ZSB0aGUgc3RhdGlzdGljYWwgc2lnbmlmaWNhbmNlIG9mIHRoZSB2YXJpYWJsZSBgdXNjYHdpdGggdGhpcyBmaXR0ZWQgbW9kZWwuXG5tZXBzX21scjUgPC0gbG0obG9nZXhwZW5kIH4gYWdlICsgcGhzdGF0ICsgYW55bGltaXQgKyBpbnN1cmUgICsgX19fLCBkYXRhID0gdHJhaW5fbWVwcylcbnN1bW1hcnkoX19fKSIsInNvbHV0aW9uIjoibWVwc19tbHIyIDwtIGxtKGxvZ2V4cGVuZCB+IGdlbmRlciArIGFnZSArIHJhY2UgKyByZWdpb24gKyBlZHVjICsgcGhzdGF0ICsgbXBvb3IgKyBhbnlsaW1pdCArIGluY29tZSArIGluc3VyZSArIHVzYyArIHVuZW1wbG95ICsgbWFuYWdlZGNhcmUsIGRhdGEgPSB0cmFpbl9tZXBzKVxuI2xpYnJhcnkoUmNtZHIpXG4jdGVtcCA8LSBzdGVwd2lzZShtZXBzX21scjIsIGRpcmVjdGlvbiA9ICdiYWNrd2FyZC9mb3J3YXJkJylcbm1vZGVsX3N0ZXB3aXNlIDwtIHN0ZXAobWVwc19tbHIyLCBkYXRhID0gWCwgZGlyZWN0aW9uPSBcImJvdGhcIiwgayA9IGxvZyhucm93KFgpKSwgdHJhY2UgPSAwKSBcbnN1bW1hcnkobW9kZWxfc3RlcHdpc2UpXG5tZXBzX21scjMgPC0gbG0obG9nZXhwZW5kIH4gZ2VuZGVyICsgYWdlICsgcGhzdGF0ICsgYW55bGltaXQgKyBpbnN1cmUgLCBkYXRhID0gdHJhaW5fbWVwcylcbnN1bW1hcnkobWVwc19tbHIzKVxubWVwc19tbHI0IDwtIGxtKGxvZ2V4cGVuZCB+IGdlbmRlciArIGFnZSArIHBoc3RhdCArIGFueWxpbWl0ICsgaW5zdXJlICArIG1wb29yLCBkYXRhID0gdHJhaW5fbWVwcylcbnN1bW1hcnkobWVwc19tbHI0KVxubWVwc19tbHI1IDwtIGxtKGxvZ2V4cGVuZCB+IGFnZSArIHBoc3RhdCArIGFueWxpbWl0ICsgaW5zdXJlICArIG1wb29yLCBkYXRhID0gdHJhaW5fbWVwcylcbnN1bW1hcnkobWVwc19tbHI1KVxuXG4jIHBhcihtZnJvdyA9IGMoMiwgMikpXG4jIHBsb3QobWVwc19tbHIzKVxuIyBcbiMgbWVwc19tbHI0IDwtIGxtKGxvZ2V4cGVuZCB+IGdlbmRlciArIGFnZSArIG1wb29yICsgYW55bGltaXQgKyBpbnN1cmUgKyB1c2MgICsgcGhzdGF0LCBkYXRhID0gdHJhaW5fbWVwcylcbiMgc3VtbWFyeShtZXBzX21scjQpXG4jIFxuIyBcbiMgbWVwc19tbHI1IDwtIGxtKGxvZ2V4cGVuZCB+IGFnZSAgKyBhbnlsaW1pdCArIG1wb29yICsgaW5zdXJlICArIHVzYyAgKyBwaHN0YXQsIGRhdGEgPSB0cmFpbl9tZXBzKVxuIyBzdW1tYXJ5KG1lcHNfbWxyNSlcbiMgYW5vdmEobWVwc19tbHI0LCBtZXBzX21scjUpXG4jIFxuIyAjYm94cGxvdCh0cmFpbl9tZXBzJGxvZ2V4cGVuZCB+IHRyYWluX21lcHMkcGhzdGF0KnRyYWluX21lcHMkdXNjKSIsInNjdCI6InN1Y2Nlc3NfbXNnKFwiRXhjZWxsZW50ISBTb21ldGltZXMgdmFyaWFibGVzIG1heSBoYXZlIGdvb2QgcHJlZGljdGl2ZSBwb3dlciBidXQgYXJlIHVuYWNjZXB0YWJsZSBmb3IgcG9saWN5IHB1cnBvc2VzIC0gaW4gaW5zdXJhbmNlLCBldGhuaWNpdHkgYW5kIHNvbWV0aW1lcyBzZXggYXJlIGdvb2QgZXhhbXBsZXMuIFRoaXMgaW1wbGllcyB0aGF0IG1vZGVsIGludGVycHJldGF0aW9uIGNhbiBiZSBqdXN0IGFzIGltcG9ydGFudCBhcyB0aGUgYWJpbGl0eSB0byBwcmVkaWN0LlwiKSJ9 5.1.5 Exercise. Model comparisons using cross-validation Assignment Text To compare alternative models, you decide to utilize cross-validation. For this exercise, you split the training sample into six subsamples of approximately equal size. In the sample code, the cross-validation procedure has been summarized into a function that you can call. The input to the function is a list of variables that you select as your model explanatory variables. With this function, you can readily test several candidate models. Instructions Run the cross validation (crossvalfct) function using the explanatory variables suggested by the stepwise function. Run the function again but adding the mpoor variable Run the function again but omitting the gender variable Note which model is suggested by the cross validation function. Hint. The cross validation function of this is very similar to the one we did earlier. Different number of subsamples, different test/training data and a different outcome variable. Except for these minor changes, it is the same function that we worked with earlier. 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 5.1.6 Exercise. Out of sample validation Assignment Text From our prior work, the training train_meps and test test_meps dataframes have already been loaded in. We think our best model is based on logarithmic expenditures as the outcome and the following explanatory variables: explvars3 &lt;- c(&quot;gender&quot;, &quot;age&quot;, &quot;phstat&quot;, &quot;anylimit&quot;, &quot;insure&quot;, &quot;mpoor&quot;) We will compare this to a benchmark model that is based on expenditures as the outcome and all 13 explanatory variables explvars4 &lt;- c(explvars3, &quot;race&quot;, &quot;income&quot;, &quot;region&quot;, &quot;educ&quot;, &quot;unemploy&quot;, &quot;managedcare&quot;, &quot;usc&quot;) The comparisons will be based on expenditures in dollars using the held-out validation sample. Instructions Use the training sample to fit a linear model with logexpend and explanatory variables listed in explvars3 Predict expenditures (not logged) for the test data and summarize the fit using the sum of absolute prediction errors. Use the training sample to fit a benchmark linear model with expendop and explanatory variables listed in explvars4 Predict expenditures for the test data and summarize the fit for the benchmark model using the sum of absolute prediction errors. Compare the predictions of the models graphically. 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 5.2 What the modeling procedure tells us In this section, you learn how to: Interpret individual effects, based on their substantive and statistical significance Describe other purposes of regression modeling, including regression function for pricing, benchmarking studies, and predicting future observations. 5.2.1 Video Video Overhead Details Show Overhead A Details. Interpreting individual effects Substantive Effect Does a 1 unit change in \\(x\\) imply an economically meaningful change in \\(y\\)? Example: Looking at urban and rural claims experience, is there a big enough difference to warrant differentiating prices by location? Statistical Significance We have standards for deciding whether or not a variable is statistically significant. A “statistically significant effect” is the result of a regression coefficient that is large relative to its standard error. Statistical significance is driven by precision of \\(s\\), collinearity (\\(VIF\\)) and sample size Causal Effects If we change \\(x\\), would \\(y\\) change? Show Overhead B Details. Other Interpretations Regression function and pricing The regression function is \\(\\mathrm{E~}y = \\beta_0 + \\beta_1 x_1 + \\cdots +\\beta _k x_k\\). Think about expected claims as our baseline price for short-term insurance coverages. Benchmarking studies In studies of CEO’s salaries, who is making a lot (or a little), controlled for industry, years of experience and so forth? In studies of medical claims, who are the high-cost patients? Prediction A new patient comes in with a given set of characteristics, what can I say about his or her future medical claims? MC Exercise. Which of the following are not important when interpreting the effects of individual variables? Substantive significance Statistical significance The amount of effort that it took to gather the data and do the analysis Role of causality Submit Answer MC Exercise. Which of the following is not a potential explanation for the lack of statistical significance of an explanatory variable? Large variation of the disturbance term High collinearity, so that the variable may be confounded with other variables The coefficient of determination, \\(R^2\\), is not sufficiently large Submit Answer MC Exercise. Which of the following is not an important purpose of regression modeling? Pricing of risks such as insurance contracts Benchmarking studies, to compare an observation to others Prediction Keeping a computer occupied with work Submit Answer 5.3 The importance of variable selection In this section, you learn how to: Describe the bias that can occur when omitting important variables Describe the principle of parsimony and reasons for adopting this approach 5.3.1 Video Video Overhead Details Show Overhead A Details. The importance of variable selection With too many or too few variables, \\(s\\) is too large an estimate of \\(\\sigma\\). Prediction intervals are too large Standard errors for the partial slopes are too large With too few or incorrect variables, we produce biased estimates of the slopes \\(\\beta\\). Thus, our predictions are biased and hence inaccurate. Show Overhead B Details. Example. Regression using one explanatory variable Too Many Variables The “true” model is \\(y_i = \\beta_0+ \\varepsilon_i\\) We mistakenly use \\(y_i = \\beta_0+ \\beta_1 x_i^* + \\varepsilon_i\\) The prediction at a generic level \\(x\\) is \\(b_0^* + b_1^* x\\). It is not to hard to confirm that \\(Bias = \\mathrm{E} (b_0^* + b_1^* x) - \\mathrm{E } y= 0\\). Too Few Variables The “true” model is \\(y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\\). We mistakenly use \\(y_i = \\beta_0^* \\varepsilon_i\\). Under the true model, \\(\\overline{y} = \\beta_0 + \\beta_1 \\overline{x} + \\overline{\\varepsilon}\\) Thus, the bias is \\[ Bias = \\text{E }\\bar{y} - \\text{E }(\\beta_0 + \\beta_1 x + \\varepsilon) \\\\ = \\text{E }(\\beta_0 + \\beta_1 \\bar{x}+\\bar{\\varepsilon})-(\\beta_0+\\beta_1 x)=\\beta_1 (\\bar{x}-x). \\] There is a persistent, long-term error in omitting the explanatory variable \\(x\\). Show Overhead C Details. Principle of parsimony The principle of parsimony, also known as Occam’s Razor, states that when there are several possible explanations for a phenomenon, use the simplest. A simpler explanation is easier to interpret. Simpler models, also known as ``more parsimonious’’ models, often do well on fitting out-of-sample data Extraneous variables can cause problems of collinearity, leading to difficulty in interpreting individual coefficients. In contrast, in a quote often attributed to Albert Einstein, we should use “the simplest model possible, but no simpler.” Omitting important variables can lead to biased results, a potentially serious error. Including extraneous variables decreases the degrees of freedom and increases the estimate of variability, typically of less concern in actuarial applications. MC Exercise. Which of the following is true about under- and over-fitting a model? When we over-fit a model, estimates of regression coefficients are over-biased as is \\(s^2\\), the estimate of model variance \\(\\sigma^2\\). When we over-fit a model, estimates of regression coefficients remain unbiased whereas \\(s^2\\), the estimate of model variance \\(\\sigma^2\\), is over-biased. When we over-fit a model, estimates of regression coefficients remain under-biased as is \\(s^2\\), the estimate of model variance \\(\\sigma^2\\). When we under-fit a model, estimates of regression coefficients remain unbiased whereas \\(s^2\\), the estimate of model variance \\(\\sigma^2\\), is over-biased. Submit Answer MC Exercise. Which of the following is not true of Occam’s Razor? When there are several possible explanations for a phenomenon, use the simplest one. Simpler models are easier to interpret. Variables can be statistically significant but practically unimportant. Simpler models often do better for predicting out-of-sample data Submit Answer "]
]
