<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Online Tutorial on Regression Modeling with Actuarial and Financial Applications</title>
  <meta name="description" content="Statistical techniques can be used to address new situations. This is important in a rapidly evolving risk management world. Analysts with a strong analytical background understand that a large data set can represent a treasure trove of information to be mined and can yield a strong competitive advantage. This course provides budding analysts with a foundation in multiple reression. Participants will learn about these statistical techniques using data on the demand for insurance, lottery sales, healthcare expenditures, and other applications. Although no specific knowledge of actuarial or risk management is presumed, the approach introduces applications in which statistical techniques can be used to analyze real data of interest.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Online Tutorial on Regression Modeling with Actuarial and Financial Applications" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Statistical techniques can be used to address new situations. This is important in a rapidly evolving risk management world. Analysts with a strong analytical background understand that a large data set can represent a treasure trove of information to be mined and can yield a strong competitive advantage. This course provides budding analysts with a foundation in multiple reression. Participants will learn about these statistical techniques using data on the demand for insurance, lottery sales, healthcare expenditures, and other applications. Although no specific knowledge of actuarial or risk management is presumed, the approach introduces applications in which statistical techniques can be used to analyze real data of interest." />
  <meta name="github-repo" content="<a href="https://github.com/ewfreesRes/RegressModel" class="uri">https://github.com/ewfreesRes/RegressModel</a>" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Online Tutorial on Regression Modeling with Actuarial and Financial Applications" />
  
  <meta name="twitter:description" content="Statistical techniques can be used to address new situations. This is important in a rapidly evolving risk management world. Analysts with a strong analytical background understand that a large data set can represent a treasure trove of information to be mined and can yield a strong competitive advantage. This course provides budding analysts with a foundation in multiple reression. Participants will learn about these statistical techniques using data on the demand for insurance, lottery sales, healthcare expenditures, and other applications. Although no specific knowledge of actuarial or risk management is presumed, the approach introduces applications in which statistical techniques can be used to analyze real data of interest." />
  

<meta name="author" content="Edward W. (Jed) Frees, University of Wisconsin-Madison">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="multiple-linear-regression.html">
<link rel="next" href="interpreting-regression-results.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script language="javascript">
function openTab(evt, tabName) {
    var i, tabcontent, tablinks;
    tabcontent = document.getElementsByClassName("tabcontent");
    for (i = 0; i < tabcontent.length; i++) {
        tabcontent[i].style.display = "none";
    }
    tablinks = document.getElementsByClassName("tablinks");
    for (i = 0; i < tablinks.length; i++) {
        tablinks[i].className = tablinks[i].className.replace(" active", "");
    }
    document.getElementById(tabName).style.display = "block";
    evt.currentTarget.className += " active";
}

// Get the element with id="defaultOpen" and click on it
document.getElementById("defaultOpen").click();
</script>

<script language="javascript">
function toggle(id1,id2) {
	var ele = document.getElementById(id1); var text = document.getElementById(id2);
	if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Solution";}
		else {ele.style.display = "block"; text.innerHTML = "Hide Solution";}}
</script>

<script language="javascript">
function togglecode(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show R Code";}
      else {ele.style.display = "block"; text.innerHTML = "Hide R Code";}}
</script>
<script language="javascript">
function toggleEX(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Example";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Example";}}
</script>
<script language="javascript">
function toggleTheory(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Theory";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Theory";}}
</script>

<script language="javascript">
function toggleDet(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Details";}
      else {ele.style.display = "block"; text.innerHTML = "Hide  Details";}}
</script>

<script language="javascript">
$(document).ready(function(){
    $('[data-toggle="tooltip"]').tooltip();
});
</script>


<script src=https://cdn.datacamp.com/datacamp-light-latest.min.js></script>

<script>
$(document).ready(function(){
    $('[data-toggle="popover"]').popover(); 
});
</script>

<style>
/* Rearrange console label */
.datacamp-exercise ol li, .datacamp-exercise ul li {
  margin-bottom: 0em !important;
}

/* Remove bullet marker */
.datacamp-exercise ol li::before, .datacamp-exercise ul li::before {
  content: '' !important;
}
</style>


<script language="javascript">
var submitAnswer = function() {

  var radios = document.getElementsByName('choice');
  var val= "";
  for (var i = 0, length = radios.length; i < length; i++) {
      if (radios[i].checked) {
         val = radios[i].value; 
         break;
       }
  }
  
  if (val == "" ) {
    alert('please select choice answer');
  } else if ( val == "Correct" ) {
    alert('Answer is correct !');
  } else {
    alert('Answer is wrong');
  }
};
</script>





<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-125587869-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-125587869-1');
</script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Regression Modeling Online Tutorial</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-regression-modeling"><i class="fa fa-check"></i>About Regression Modeling</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#resources"><i class="fa fa-check"></i>Resources</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#tutorial-description"><i class="fa fa-check"></i>Tutorial Description</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#welcome-to-the-tutorial-video"><i class="fa fa-check"></i>Welcome to the Tutorial Video</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html"><i class="fa fa-check"></i><b>1</b> Regression and the Normal Distribution</a><ul>
<li class="chapter" data-level="1.1" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#fitting-a-normal-distribution"><i class="fa fa-check"></i><b>1.1</b> Fitting a normal distribution</a><ul>
<li class="chapter" data-level="1.1.1" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#video"><i class="fa fa-check"></i><b>1.1.1</b> Video</a></li>
<li class="chapter" data-level="1.1.2" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#exercise.-fitting-galtons-height-data"><i class="fa fa-check"></i><b>1.1.2</b> Exercise. Fitting Galton’s height data</a></li>
<li class="chapter" data-level="1.1.3" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#exercise.-visualizing-childs-height-distribution"><i class="fa fa-check"></i><b>1.1.3</b> Exercise. Visualizing child’s height distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#visualizing-distributions"><i class="fa fa-check"></i><b>1.2</b> Visualizing distributions</a><ul>
<li class="chapter" data-level="1.2.1" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#video-1"><i class="fa fa-check"></i><b>1.2.1</b> Video</a></li>
<li class="chapter" data-level="1.2.2" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#exercise.-visualizing-bodily-injury-claims-with-density-plots"><i class="fa fa-check"></i><b>1.2.2</b> Exercise. Visualizing bodily injury claims with density plots</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#summarizing-distributions"><i class="fa fa-check"></i><b>1.3</b> Summarizing distributions</a><ul>
<li class="chapter" data-level="1.3.1" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#video-2"><i class="fa fa-check"></i><b>1.3.1</b> Video</a></li>
<li class="chapter" data-level="1.3.2" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#exercise.-summarizing-bodily-injury-claims-with-box-and-qq-plots"><i class="fa fa-check"></i><b>1.3.2</b> Exercise. Summarizing bodily injury claims with box and qq plots</a></li>
<li class="chapter" data-level="1.3.3" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#exercise.-effects-on-distributions-of-removing-the-largest-claim"><i class="fa fa-check"></i><b>1.3.3</b> Exercise. Effects on distributions of removing the largest claim</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#transformations"><i class="fa fa-check"></i><b>1.4</b> Transformations</a><ul>
<li class="chapter" data-level="1.4.1" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#video-3"><i class="fa fa-check"></i><b>1.4.1</b> Video</a></li>
<li class="chapter" data-level="1.4.2" data-path="regression-and-the-normal-distribution.html"><a href="regression-and-the-normal-distribution.html#exercise.-distribution-of-transformed-bodily-injury-claims"><i class="fa fa-check"></i><b>1.4.2</b> Exercise. Distribution of transformed bodily injury claims</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html"><i class="fa fa-check"></i><b>2</b> Basic Linear Regression</a><ul>
<li class="chapter" data-level="2.1" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#correlation"><i class="fa fa-check"></i><b>2.1</b> Correlation</a><ul>
<li class="chapter" data-level="2.1.1" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#video-4"><i class="fa fa-check"></i><b>2.1.1</b> Video</a></li>
<li class="chapter" data-level="2.1.2" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#exercise.-correlations-and-the-wisconsin-lottery"><i class="fa fa-check"></i><b>2.1.2</b> Exercise. Correlations and the Wisconsin lottery</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#method-of-least-squares"><i class="fa fa-check"></i><b>2.2</b> Method of least squares</a><ul>
<li class="chapter" data-level="2.2.1" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#video-5"><i class="fa fa-check"></i><b>2.2.1</b> Video</a></li>
<li class="chapter" data-level="2.2.2" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#exercise.-least-squares-fit-using-housing-prices"><i class="fa fa-check"></i><b>2.2.2</b> Exercise. Least squares fit using housing prices</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#understanding-variability"><i class="fa fa-check"></i><b>2.3</b> Understanding variability</a><ul>
<li class="chapter" data-level="2.3.1" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#video-6"><i class="fa fa-check"></i><b>2.3.1</b> Video</a></li>
<li class="chapter" data-level="2.3.2" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#exercise.-summarizing-measures-of-uncertainty"><i class="fa fa-check"></i><b>2.3.2</b> Exercise. Summarizing measures of uncertainty</a></li>
<li class="chapter" data-level="2.3.3" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#exercise.-effects-of-linear-transforms-on-measures-of-uncertainty"><i class="fa fa-check"></i><b>2.3.3</b> Exercise. Effects of linear transforms on measures of uncertainty</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#statistical-inference"><i class="fa fa-check"></i><b>2.4</b> Statistical inference</a><ul>
<li class="chapter" data-level="2.4.1" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#video-7"><i class="fa fa-check"></i><b>2.4.1</b> Video</a></li>
<li class="chapter" data-level="2.4.2" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#exercise.-statistical-inference-and-wisconsin-lottery"><i class="fa fa-check"></i><b>2.4.2</b> Exercise. Statistical inference and Wisconsin lottery</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#diagnostics"><i class="fa fa-check"></i><b>2.5</b> Diagnostics</a><ul>
<li class="chapter" data-level="2.5.1" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#video-8"><i class="fa fa-check"></i><b>2.5.1</b> Video</a></li>
<li class="chapter" data-level="2.5.2" data-path="basic-linear-regression.html"><a href="basic-linear-regression.html#exercise.-assessing-outliers-in-lottery-sales"><i class="fa fa-check"></i><b>2.5.2</b> Exercise. Assessing outliers in lottery sales</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html"><i class="fa fa-check"></i><b>3</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#term-life-data"><i class="fa fa-check"></i>Term Life Data</a></li>
<li class="chapter" data-level="3.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#method-of-least-squares-1"><i class="fa fa-check"></i><b>3.1</b> Method of least squares</a><ul>
<li class="chapter" data-level="3.1.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#video-9"><i class="fa fa-check"></i><b>3.1.1</b> Video</a></li>
<li class="chapter" data-level="3.1.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise.-least-squares-and-term-life-data"><i class="fa fa-check"></i><b>3.1.2</b> Exercise. Least squares and term life data</a></li>
<li class="chapter" data-level="3.1.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise.-interpreting-coefficients-as-proportional-changes"><i class="fa fa-check"></i><b>3.1.3</b> Exercise. Interpreting coefficients as proportional changes</a></li>
<li class="chapter" data-level="3.1.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise.-interpreting-coefficients-as-elasticities"><i class="fa fa-check"></i><b>3.1.4</b> Exercise. Interpreting coefficients as elasticities</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#statistical-inference-and-multiple-linear-regresson"><i class="fa fa-check"></i><b>3.2</b> Statistical inference and multiple linear regresson</a><ul>
<li class="chapter" data-level="3.2.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#video-10"><i class="fa fa-check"></i><b>3.2.1</b> Video</a></li>
<li class="chapter" data-level="3.2.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise.-statistical-inference-and-term-life"><i class="fa fa-check"></i><b>3.2.2</b> Exercise. Statistical inference and term life</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#binary-variables"><i class="fa fa-check"></i><b>3.3</b> Binary variables</a><ul>
<li class="chapter" data-level="3.3.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#video-11"><i class="fa fa-check"></i><b>3.3.1</b> Video</a></li>
<li class="chapter" data-level="3.3.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise.-binary-variables-and-term-life"><i class="fa fa-check"></i><b>3.3.2</b> Exercise. Binary variables and term life</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#categorical-variables"><i class="fa fa-check"></i><b>3.4</b> Categorical variables</a><ul>
<li class="chapter" data-level="3.4.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#video-12"><i class="fa fa-check"></i><b>3.4.1</b> Video</a></li>
<li class="chapter" data-level="3.4.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise.-categorical-variables-and-wisconsin-hospital-costs"><i class="fa fa-check"></i><b>3.4.2</b> Exercise. Categorical variables and Wisconsin hospital costs</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#general-linear-hypothesis"><i class="fa fa-check"></i><b>3.5</b> General linear hypothesis</a><ul>
<li class="chapter" data-level="3.5.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#video-13"><i class="fa fa-check"></i><b>3.5.1</b> Video</a></li>
<li class="chapter" data-level="3.5.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise.-hypothesis-testing-and-term-life"><i class="fa fa-check"></i><b>3.5.2</b> Exercise. Hypothesis testing and term life</a></li>
<li class="chapter" data-level="3.5.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise.-hypothesis-testing-and-wisconsin-hospital-costs"><i class="fa fa-check"></i><b>3.5.3</b> Exercise. Hypothesis testing and Wisconsin hospital costs</a></li>
<li class="chapter" data-level="3.5.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise.-hypothesis-testing-and-auto-claims"><i class="fa fa-check"></i><b>3.5.4</b> Exercise. Hypothesis testing and auto claims</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="variable-selection.html"><a href="variable-selection.html"><i class="fa fa-check"></i><b>4</b> Variable Selection</a><ul>
<li class="chapter" data-level="4.1" data-path="variable-selection.html"><a href="variable-selection.html#an-iterative-approach-to-data-analysis-and-modeling"><i class="fa fa-check"></i><b>4.1</b> An iterative approach to data analysis and modeling</a><ul>
<li class="chapter" data-level="4.1.1" data-path="variable-selection.html"><a href="variable-selection.html#video-14"><i class="fa fa-check"></i><b>4.1.1</b> Video</a></li>
<li class="chapter" data-level="4.1.2" data-path="variable-selection.html"><a href="variable-selection.html#mc-exercise.-an-iterative-approach-to-data-modeling"><i class="fa fa-check"></i><b>4.1.2</b> MC Exercise. An iterative approach to data modeling</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="variable-selection.html"><a href="variable-selection.html#automatic-variable-selection-procedures"><i class="fa fa-check"></i><b>4.2</b> Automatic variable selection procedures</a><ul>
<li class="chapter" data-level="4.2.1" data-path="variable-selection.html"><a href="variable-selection.html#video-15"><i class="fa fa-check"></i><b>4.2.1</b> Video</a></li>
<li class="chapter" data-level="4.2.2" data-path="variable-selection.html"><a href="variable-selection.html#exercise.-data-snooping-in-stepwise-regression"><i class="fa fa-check"></i><b>4.2.2</b> Exercise. Data-snooping in stepwise regression</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="variable-selection.html"><a href="variable-selection.html#residual-analysis"><i class="fa fa-check"></i><b>4.3</b> Residual analysis</a><ul>
<li class="chapter" data-level="4.3.1" data-path="variable-selection.html"><a href="variable-selection.html#video-16"><i class="fa fa-check"></i><b>4.3.1</b> Video</a></li>
<li class="chapter" data-level="4.3.2" data-path="variable-selection.html"><a href="variable-selection.html#exercise.-residual-analysis-and-risk-manager-survey"><i class="fa fa-check"></i><b>4.3.2</b> Exercise. Residual analysis and risk manager survey</a></li>
<li class="chapter" data-level="4.3.3" data-path="variable-selection.html"><a href="variable-selection.html#exercise.-added-variable-plot-and-refrigerator-prices"><i class="fa fa-check"></i><b>4.3.3</b> Exercise. Added variable plot and refrigerator prices</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="variable-selection.html"><a href="variable-selection.html#unusual-observations"><i class="fa fa-check"></i><b>4.4</b> Unusual observations</a><ul>
<li class="chapter" data-level="4.4.1" data-path="variable-selection.html"><a href="variable-selection.html#video-17"><i class="fa fa-check"></i><b>4.4.1</b> Video</a></li>
<li class="chapter" data-level="4.4.2" data-path="variable-selection.html"><a href="variable-selection.html#exercise.-outlier-example"><i class="fa fa-check"></i><b>4.4.2</b> Exercise. Outlier example</a></li>
<li class="chapter" data-level="4.4.3" data-path="variable-selection.html"><a href="variable-selection.html#exercise.-high-leverage-and-risk-manager-survey"><i class="fa fa-check"></i><b>4.4.3</b> Exercise. High leverage and risk manager survey</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="variable-selection.html"><a href="variable-selection.html#collinearity"><i class="fa fa-check"></i><b>4.5</b> Collinearity</a><ul>
<li class="chapter" data-level="4.5.1" data-path="variable-selection.html"><a href="variable-selection.html#video-18"><i class="fa fa-check"></i><b>4.5.1</b> Video</a></li>
<li class="chapter" data-level="4.5.2" data-path="variable-selection.html"><a href="variable-selection.html#exercise.-collinearity-and-term-life"><i class="fa fa-check"></i><b>4.5.2</b> Exercise. Collinearity and term life</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="variable-selection.html"><a href="variable-selection.html#selection-criteria"><i class="fa fa-check"></i><b>4.6</b> Selection criteria</a><ul>
<li class="chapter" data-level="4.6.1" data-path="variable-selection.html"><a href="variable-selection.html#video-19"><i class="fa fa-check"></i><b>4.6.1</b> Video</a></li>
<li class="chapter" data-level="4.6.2" data-path="variable-selection.html"><a href="variable-selection.html#exercise.-cross-validation-and-term-life"><i class="fa fa-check"></i><b>4.6.2</b> Exercise. Cross-validation and term life</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html"><i class="fa fa-check"></i><b>5</b> Interpreting Regression Results</a><ul>
<li class="chapter" data-level="5.1" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#case-study-meps-health-expenditures"><i class="fa fa-check"></i><b>5.1</b> Case study: MEPS health expenditures</a><ul>
<li class="chapter" data-level="5.1.1" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#video-20"><i class="fa fa-check"></i><b>5.1.1</b> Video</a></li>
<li class="chapter" data-level="5.1.2" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#exercise.-summarizing-data"><i class="fa fa-check"></i><b>5.1.2</b> Exercise. Summarizing data</a></li>
<li class="chapter" data-level="5.1.3" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#exercise.-fit-a-benchmark-multiple-linear-regression-model"><i class="fa fa-check"></i><b>5.1.3</b> Exercise. Fit a benchmark multiple linear regression model</a></li>
<li class="chapter" data-level="5.1.4" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#exercise.-variable-selection"><i class="fa fa-check"></i><b>5.1.4</b> Exercise. Variable selection</a></li>
<li class="chapter" data-level="5.1.5" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#exercise.-model-comparisons-using-cross-validation"><i class="fa fa-check"></i><b>5.1.5</b> Exercise. Model comparisons using cross-validation</a></li>
<li class="chapter" data-level="5.1.6" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#exercise.-out-of-sample-validation"><i class="fa fa-check"></i><b>5.1.6</b> Exercise. Out of sample validation</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#what-the-modeling-procedure-tells-us"><i class="fa fa-check"></i><b>5.2</b> What the modeling procedure tells us</a><ul>
<li class="chapter" data-level="5.2.1" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#video-21"><i class="fa fa-check"></i><b>5.2.1</b> Video</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#the-importance-of-variable-selection"><i class="fa fa-check"></i><b>5.3</b> The importance of variable selection</a><ul>
<li class="chapter" data-level="5.3.1" data-path="interpreting-regression-results.html"><a href="interpreting-regression-results.html#video-22"><i class="fa fa-check"></i><b>5.3.1</b> Video</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/ewfreesRes/RegressModel" target="blank">Regression Modeling Tutorial on GitHub</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Online Tutorial on <code>Regression Modeling with Actuarial and Financial Applications</code></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="variable-selection" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Variable Selection</h1>
<p><strong>Chapter description</strong></p>
<p>This chapter describes tools and techniques to help you select variables to enter into a linear regression model, beginning with an iterative model selection process. In applications with many potential explanatory variables, automatic variable selection procedures are available that will help you quickly evaluate many models. Nonetheless, automatic procedures have serious limitations including the inability to account properly for nonlinearities such as the impact of unusual points; this chapter expands upon the Chapter 2 discussion of unusual points. It also describes collinearity, a common feature of regression data where explanatory variables are linearly related to one another. Other topics that impact variable selection, including out-of-sample validation, are also introduced.</p>
<div id="an-iterative-approach-to-data-analysis-and-modeling" class="section level2">
<h2><span class="header-section-number">4.1</span> An iterative approach to data analysis and modeling</h2>
<hr />
<p>In this section, you learn how to:</p>
<ul>
<li>Describe the iterative approach to data analysis and modeling.</li>
</ul>
<hr />
<div id="video-14" class="section level3">
<h3><span class="header-section-number">4.1.1</span> Video</h3>
<center>
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/sp/166090200/embedIframeJs/uiconf_id/25916071/partner_id/1660902?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_53yv68dd&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;&amp;wid=0_yxfgvaql" width="649" height="401" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" frameborder="0" title="Kaltura Player">
</iframe>
</center>
<div id="video-overhead-details-14" class="section level4 unnumbered">
<h4>Video Overhead Details</h4>
<div class="tab">
<button class="tablinks" onclick="openTab(event, 'Over4.1A')">
A Details. Iterative approach
</button>
<button class="tablinks" onclick="openTab(event, 'Over4.1B')">
B Details. Many possible models
</button>
<button class="tablinks" onclick="openTab(event, 'Over4.1C')">
C Details. Model validation
</button>
</div>
<div id="Over4.1A" class="tabcontent">
<span class="topright" onclick="this.parentElement.style.display=&#39;none&#39;">Hide</span>
<h3>
A Details. Iterative approach
</h3>
<p>
<ul>
<li>Model formulation stage</li>
<li>Fitting</li>
<li>Diagnostic checking - the data and model must be consistent with one another before additional inferences can be made.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot.new</span>()
<span class="kw">par</span>(<span class="dt">mar=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>), <span class="dt">cex=</span><span class="fl">0.9</span>)
<span class="kw">plot.window</span>(<span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">18</span>),<span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">5</span>,<span class="dv">5</span>))

<span class="kw">text</span>(<span class="dv">1</span>,<span class="dv">3</span>,<span class="dt">labels=</span><span class="st">&quot;DATA&quot;</span>,<span class="dt">adj=</span><span class="dv">0</span>, <span class="dt">cex=</span><span class="fl">0.8</span>)
<span class="kw">text</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dt">labels=</span><span class="st">&quot;PLOTS&quot;</span>,<span class="dt">adj=</span><span class="dv">0</span>, <span class="dt">cex=</span><span class="fl">0.8</span>)
<span class="kw">text</span>(<span class="dv">1</span>,<span class="op">-</span><span class="dv">3</span>,<span class="dt">labels=</span><span class="st">&quot;THEORY&quot;</span>,<span class="dt">adj=</span><span class="dv">0</span>, <span class="dt">cex=</span><span class="fl">0.8</span>)
<span class="kw">text</span>(<span class="fl">3.9</span>,<span class="dv">0</span>,<span class="dt">labels=</span><span class="st">&quot;MODEL</span><span class="ch">\n</span><span class="st">FORMULATION&quot;</span>,<span class="dt">adj=</span><span class="dv">0</span>, <span class="dt">cex=</span><span class="fl">0.8</span>)
<span class="kw">text</span>(<span class="fl">8.1</span>,<span class="dv">0</span>,<span class="dt">labels=</span><span class="st">&quot;FITTING&quot;</span>,<span class="dt">adj=</span><span class="dv">0</span>, <span class="dt">cex=</span><span class="fl">0.8</span>)
<span class="kw">text</span>(<span class="dv">11</span>,<span class="dv">0</span>,<span class="dt">labels=</span><span class="st">&quot;DIAGNOSTIC</span><span class="ch">\n</span><span class="st">CHECKING&quot;</span>,<span class="dt">adj=</span><span class="dv">0</span>, <span class="dt">cex=</span><span class="fl">0.8</span>)
<span class="kw">text</span>(<span class="dv">15</span>,<span class="dv">0</span>,<span class="dt">labels=</span><span class="st">&quot;INFERENCE&quot;</span>,<span class="dt">adj=</span><span class="dv">0</span>, <span class="dt">cex=</span><span class="fl">0.8</span>)
<span class="kw">text</span>(<span class="fl">14.1</span>,<span class="fl">0.5</span>,<span class="dt">labels=</span><span class="st">&quot;OK&quot;</span>,<span class="dt">adj=</span><span class="dv">0</span>, <span class="dt">cex=</span><span class="fl">0.6</span>)

<span class="kw">rect</span>(<span class="fl">0.8</span>,<span class="fl">2.0</span>,<span class="fl">2.6</span>,<span class="fl">4.0</span>)
<span class="kw">arrows</span>(<span class="fl">1.7</span>,<span class="fl">2.0</span>,<span class="fl">1.7</span>,<span class="fl">1.0</span>,<span class="dt">code=</span><span class="dv">2</span>,<span class="dt">lwd=</span><span class="dv">2</span>,<span class="dt">angle=</span><span class="dv">25</span>,<span class="dt">length=</span><span class="fl">0.10</span>)
<span class="kw">rect</span>(<span class="fl">0.8</span>,<span class="op">-</span><span class="fl">1.0</span>,<span class="fl">2.6</span>,<span class="fl">1.0</span>)
<span class="kw">arrows</span>(<span class="fl">1.7</span>,<span class="op">-</span><span class="fl">2.0</span>,<span class="fl">1.7</span>,<span class="op">-</span><span class="fl">1.0</span>,<span class="dt">code=</span><span class="dv">2</span>,<span class="dt">lwd=</span><span class="dv">2</span>,<span class="dt">angle=</span><span class="dv">25</span>,<span class="dt">length=</span><span class="fl">0.10</span>)
<span class="kw">rect</span>(<span class="fl">0.8</span>,<span class="op">-</span><span class="fl">4.0</span>,<span class="fl">2.6</span>,<span class="op">-</span><span class="fl">2.0</span>)

<span class="kw">arrows</span>(<span class="fl">2.6</span>,<span class="dv">0</span>,<span class="fl">3.2</span>,<span class="dv">0</span>,<span class="dt">code=</span><span class="dv">2</span>,<span class="dt">lwd=</span><span class="dv">2</span>,<span class="dt">angle=</span><span class="dv">25</span>,<span class="dt">length=</span><span class="fl">0.10</span>)

x&lt;-<span class="kw">c</span>(<span class="dv">5</span>,<span class="fl">7.0</span>,<span class="dv">5</span>,<span class="fl">3.2</span>)
y&lt;-<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">0</span>,<span class="op">-</span><span class="dv">2</span>,<span class="dv">0</span>)
<span class="kw">polygon</span>(x,y)
<span class="kw">arrows</span>(<span class="fl">7.0</span>,<span class="dv">0</span>,<span class="fl">8.0</span>,<span class="dv">0</span>,<span class="dt">code=</span><span class="dv">2</span>,<span class="dt">lwd=</span><span class="dv">2</span>,<span class="dt">angle=</span><span class="dv">25</span>,<span class="dt">length=</span><span class="fl">0.10</span>)

<span class="kw">rect</span>(<span class="fl">8.0</span>,<span class="op">-</span><span class="fl">1.0</span>,<span class="fl">9.7</span>,<span class="fl">1.0</span>)
<span class="kw">arrows</span>(<span class="fl">9.7</span>,<span class="dv">0</span>,<span class="fl">10.2</span>,<span class="dv">0</span>,<span class="dt">code=</span><span class="dv">2</span>,<span class="dt">lwd=</span><span class="dv">2</span>,<span class="dt">angle=</span><span class="dv">25</span>,<span class="dt">length=</span><span class="fl">0.10</span>)

x1&lt;-<span class="kw">c</span>(<span class="dv">12</span>,<span class="fl">14.0</span>,<span class="dv">12</span>,<span class="fl">10.2</span>)
y1&lt;-<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">0</span>,<span class="op">-</span><span class="dv">2</span>,<span class="dv">0</span>)
<span class="kw">polygon</span>(x1,y1)
<span class="kw">arrows</span>(<span class="fl">14.0</span>,<span class="dv">0</span>,<span class="fl">14.8</span>,<span class="dv">0</span>,<span class="dt">code=</span><span class="dv">2</span>,<span class="dt">lwd=</span><span class="dv">2</span>,<span class="dt">angle=</span><span class="dv">25</span>,<span class="dt">length=</span><span class="fl">0.10</span>)

<span class="kw">rect</span>(<span class="fl">14.8</span>,<span class="op">-</span><span class="fl">1.0</span>,<span class="fl">17.5</span>,<span class="fl">1.0</span>)
<span class="kw">arrows</span>(<span class="dv">12</span>,<span class="op">-</span><span class="fl">2.0</span>,<span class="dv">12</span>,<span class="op">-</span><span class="dv">3</span>,<span class="dt">code=</span><span class="dv">2</span>,<span class="dt">lwd=</span><span class="dv">2</span>,<span class="dt">angle=</span><span class="dv">25</span>,<span class="dt">length=</span><span class="fl">0.10</span>)
<span class="kw">arrows</span>(<span class="dv">12</span>,<span class="op">-</span><span class="fl">3.0</span>,<span class="dv">5</span>,<span class="op">-</span><span class="dv">3</span>,<span class="dt">code=</span><span class="dv">2</span>,<span class="dt">lwd=</span><span class="dv">2</span>,<span class="dt">angle=</span><span class="dv">25</span>,<span class="dt">length=</span><span class="fl">0.10</span>)
<span class="kw">arrows</span>(<span class="dv">5</span>,<span class="op">-</span><span class="fl">3.0</span>,<span class="dv">5</span>,<span class="op">-</span><span class="dv">2</span>,<span class="dt">code=</span><span class="dv">2</span>,<span class="dt">lwd=</span><span class="dv">2</span>,<span class="dt">angle=</span><span class="dv">25</span>,<span class="dt">length=</span><span class="fl">0.10</span>)</code></pre></div>
<p><img src="RegressModelDataCamp_files/figure-html/unnamed-chunk-138-1.png" width="672" /></p>
</p>
</div>
<div id="Over4.1B" class="tabcontent">
<span class="topright" onclick="this.parentElement.style.display=&#39;none&#39;">Hide</span>
<h3>
B Details. Many possible models
</h3>
<p>
<p><span class="math display">\[
\begin{array}{l|r}
\hline
\text{E }y = \beta _{0} &amp;     \text{1 model with no variables }  \\
\text{E }y = \beta _{0}+\beta_1 x_{i}, &amp;  \text{4 models with one variable}  \\
\text{E }y = \beta _{0}+\beta_1 x_{i}+\beta_{2} x_{j}, &amp;  \text{6 models with two variables}  \\
\text{E }y = \beta _{0}+\beta_{1} x_{1}+\beta_{2} x_{j} +\beta_{3} x_{k},&amp;  \text{4 models with three variables} \\
\text{E }y = \beta_{0} + \beta_{1} x_{1} + \beta_{2} x_{2} +\beta_{3} x_{3}+\beta_{4} x_{4} &amp;  \text{1 model with all variables}  \\ 
\hline
\end{array}
\]</span></p>
<ul>
<li>With <em>k</em> explanatory variables, there are <span class="math inline">\(2^k\)</span> possible linear models</li>
<li>There are infinitely many nonlinear ones!!</li>
</ul>
</p>
</div>
<div id="Over4.1C" class="tabcontent">
<span class="topright" onclick="this.parentElement.style.display=&#39;none&#39;">Hide</span>
<h3>
C Details. Model validation
</h3>
<p>
<ul>
<li>Model validation is the process of confirming our proposed model.</li>
<li>Concern: <em>data-snooping</em> - fitting many models to a single set of data.
<ul>
<li>Response to concern: <em>out-of-sample validation</em>.</li>
<li>Divide the data into <em>model development</em>, or <em>training</em> and <em>validation</em>, or <em>test</em>, subsamples.</li>
</ul></li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mai=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="fl">0.1</span>,<span class="dv">0</span>,<span class="dv">0</span>))
<span class="kw">plot.new</span>()
<span class="kw">plot.window</span>(<span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">18</span>),<span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">10</span>,<span class="dv">10</span>))
<span class="kw">rect</span>(<span class="dv">1</span>,<span class="op">-</span><span class="fl">1.2</span>,<span class="dv">14</span>,<span class="fl">1.2</span>)
<span class="kw">rect</span>(<span class="dv">7</span>,<span class="dv">4</span>,<span class="dv">15</span>,<span class="dv">8</span>)
<span class="kw">rect</span>(<span class="dv">1</span>,<span class="op">-</span><span class="dv">8</span>,<span class="dv">6</span>,<span class="op">-</span><span class="dv">4</span>)
x&lt;-<span class="kw">seq</span>(<span class="fl">1.5</span>,<span class="dv">9</span>,<span class="dt">length=</span><span class="dv">6</span>)
y&lt;-<span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">6</span>)
<span class="kw">text</span>(x,y,<span class="dt">labels=</span><span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">6</span>),<span class="dt">cex=</span><span class="fl">1.5</span>)
x1&lt;-<span class="kw">seq</span>(<span class="fl">10.5</span>,<span class="fl">11.5</span>,<span class="dt">length=</span><span class="dv">3</span>)
y1&lt;-<span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">3</span>)
<span class="kw">text</span>(x1,y1,<span class="dt">labels=</span><span class="kw">rep</span>(<span class="st">&quot;.&quot;</span>,<span class="dv">3</span>),<span class="dt">cex=</span><span class="dv">3</span>)
<span class="kw">text</span>(<span class="dv">13</span>,<span class="dv">0</span>,<span class="dt">labels=</span><span class="st">&quot;n&quot;</span>,<span class="dt">cex=</span><span class="fl">1.5</span>)

<span class="kw">text</span>(<span class="dv">15</span>,<span class="dv">0</span>,<span class="dt">labels=</span><span class="st">&quot;ORIGINAL</span><span class="ch">\n</span><span class="st">SAMPLE</span><span class="ch">\n</span><span class="st">SIZE n&quot;</span>,<span class="dt">adj=</span><span class="dv">0</span>)
<span class="kw">text</span>(<span class="fl">7.5</span>,<span class="dv">6</span>,<span class="dt">labels=</span><span class="st">&quot;MODEL DEVELOPMENT</span><span class="ch">\n</span><span class="st">SUBSAMPLE SIZE&quot;</span>,<span class="dt">adj=</span><span class="dv">0</span>)
<span class="kw">text</span>(<span class="fl">12.5</span>,<span class="fl">5.3</span>, <span class="kw">expression</span>(n[<span class="dv">1</span>]), <span class="dt">adj=</span><span class="dv">0</span>, <span class="dt">cex=</span><span class="fl">1.1</span>)
<span class="kw">text</span>(<span class="fl">1.4</span>,<span class="op">-</span><span class="dv">6</span>,<span class="dt">labels=</span><span class="st">&quot;VALIDATION</span><span class="ch">\n</span><span class="st">SUBSAMPLE</span><span class="ch">\n</span><span class="st">SIZE&quot;</span>,<span class="dt">adj=</span><span class="dv">0</span>)
<span class="kw">text</span>(<span class="fl">2.8</span>,<span class="op">-</span><span class="fl">7.2</span>,<span class="kw">expression</span>(n[<span class="dv">2</span>]),<span class="dt">adj=</span><span class="dv">0</span>, <span class="dt">cex=</span><span class="fl">1.1</span>)

<span class="kw">arrows</span>(<span class="fl">1.8</span>,<span class="fl">0.8</span>,<span class="fl">8.3</span>,<span class="fl">3.9</span>,<span class="dt">code=</span><span class="dv">2</span>,<span class="dt">lwd=</span><span class="dv">2</span>,<span class="dt">angle=</span><span class="dv">15</span>,<span class="dt">length=</span><span class="fl">0.2</span>)
<span class="kw">arrows</span>(<span class="fl">4.8</span>,<span class="fl">0.8</span>,<span class="dv">9</span>,<span class="fl">3.8</span>,<span class="dt">code=</span><span class="dv">2</span>,<span class="dt">lwd=</span><span class="dv">2</span>,<span class="dt">angle=</span><span class="dv">15</span>,<span class="dt">length=</span><span class="fl">0.2</span>)
<span class="kw">arrows</span>(<span class="fl">9.1</span>,<span class="fl">0.9</span>,<span class="fl">9.5</span>,<span class="fl">3.8</span>,<span class="dt">code=</span><span class="dv">2</span>,<span class="dt">lwd=</span><span class="dv">2</span>,<span class="dt">angle=</span><span class="dv">15</span>,<span class="dt">length=</span><span class="fl">0.2</span>)
<span class="kw">arrows</span>(<span class="fl">12.8</span>,<span class="fl">0.8</span>,<span class="dv">10</span>,<span class="fl">3.8</span>,<span class="dt">code=</span><span class="dv">2</span>,<span class="dt">lwd=</span><span class="dv">2</span>,<span class="dt">angle=</span><span class="dv">15</span>,<span class="dt">length=</span><span class="fl">0.2</span>)
<span class="kw">arrows</span>(<span class="fl">2.9</span>,<span class="op">-</span><span class="fl">0.9</span>,<span class="fl">2.5</span>,<span class="op">-</span><span class="fl">3.8</span>,<span class="dt">code=</span><span class="dv">2</span>,<span class="dt">lwd=</span><span class="dv">2</span>,<span class="dt">angle=</span><span class="dv">15</span>,<span class="dt">length=</span><span class="fl">0.2</span>)
<span class="kw">arrows</span>(<span class="fl">5.9</span>,<span class="op">-</span><span class="fl">0.9</span>,<span class="fl">3.1</span>,<span class="op">-</span><span class="fl">3.8</span>,<span class="dt">code=</span><span class="dv">2</span>,<span class="dt">lwd=</span><span class="dv">2</span>,<span class="dt">angle=</span><span class="dv">15</span>,<span class="dt">length=</span><span class="fl">0.2</span>)
<span class="kw">arrows</span>(<span class="fl">7.4</span>,<span class="op">-</span><span class="fl">0.9</span>,<span class="fl">3.5</span>,<span class="op">-</span><span class="fl">3.8</span>,<span class="dt">code=</span><span class="dv">2</span>,<span class="dt">lwd=</span><span class="dv">2</span>,<span class="dt">angle=</span><span class="dv">15</span>,<span class="dt">length=</span><span class="fl">0.2</span>)</code></pre></div>
<p><img src="RegressModelDataCamp_files/figure-html/unnamed-chunk-139-1.png" width="672" /></p>
</p>
</div>
</div>
</div>
<div id="mc-exercise.-an-iterative-approach-to-data-modeling" class="section level3">
<h3><span class="header-section-number">4.1.2</span> MC Exercise. An iterative approach to data modeling</h3>
<p>Which of the following is not true?</p>
<ul>
<li>A. Diagnostic checking reveals symptoms of mistakes made in previous specifications.</li>
<li>B. Diagnostic checking provides ways to correct mistakes made in previous specifications.</li>
<li>C. Model formulation is accomplished by using prior knowledge of relationships.</li>
<li>D. Understanding theoretical model properties is not really helpful when matching a model to data or inferring general relationships based on the data.</li>
</ul>
</div>
</div>
<div id="automatic-variable-selection-procedures" class="section level2">
<h2><span class="header-section-number">4.2</span> Automatic variable selection procedures</h2>
<hr />
<p>In this section, you learn how to:</p>
<ul>
<li>Identify some examples of automatic variable selection procedures</li>
<li>Describe the purpose of automatic variable selection procedures and their limitations</li>
<li>Describe “data-snooping”</li>
</ul>
<hr />
<div id="video-15" class="section level3">
<h3><span class="header-section-number">4.2.1</span> Video</h3>
<center>
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/sp/166090200/embedIframeJs/uiconf_id/25916071/partner_id/1660902?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_94bk7tyn&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;&amp;wid=0_vq3apay6" width="649" height="401" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" frameborder="0" title="Kaltura Player">
</iframe>
</center>
<div id="video-overhead-details-15" class="section level4 unnumbered">
<h4>Video Overhead Details</h4>
<div class="tab">
<button class="tablinks" onclick="openTab(event, 'Over4.2A')">
A Details. Classic stepwise regression algorithm
</button>
<button class="tablinks" onclick="openTab(event, 'Over4.2B')">
B Details. Drawbacks of stepwise regression
</button>
<button class="tablinks" onclick="openTab(event, 'Over4.2C')">
C Details. Data-snooping in stepwise regression
</button>
<button class="tablinks" onclick="openTab(event, 'Over4.2D')">
D Details. Variants of stepwise regression
</button>
<button class="tablinks" onclick="openTab(event, 'Over4.2E')">
E Details. Automatic variable selection procedures
</button>
</div>
<div id="Over4.2A" class="tabcontent">
<span class="topright" onclick="this.parentElement.style.display=&#39;none&#39;">Hide</span>
<h3>
A Details. Classic stepwise regression algorithm
</h3>
<p>
<p>Suppose that the analyst has identified one variable as the outcome, <span class="math inline">\(y\)</span>, and <span class="math inline">\(k\)</span> potential explanatory variables, <span class="math inline">\(x_1, x_2, \ldots, x_k\)</span>.</p>
<ul>
<li>(i). Consider all possible regressions using one explanatory variable. Choose the one with the highest <em>t</em>-statistic.</li>
<li>(ii). Add a variable to the model from the previous step. The variable to enter is with the highest <em>t</em>-statistic.</li>
<li>(iii). Delete a variable to the model from the previous step. Delete the variable with the small <em>t</em>-statistic if the statistic is less than, e.g., 2 in absolute value.</li>
<li>(iv). Repeat steps (ii) and (iii) until all possible additions and deletions are performed.</li>
</ul>
</p>
</div>
<div id="Over4.2B" class="tabcontent">
<span class="topright" onclick="this.parentElement.style.display=&#39;none&#39;">Hide</span>
<h3>
B Details. Drawbacks of stepwise regression
</h3>
<p>
<ul>
<li>The procedure “snoops” through a large number of models and may fit the data “too well.”</li>
<li>There is no guarantee that the selected model is the best.
<ul>
<li>The algorithm does not consider models that are based on nonlinear combinations of explanatory variables.</li>
<li>It ignores the presence of outliers and high leverage points.</li>
</ul></li>
</ul>
</p>
</div>
<div id="Over4.2C" class="tabcontent">
<span class="topright" onclick="this.parentElement.style.display=&#39;none&#39;">Hide</span>
<h3>
C Details. Data-snooping in stepwise regression
</h3>
<p>
<ul>
<li>Generate <span class="math inline">\(y\)</span> and <span class="math inline">\(x_1 - x_{50}\)</span> using a random number generator</li>
<li>By design, there is no relation between <span class="math inline">\(y\)</span> and <span class="math inline">\(x_1 - x_{50}\)</span>.</li>
<li><strong>But</strong>, through stepwise regression, we <strong>“discover”</strong> a relationship that explains 14% of the variation!!!</li>
</ul>
<pre><code>Call: lm(formula = y ~ xvar27 + xvar29 + xvar32, data = X)

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept) -0.04885    0.09531  -0.513   0.6094  
xvar27        0.21063    0.09724   2.166   0.0328 *
xvar29        0.24887    0.10185   2.443   0.0164 *
xvar32        0.25390    0.09823   2.585   0.0112 *

Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 0.9171 on 96 degrees of freedom
Multiple R-squared:  0.1401,    Adjusted R-squared:  0.1132 
F-statistic: 5.212 on 3 and 96 DF,  p-value: 0.002233</code></pre>
</p>
</div>
<div id="Over4.2D" class="tabcontent">
<span class="topright" onclick="this.parentElement.style.display=&#39;none&#39;">Hide</span>
<h3>
D Details. Variants of stepwise regression
</h3>
<p>
<p>This uses the <code>R</code> function <a href="https://www.rdocumentation.org/packages/stats/versions/3.5.0/topics/step">step()</a></p>
<ul>
<li>The option <code>direction</code> can be used to change how variables enter
<ul>
<li>Forward selection. Add one variable at a time without trying to delete variables.</li>
<li>Backwards selection. Start with the full model and delete one variable at a time without trying to add variables.</li>
</ul></li>
<li>The option <code>scope</code> can be used to specify which variables must be included</li>
</ul>
</p>
</div>
<div id="Over4.2E" class="tabcontent">
<span class="topright" onclick="this.parentElement.style.display=&#39;none&#39;">Hide</span>
<h3>
E Details. Automatic variable selection procedures
</h3>
<p>
<ul>
<li>Stepwise regression is a type of automatic variable selection procedure.</li>
<li>These procedures are useful because they can quickly search through several candidate models. They mechanize certain routine tasks and are excellent at discovering patterns in data.</li>
<li>They are so good at detecting patterns that they analyst must be wary of overfitting (data-snooping)<br />
</li>
<li>They can miss certain patterns (nonlinearities, unusual points)</li>
<li>A model suggested by automatic variable selection procedures should be subject to the same careful diagnostic checking procedures as a model arrived at by any other means</li>
</ul>
</p>
</div>
</div>
</div>
<div id="exercise.-data-snooping-in-stepwise-regression" class="section level3">
<h3><span class="header-section-number">4.2.2</span> Exercise. Data-snooping in stepwise regression</h3>
<p><strong>Assignment Text</strong></p>
<p>Automatic variable selection procedures, such as the classic stepwise regression algorithm, are very good at detecting patterns. Sometimes they are too good in the sense that they detect patterns in the sample that are not evident in the population from which the data are drawn. The detect “spurious” patterns.</p>
<p>This exercise illustrates this phenomenom by using a simulation, designed so that the outcome variable (<em>y</em>) and the explanatory variables are mutually independent. So, by design, there is no relationship between the outcome and the explanatory variables.</p>
<p>As part of the code set-up, we have <em>n</em> = 100 observations generated of the outcome <em>y</em> and 50 explanatory variables, <code>xvar1</code> through <code>xvar50</code>. As anticipated, collections of explanatory variables are not statistically significant. However, with the <a href="https://www.rdocumentation.org/packages/stats/versions/3.5.0/topics/step">step()</a> function, you will find some statistically significant relationships!</p>
<p><strong>Instructions</strong></p>
<ul>
<li>Fit a basic linear regression model and MLR model with the first ten explanatory variables. Compare the models via an <em>F</em> test.</li>
<li>Fit a multiple linear regression model with all fifty explanatory variables. Compare this model to the one with ten variables via an <em>F</em> test.</li>
<li>Use the <code>step</code> function to find the best model starting with the fitted model containing all fifty explanatory variables and summarize the fit.</li>
</ul>
<p><strong>Hint.</strong> The code shows stepwise regression using BIC, a criterion that results in simpler models than AIC. For AIC, use the option <code>k=2</code> in the [step()] function (the default)</p>
<div data-datacamp-exercise="" data-height="300" data-encoded="true">
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
</div>
</div>
</div>
<div id="residual-analysis" class="section level2">
<h2><span class="header-section-number">4.3</span> Residual analysis</h2>
<hr />
<p>In this section, you learn how to:</p>
<ul>
<li>Explain how residual analysis can be used to improve a model specification</li>
<li>Use relationships between residuals and potential explanatory variables to improve model specification</li>
</ul>
<hr />
<div id="video-16" class="section level3">
<h3><span class="header-section-number">4.3.1</span> Video</h3>
<center>
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/sp/166090200/embedIframeJs/uiconf_id/25916071/partner_id/1660902?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_bwhmsm8k&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;&amp;wid=0_zvacn361" width="649" height="401" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" frameborder="0" title="Kaltura Player">
</iframe>
</center>
<div id="video-overhead-details-16" class="section level4 unnumbered">
<h4>Video Overhead Details</h4>
<div class="tab">
<button class="tablinks" onclick="openTab(event, 'Over4.3A')">
A Details. Residual analysis
</button>
<button class="tablinks" onclick="openTab(event, 'Over4.3B')">
B Details. Using residuals to select explanatory variables
</button>
<button class="tablinks" onclick="openTab(event, 'Over4.3C')">
C Details. Detecting relationships between residuals and explanatory variables
</button>
</div>
<div id="Over4.3A" class="tabcontent">
<span class="topright" onclick="this.parentElement.style.display=&#39;none&#39;">Hide</span>
<h3>
A Details. Residual analysis
</h3>
<p>
<ul>
<li>Use <span class="math inline">\(e_i = y_i - \hat{y}_i\)</span> as the <em>i</em>th residual.</li>
<li>Later, I will discuss rescaling by, for example, <span class="math inline">\(s\)</span>, to get a standardized residual.</li>
<li><em>Role of residuals</em>: If the model formulation is correct, then residuals should be approximately equal to random errors or “white noise.”</li>
<li><em>Method of attack</em>: Look for patterns in the residuals. Use this information to improve the model specification.</li>
</ul>
</p>
</div>
<div id="Over4.3B" class="tabcontent">
<span class="topright" onclick="this.parentElement.style.display=&#39;none&#39;">Hide</span>
<h3>
B Details. Using residuals to select explanatory variables
</h3>
<p>
<ul>
<li>Residual analysis can help identify additional explanatory variables that may be used to improve the formulation of the model.</li>
<li>If the model is correct, then residuals should resemble random errors and contain no discernible patterns.</li>
<li>Thus, when comparing residuals to explanatory variables, we do not expect any relationships.</li>
<li>If we do detect a relationship, then this suggests the need to control for this additional variable.</li>
</ul>
</p>
</div>
<div id="Over4.3C" class="tabcontent">
<span class="topright" onclick="this.parentElement.style.display=&#39;none&#39;">Hide</span>
<h3>
C Details. Detecting relationships between residuals and explanatory variables
</h3>
<p>
<ul>
<li>Calculate summary statistics and display the distribution of residuals to identify outliers.</li>
<li>Calculate the correlation between the residuals and additional explanatory variables to search for linear relationships.</li>
<li>Create scatter plots between the residuals and additional explanatory variables to search for nonlinear relationships.</li>
</ul>
</p>
</div>
</div>
</div>
<div id="exercise.-residual-analysis-and-risk-manager-survey" class="section level3">
<h3><span class="header-section-number">4.3.2</span> Exercise. Residual analysis and risk manager survey</h3>
<p><strong>Assignment Text</strong></p>
<p>This exercise examines data, pre-loaded in the dataframe <code>survey</code>, from a survey on the cost effectiveness of risk management practices. Risk management practices are activities undertaken by a firm to minimize the potential cost of future losses, such as the event of a fire in a warehouse or an accident that injures employees. This exercise develops a model that can be used to make statements about cost of managing risks.</p>
<p>A measure of risk management cost effectiveness, <code>logcost</code>, is the outcome variable. This variable is defined as total property and casualty premiums and uninsured losses as a proportion of total assets, in logarithmic units. It is a proxy for annual expenditures associated with insurable events, standardized by company size. Explanatory variables include <code>logsize</code>, the logarithm of total firm assets, and <code>indcost</code>, a measure of the firm’s industry risk.</p>
<p><strong>Instructions</strong></p>
<ul>
<li>Fit and summarize a MLR model using <code>logcost</code> as the outcome variable and <code>logsize</code> and <code>indcost</code> as explanatory variables.</li>
<li>Plot residuals of the fitted model versus <code>indcost</code> and superimpose a locally fitted line using the <code>R</code> function <a href="https://www.rdocumentation.org/packages/stats/versions/3.5.0/topics/lowess">lowess()</a>.</li>
<li>Fit and summarize a MLR model of <code>logcost</code> on <code>logsize</code>, <code>indcost</code> and a squared version of <code>indcost</code>.</li>
<li>Plot residuals of the fitted model versus `indcost’ and superimpose a locally fitted line using <a href="https://www.rdocumentation.org/packages/stats/versions/3.5.0/topics/lowess">lowess()</a>.</li>
</ul>
<p><strong>Hint.</strong> You can access model residuals using <code>mlr.survey1$residuals</code> or <code>mlr.survey1($residuals)</code></p>
<div data-datacamp-exercise="" data-height="300" data-encoded="true">
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
</div>
</div>
<div id="exercise.-added-variable-plot-and-refrigerator-prices" class="section level3">
<h3><span class="header-section-number">4.3.3</span> Exercise. Added variable plot and refrigerator prices</h3>
<p><strong>Assignment Text</strong></p>
<p>What characteristics of a refrigerator are important in determining its price (<code>price</code>)? We consider here several characteristics of a refrigerator, including the size of the refrigerator in cubic feet (<code>rsize</code>), the size of the freezer compartment in cubic feet (<code>fsize</code>), the average amount of money spent per year to operate the refrigerator (<code>ecost</code>, for energy cost), the number of shelves in the refrigerator and freezer doors (<code>shelves</code>), and the number of features (<code>features</code>). The features variable includes shelves for cans, see-through crispers, ice makers, egg racks and so on.</p>
<p>Both consumers and manufacturers are interested in models of refrigerator prices. Other things equal, consumers generally prefer larger refrigerators with lower energy costs that have more features. Due to forces of supply and demand, we would expect consumers to pay more for these refrigerators. A larger refrigerator with lower energy costs that has more features at the similar price is considered a bargain to the consumer. How much extra would the consumer be willing to pay for this additional space? A model of prices for refrigerators on the market provides some insight to this question.</p>
<p>To this end, we analyze data from <em>n</em> = 37 refrigerators.</p>
<p><strong>Instructions</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Pre-exercise code</span>
Refrig &lt;-<span class="st"> </span><span class="kw">read.table</span>(<span class="st">&quot;CSVData</span><span class="ch">\\</span><span class="st">Refrig.csv&quot;</span>, <span class="dt">header =</span> <span class="ot">TRUE</span>, <span class="dt">sep =</span> <span class="st">&quot;,&quot;</span>)
<span class="kw">summary</span>(Refrig)
Refrig1 &lt;-<span class="st"> </span>Refrig[<span class="kw">c</span>(<span class="st">&quot;price&quot;</span>, <span class="st">&quot;ecost&quot;</span>, <span class="st">&quot;rsize&quot;</span>, <span class="st">&quot;fsize&quot;</span>, <span class="st">&quot;shelves&quot;</span>, <span class="st">&quot;s_sq_ft&quot;</span>, <span class="st">&quot;features&quot;</span>)]
<span class="kw">round</span>(<span class="kw">cor</span>(Refrig1), <span class="dt">digits =</span> <span class="dv">3</span>)
refrig_mlr1 &lt;-<span class="st"> </span><span class="kw">lm</span>(price <span class="op">~</span><span class="st"> </span>rsize <span class="op">+</span><span class="st"> </span>fsize <span class="op">+</span><span class="st"> </span>shelves <span class="op">+</span><span class="st"> </span>features, <span class="dt">data =</span> Refrig)
<span class="kw">summary</span>(refrig_mlr1)
Refrig<span class="op">$</span>residuals1 &lt;-<span class="st"> </span><span class="kw">residuals</span>(refrig_mlr1)
refrig_mlr2 &lt;-<span class="st"> </span><span class="kw">lm</span>(ecost <span class="op">~</span><span class="st"> </span>rsize <span class="op">+</span><span class="st"> </span>fsize <span class="op">+</span><span class="st"> </span>shelves <span class="op">+</span><span class="st"> </span>features, <span class="dt">data =</span> Refrig)
<span class="kw">summary</span>(refrig_mlr2)
Refrig<span class="op">$</span>residuals2 &lt;-<span class="st"> </span><span class="kw">residuals</span>(refrig_mlr2)
<span class="kw">plot</span>(Refrig<span class="op">$</span>residuals2, Refrig<span class="op">$</span>residuals1)</code></pre></div>
<p><img src="RegressModelDataCamp_files/figure-html/unnamed-chunk-148-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#library(Rcmdr)</span>
<span class="co">#refrig_mlr3 &lt;- lm(price ~ rsize + fsize + shelves + features + ecost, data = Refrig)</span>
<span class="co">#avPlots(refrig_mlr3, terms = &quot;ecost&quot;)</span></code></pre></div>
<pre><code>     price            ecost           rsize          fsize      
 Min.   : 460.0   Min.   :60.00   Min.   :12.6   Min.   :4.100  
 1st Qu.: 545.0   1st Qu.:66.00   1st Qu.:12.9   1st Qu.:4.400  
 Median : 590.0   Median :68.00   Median :13.2   Median :5.100  
 Mean   : 626.4   Mean   :70.51   Mean   :13.4   Mean   :5.184  
 3rd Qu.: 685.0   3rd Qu.:75.00   3rd Qu.:13.9   3rd Qu.:5.700  
 Max.   :1200.0   Max.   :94.00   Max.   :14.7   Max.   :7.400  
    shelves         s_sq_ft         features     
 Min.   :1.000   Min.   :20.60   Min.   : 1.000  
 1st Qu.:2.000   1st Qu.:23.40   1st Qu.: 2.000  
 Median :2.000   Median :24.00   Median : 3.000  
 Mean   :2.514   Mean   :24.53   Mean   : 3.459  
 3rd Qu.:3.000   3rd Qu.:25.50   3rd Qu.: 5.000  
 Max.   :5.000   Max.   :30.20   Max.   :12.000  
          price  ecost  rsize  fsize shelves s_sq_ft features
price     1.000  0.522 -0.024  0.720   0.400   0.155    0.697
ecost     0.522  1.000 -0.033  0.855   0.188   0.058    0.334
rsize    -0.024 -0.033  1.000 -0.235  -0.363   0.401   -0.096
fsize     0.720  0.855 -0.235  1.000   0.251   0.110    0.439
shelves   0.400  0.188 -0.363  0.251   1.000  -0.527    0.160
s_sq_ft   0.155  0.058  0.401  0.110  -0.527   1.000    0.083
features  0.697  0.334 -0.096  0.439   0.160   0.083    1.000

Call:
lm(formula = price ~ rsize + fsize + shelves + features, data = Refrig)

Residuals:
     Min       1Q   Median       3Q      Max 
-128.200  -34.963    7.081   28.716  155.096 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  -698.89     302.60  -2.310  0.02752 *  
rsize          56.50      20.56   2.748  0.00977 ** 
fsize          75.40      13.93   5.414 5.96e-06 ***
shelves        35.92      11.08   3.243  0.00277 ** 
features       25.16       5.04   4.992 2.04e-05 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 68.11 on 32 degrees of freedom
Multiple R-squared:  0.789, Adjusted R-squared:  0.7626 
F-statistic: 29.92 on 4 and 32 DF,  p-value: 2.102e-10


Call:
lm(formula = ecost ~ rsize + fsize + shelves + features, data = Refrig)

Residuals:
    Min      1Q  Median      3Q     Max 
-7.8483 -4.3064  0.5154  2.6324  9.3596 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -14.2165    20.9365  -0.679   0.5020    
rsize         2.8744     1.4225   2.021   0.0517 .  
fsize         8.9085     0.9636   9.245 1.49e-10 ***
shelves       0.2895     0.7664   0.378   0.7081    
features     -0.2006     0.3487  -0.575   0.5692    
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 4.712 on 32 degrees of freedom
Multiple R-squared:  0.7637,    Adjusted R-squared:  0.7342 
F-statistic: 25.86 on 4 and 32 DF,  p-value: 1.247e-09</code></pre>
</div>
</div>
<div id="unusual-observations" class="section level2">
<h2><span class="header-section-number">4.4</span> Unusual observations</h2>
<hr />
<p>In this section, you learn how to:</p>
<ul>
<li>Compare and contrast three alternative definitions of a standardized residual</li>
<li>Evaluate three alternative options for dealing with outliers</li>
<li>Assess the impact of a high leverage observation</li>
<li>Evaluate options for dealing with high leverage observations</li>
<li>Describe the notion of influence and Cook’s Distance for quantifying influence</li>
</ul>
<hr />
<div id="video-17" class="section level3">
<h3><span class="header-section-number">4.4.1</span> Video</h3>
<center>
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/sp/166090200/embedIframeJs/uiconf_id/25916071/partner_id/1660902?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_rhh81ftg&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;&amp;wid=1_58daiyn0" width="649" height="401" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" frameborder="0" title="Kaltura Player">
</iframe>
</center>
<div id="video-overhead-details-17" class="section level4 unnumbered">
<h4>Video Overhead Details</h4>
<div class="tab">
<button class="tablinks" onclick="openTab(event, 'Over4.4A')">
A Details. Unusual observations
</button>
<button class="tablinks" onclick="openTab(event, 'Over4.4B')">
B Details. Standardized residuals
</button>
<button class="tablinks" onclick="openTab(event, 'Over4.4C')">
C Details. Outlier - an unusal standardized residual
</button>
<button class="tablinks" onclick="openTab(event, 'Over4.4D')">
D Details. High leverage points
</button>
<button class="tablinks" onclick="openTab(event, 'Over4.4E')">
E Details. High leverage point graph
</button>
<button class="tablinks" onclick="openTab(event, 'Over4.4F')">
F Details. Leverage
</button>
</div>
<div id="Over4.4A" class="tabcontent">
<span class="topright" onclick="this.parentElement.style.display=&#39;none&#39;">Hide</span>
<h3>
A Details. Unusual observations
</h3>
<p>
<ul>
<li>Regression coefficients can be expressed as (matrix) weighted averages of outcomes
<ul>
<li>Averages, even weighted averages can be strongly influenced by unusual observations</li>
</ul></li>
<li>Observations may be unusual in the <em>y</em> direction or in the <em>X</em> space</li>
<li>For unusual in the <em>y</em> direction, we use a residual <span class="math inline">\(e = y - \hat{y}\)</span>
<ul>
<li>By subtracting the fitted value <span class="math inline">\(\hat{y}\)</span>, we look to the <em>y</em> distance from the regression plane</li>
<li>In this way, we “control” for values of explanatory variables</li>
</ul></li>
</ul>
</p>
</div>
<div id="Over4.4B" class="tabcontent">
<span class="topright" onclick="this.parentElement.style.display=&#39;none&#39;">Hide</span>
<h3>
B Details. Standardized residuals
</h3>
<p>
<p>We standardize residuals so that we can focus on relationships of interest and achieve carry-over of experience from one data set to another.</p>
<p>Three commonly used definitions of standardize residuals are:</p>
<p><span class="math display">\[
\text{(a) }\frac{e_i}{s}, \ \ \ \text{ (b) }\frac{e_i}{s\sqrt{1-h_{ii}}}, \  \   \    
\text{(c)}\frac{e_i}{s_{(i)}\sqrt{1-h_{ii}}}.
\]</span></p>
<ul>
<li>First choice is simple</li>
<li>Second choice, from theory, <span class="math inline">\(\mathrm{Var}(e_i)=\sigma ^{2}(1-h_{ii}).\)</span> Here, <span class="math inline">\(h_{ii}\)</span> is the <span class="math inline">\(i\)</span>th <em>leverage</em> (defined later).</li>
<li>Third choice is termed “studentized residuals”. Idea: numerator is independent of the denominator.</li>
</ul>
</p>
</div>
<div id="Over4.4C" class="tabcontent">
<span class="topright" onclick="this.parentElement.style.display=&#39;none&#39;">Hide</span>
<h3>
C Details. Outlier - an unusal standardized residual
</h3>
<p>
<ul>
<li>An <em>outlier</em> is an observation that is not well fit by the model; these are observations where the residual is unusually large.</li>
<li>Unusual means what? Many packages mark a point if the |standardized residual| &gt; 2.</li>
<li>Options for handling outliers
<ul>
<li>Ignore them in the analysis but be sure to discuss their effects.</li>
<li>Delete them from the data set (but be sure to discuss their effects).</li>
<li>Create a binary variable to indicator their presence. (This will increase your <span class="math inline">\(R^2\)</span>!)</li>
</ul></li>
</ul>
</p>
</div>
<div id="Over4.4D" class="tabcontent">
<span class="topright" onclick="this.parentElement.style.display=&#39;none&#39;">Hide</span>
<h3>
D Details. High leverage points
</h3>
<p>
<ul>
<li>A high leverage point is an observation that is “far away” in the <span class="math inline">\(x\)</span>-space from others.</li>
<li>One can get a feel for high leverage observations by looking a summary statistics (mins, maxs) for each explanatory variable.</li>
<li>Options for dealing with high leverage points are comparable to outliers, we can ignore their effects, delete them, or mark them with a binary indicator variable.</li>
</ul>
</p>
</div>
<div id="Over4.4E" class="tabcontent">
<span class="topright" onclick="this.parentElement.style.display=&#39;none&#39;">Hide</span>
<h3>
E Details. High leverage point graph
</h3>
<p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(cluster)
<span class="co">#library(MASS)</span>
<span class="kw">par</span>(<span class="dt">mar=</span><span class="kw">c</span>(<span class="fl">3.2</span>,<span class="fl">5.4</span>,.<span class="dv">2</span>,.<span class="dv">2</span>))
<span class="kw">plot</span>(<span class="dv">1</span>,<span class="dv">5</span>,<span class="dt">type=</span><span class="st">&quot;p&quot;</span>,<span class="dt">pch=</span><span class="dv">19</span>,<span class="dt">cex=</span><span class="fl">1.5</span>,<span class="dt">xlab=</span><span class="st">&quot;&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;&quot;</span>,<span class="dt">cex.lab=</span><span class="fl">1.5</span>,<span class="dt">xaxt=</span><span class="st">&quot;n&quot;</span>,<span class="dt">yaxt=</span><span class="st">&quot;n&quot;</span>,<span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">3</span>,<span class="dv">5</span>),<span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">12</span>,<span class="dv">12</span>))
<span class="kw">mtext</span>(<span class="kw">expression</span>(x[<span class="dv">2</span>]), <span class="dt">side=</span><span class="dv">1</span>,<span class="dt">line=</span><span class="dv">2</span>, <span class="dt">cex=</span><span class="fl">2.0</span>)
<span class="kw">mtext</span>(<span class="kw">expression</span>(x[<span class="dv">1</span>]), <span class="dt">side=</span><span class="dv">2</span>, <span class="dt">line=</span><span class="dv">2</span>, <span class="dt">las=</span><span class="dv">2</span>, <span class="dt">cex=</span><span class="fl">2.0</span>)
<span class="kw">arrows</span>(<span class="fl">1.5</span>,<span class="dv">5</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dt">code=</span><span class="dv">1</span>,<span class="dt">lwd=</span><span class="dv">2</span>,<span class="dt">angle=</span><span class="dv">15</span>,<span class="dt">length=</span><span class="fl">0.25</span>)
xycov&lt;-<span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">2</span>, <span class="op">-</span><span class="dv">5</span>,<span class="op">-</span><span class="dv">5</span>, <span class="dv">20</span>),<span class="dt">nrow=</span><span class="dv">2</span>,<span class="dt">ncol=</span><span class="dv">2</span>)
xyloc&lt;-<span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>),<span class="dt">nrow=</span><span class="dv">1</span>,<span class="dt">ncol=</span><span class="dv">2</span>)
<span class="kw">polygon</span>(<span class="kw">ellipsoidPoints</span>(xycov, <span class="dt">d2 =</span> <span class="dv">2</span>, <span class="dt">loc=</span>xyloc),<span class="dt">col=</span><span class="st">&quot;black&quot;</span>)</code></pre></div>
<p><img src="RegressModelDataCamp_files/figure-html/unnamed-chunk-149-1.png" width="672" /></p>
</p>
</div>
<div id="Over4.4F" class="tabcontent">
<span class="topright" onclick="this.parentElement.style.display=&#39;none&#39;">Hide</span>
<h3>
F Details. Leverage
</h3>
<p>
<ul>
<li>Using matrix algebra, one can express the <em>i</em>th fitted value as a linear combination of observations</li>
</ul>
<p><span class="math display">\[
\hat{y}_{i} = h_{i1} y_{1} + \cdots +h_{ii}y_{i}+\cdots+h_{in}y_{n}.
\]</span></p>
<ul>
<li>The term <span class="math inline">\(h_{ii}\)</span> is known as the <em>i</em>th leverage
<ul>
<li>The larger the value of <span class="math inline">\(h_{ii}\)</span>, the greater the effect of the <em>i</em>th observation <span class="math inline">\(y_i\)</span> on the <em>i</em>th fitted value <span class="math inline">\(\hat{y}_i\)</span>.</li>
<li>Statistical routines have values of the leverage coded, so computing this quantity. The key thing to know is that <span class="math inline">\(h_{ii}\)</span> is based solely on the explanatory variables. If you change the <span class="math inline">\(y\)</span> values, the leverage does not change.</li>
<li>As a commonly used rule of thumb, a leverage is deemed to be “unusual” if its value exceeds three times the average (= number of regression coefficients divided by the number of observations.)</li>
</ul></li>
</ul>
</p>
</div>
</div>
</div>
<div id="exercise.-outlier-example" class="section level3">
<h3><span class="header-section-number">4.4.2</span> Exercise. Outlier example</h3>
<p>In chapter 2, we consider a fictitious data set of 19 “base” points plus three different types of unusual points. In this exercise, we consider the effect of one unusal point, “C”, this both an outlier (unusual in the “y” direction) and a high leverage point (usual in the x-space). The data have been pre-loaded in the dataframe <code>outlrC</code>.</p>
<p><strong>Instructions</strong></p>
<ul>
<li>Fit a basic linear regression model of <code>y</code> on <code>x</code> and store the result in an object.</li>
<li>Use the function <a href="https://www.rdocumentation.org/packages/stats/versions/3.5.0/topics/influence.measures">rstandard()</a> to extract the standardized residuals from the fitted regression model object and summarize them.</li>
<li>Use the function <a href="https://www.rdocumentation.org/packages/stats/versions/3.5.0/topics/influence.measures">hatvalues()</a> to extract the leverages from the model fitted and summarize them.</li>
<li>Plot the standardized residuals versus the leverages to see the relationship between these two measures that calibrate how unusual an observation is.</li>
</ul>
<div data-datacamp-exercise="" data-height="300" data-encoded="true">
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
</div>
</div>
<div id="exercise.-high-leverage-and-risk-manager-survey" class="section level3">
<h3><span class="header-section-number">4.4.3</span> Exercise. High leverage and risk manager survey</h3>
<p><strong>Assignment Text</strong></p>
<p>In a prior exercise, we fit a regression model of <code>logcost</code> on <code>logsize</code>, <code>indcost</code> and a squared version of <code>indcost</code>. This model is summarized in the object <code>mlr_survey2</code>. In this exercise, we examine the robustness of the model to unusual observations.</p>
<p><strong>Instructions</strong></p>
<ul>
<li>Use the <code>R</code> functions <a href="https://www.rdocumentation.org/packages/stats/versions/3.5.0/topics/influence.measures">rstandard()</a> and <a href="https://www.rdocumentation.org/packages/stats/versions/3.5.0/topics/influence.measures">hatvalues()</a> to extract the standardized residuals and leverages from the model fitted. Summarize the distributions graphically.</li>
<li>You will see that there are two observations where the leverages are high, numbers 10 and 16. On looking at the dataset, these turn out to be observations in a high risk industry. Create a histogram of the variable <code>indcost</code> to corroborate this.</li>
<li>Re-run the regression omitting observations 10 and 16. Summarize this regression and the regression in the object <code>mlr_survey2</code>, noting differences in the coefficients.</li>
</ul>
<div data-datacamp-exercise="" data-height="300" data-encoded="true">
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
</div>
</div>
</div>
<div id="collinearity" class="section level2">
<h2><span class="header-section-number">4.5</span> Collinearity</h2>
<hr />
<p>In this section, you learn how to:</p>
<ul>
<li>Define collinearity and describe its potential impact on regression inference</li>
<li>Define a variance inflation factor and describe its effect on a regression coefficients standard error</li>
<li>Describe rules of thumb for assessing collinearity and options for model reformulation in the presence of severe collinearity</li>
<li>Compare and contrast effects of leverage and collinearity</li>
</ul>
<hr />
<div id="video-18" class="section level3">
<h3><span class="header-section-number">4.5.1</span> Video</h3>
<center>
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/sp/166090200/embedIframeJs/uiconf_id/25916071/partner_id/1660902?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_30hrfg77&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;&amp;wid=1_khmh83je" width="649" height="401" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" frameborder="0" title="Kaltura Player">
</iframe>
</center>
<div id="video-overhead-details-18" class="section level4 unnumbered">
<h4>Video Overhead Details</h4>
<div class="tab">
<button class="tablinks" onclick="openTab(event, 'Over4.5A')">
A Details. Collinearity
</button>
<button class="tablinks" onclick="openTab(event, 'Over4.5B')">
B Details. Quantifying collinearity
</button>
<button class="tablinks" onclick="openTab(event, 'Over4.5C')">
C Details. Options for handling collinearity
</button>
</div>
<div id="Over4.5A" class="tabcontent">
<span class="topright" onclick="this.parentElement.style.display=&#39;none&#39;">Hide</span>
<h3>
A Details. Collinearity
</h3>
<p>
<ul>
<li><em>Collinearity</em>, or <em>multicollinearity</em>, occurs when one explanatory variable is, or nearly is, a linear combination of the other explanatory variables.
<ul>
<li>Useful to think of the explanatory variables as being highly correlated with one another.</li>
</ul></li>
<li>Collinearity neither precludes us from getting good fits nor from making predictions of new observations.
<ul>
<li>Estimates of error variances and, therefore, tests of model adequacy, are still reliable.</li>
</ul></li>
<li>In cases of serious collinearity, standard errors of individual regression coefficients can be large.
<ul>
<li>With large standard errors, individual regression coefficients may not be meaningful.</li>
<li>Because a large standard error means that the corresponding <em>t</em>-ratio is small, it is difficult to detect the importance of a variable.</li>
</ul></li>
</ul>
</p>
</div>
<div id="Over4.5B" class="tabcontent">
<span class="topright" onclick="this.parentElement.style.display=&#39;none&#39;">Hide</span>
<h3>
B Details. Quantifying collinearity
</h3>
<p>
<p>A common way to quantify collinearity is through the <em>variance inflation factor (VIF)</em>.</p>
<ul>
<li>Suppose that the set of explanatory variables is labeled <span class="math inline">\(x_{1},x_{2},\dots,x_{k}\)</span>.</li>
<li>Run the regression using <span class="math inline">\(x_{j}\)</span> as the “outcome” and the other <span class="math inline">\(x\)</span>’s as the explanatory variables.</li>
<li>Denote the coefficient of determination from this regression by <span class="math inline">\(R_j^2\)</span>.</li>
<li>Define the variance inflation factor</li>
</ul>
<p><span class="math display">\[
VIF_{j}=\frac{1}{1-R_{j}^{2}},\ \ \ \text{ for } j = 1,2,\ldots, k.
\]</span></p>
</p>
</div>
<div id="Over4.5C" class="tabcontent">
<span class="topright" onclick="this.parentElement.style.display=&#39;none&#39;">Hide</span>
<h3>
C Details. Options for handling collinearity
</h3>
<p>
<ul>
<li>Rule of thumb: When <span class="math inline">\(VIF_{j}\)</span> exceeds 10 (which is equivalent to <span class="math inline">\(R_{j}^{2}&gt;90\%\)</span>), we say that severe collinearity exists. This may signal is a need for action.</li>
<li>Recode the variables by “centering” - that is, subtract the mean and divide by the standard deviation.</li>
<li>Ignore the collinearity in the analysis but comment on it in the interpretation. Probably the most common approach.</li>
<li>Replace one or more variables by auxiliary variables or transformed versions.</li>
<li>Remove one or more variables. Easy. Which One? is hard.
<ul>
<li>Use interpretation. Which variable(s) do you feel most comfortable with?</li>
<li>Use automatic variable selection procedures to suggest a model.</li>
</ul></li>
</ul>
</p>
</div>
</div>
</div>
<div id="exercise.-collinearity-and-term-life" class="section level3">
<h3><span class="header-section-number">4.5.2</span> Exercise. Collinearity and term life</h3>
<p><strong>Assignment Text</strong> We have seen that adding an explanatory variable <span class="math inline">\(x^2\)</span> to a model is sometimes helpful even though it is perfectly related to <span class="math inline">\(x\)</span> (such as through the function <span class="math inline">\(f(x)=x^2\)</span>). But, for some data sets, higher order polynomials and interactions can be approximately linearly related (depending on the range of the data).</p>
<p>This exercise returns to our term life data set <code>Term1</code> (preloaded) and demonstrates that collinearity can be severe when introducing interaction terms.</p>
<p><strong>Instructions</strong></p>
<ul>
<li>Fit a MLR model of <code>logface</code> on explantory variables <code>education</code>, <code>numhh</code> and <code>logincome</code></li>
<li>Use the function <a href="https://www.rdocumentation.org/packages/car/versions/3.0-0/topics/vif">vif()</a> from the <code>car</code> package (preloaded) to calculate variance inflation factors.</li>
<li>Fit and summarize a MLR model of <code>logface</code> on explantory variables <code>education</code> , <code>numhh</code> and <code>logincome</code> with an interaction between <code>numhh</code> and <code>logincome</code>, then extract variance inflation factors.</li>
</ul>
<p><strong>Hint.</strong> If the <code>car</code> package is not available to you, then you could calculate vifs using the [lm()] function, treating each variable separately. For example</p>
<p><code>1/(1-summary(lm(education ~ numhh + logincome, data = Term1))$r.squared)</code></p>
<p>gives the <code>education</code> vif.</p>
<div data-datacamp-exercise="" data-height="300" data-encoded="true">
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
</div>
</div>
</div>
<div id="selection-criteria" class="section level2">
<h2><span class="header-section-number">4.6</span> Selection criteria</h2>
<hr />
<p>In this section, you learn how to:</p>
<ul>
<li>Summarize a regression fit using alternative goodness of fit measures</li>
<li>Validate a model using in-sample and out-of-sample data to mitigate issues of data-snooping</li>
<li>Compare and contrast <em>SSPE</em> and <em>PRESS</em> statistics for model validation</li>
</ul>
<hr />
<div id="video-19" class="section level3">
<h3><span class="header-section-number">4.6.1</span> Video</h3>
<center>
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/sp/166090200/embedIframeJs/uiconf_id/25916071/partner_id/1660902?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=0_i5dlmsna&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;&amp;wid=0_kvkokgm2" width="649" height="401" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" frameborder="0" title="Kaltura Player">
</iframe>
</center>
<div id="video-overhead-details-19" class="section level4 unnumbered">
<h4>Video Overhead Details</h4>
<div class="tab">
<button class="tablinks" onclick="openTab(event, 'Over4.6A')">
A Details. Goodness of fit
</button>
<button class="tablinks" onclick="openTab(event, 'Over4.6B')">
B Details. Goodness of fit and information criteria
</button>
<button class="tablinks" onclick="openTab(event, 'Over4.6C')">
C Details. Out of sample validation
</button>
<button class="tablinks" onclick="openTab(event, 'Over4.6D')">
D Details. Out of sample validation procedure
</button>
<button class="tablinks" onclick="openTab(event, 'Over4.6E')">
E Details. Cross - validation
</button>
</div>
<div id="Over4.6A" class="tabcontent">
<span class="topright" onclick="this.parentElement.style.display=&#39;none&#39;">Hide</span>
<h3>
A Details. Goodness of fit
</h3>
<p>
<ul>
<li>Criteria that measure the proximity of the fitted model and realized data are known as <em>goodness of fit</em> statistics.</li>
<li>Basic examples include:
<ul>
<li>the coefficient of determination <span class="math inline">\((R^{2})\)</span>,</li>
<li>an adjusted version <span class="math inline">\((R_{a}^{2})\)</span>,</li>
<li>the size of the typical error <span class="math inline">\((s)\)</span>, and</li>
<li><span class="math inline">\(t\)</span>-ratios for each regression coefficient.</li>
</ul></li>
</ul>
</p>
</div>
<div id="Over4.6B" class="tabcontent">
<span class="topright" onclick="this.parentElement.style.display=&#39;none&#39;">Hide</span>
<h3>
A Details. Goodness of fit
</h3>
<p>
<p>A general measure is <em>Akaike’s Information Criterion</em>, defined as</p>
<p><span class="math display">\[
AIC = -2 \times (fitted~log~likelihood) + 2 \times
(number~of~parameters)
\]</span></p>
<ul>
<li>For model comparison, the smaller the <span class="math inline">\(AIC,\)</span> the better is the fit.</li>
<li>This measures balances the fit (in the first part) with a penalty for complexity (in the second part)</li>
<li>It is a general measure - for linear regression, it reduces to</li>
</ul>
<p><span class="math display">\[
AIC = n \ln (s^2) + n \ln (2 \pi) +n +k + 3 .
\]</span></p>
<ul>
<li>So, selecting a model to minimize <span class="math inline">\(s\)</span> or <span class="math inline">\(s^2\)</span> is equivalent to model selection based on minimizing <span class="math inline">\(AIC\)</span> (same <em>k</em>).</li>
</ul>
</p>
</div>
<div id="Over4.6C" class="tabcontent">
<span class="topright" onclick="this.parentElement.style.display=&#39;none&#39;">Hide</span>
<h3>
C Details. Out of sample validation
</h3>
<p>
<ul>
<li>When you choose a model to minimize <span class="math inline">\(s\)</span> or <span class="math inline">\(AIC\)</span>, it is based on how well the model fits the data at hand, or the <em>model development</em>, or <em>training</em>, data</li>
<li>As we have seen, this approach is susceptible to overfitting.</li>
<li>A better approach is to validate the model on a <em>model validation</em>, or <em>test</em> data set, held out for this purpose.</li>
</ul>
</p>
</div>
<div id="Over4.6D" class="tabcontent">
<span class="topright" onclick="this.parentElement.style.display=&#39;none&#39;">Hide</span>
<h3>
D Details. Out of sample validation procedure
</h3>
<p>
<ul>
<li><ol style="list-style-type: lower-roman">
<li>Using the model development subsample, fit a candidate model.</li>
</ol></li>
<li><ol start="2" style="list-style-type: lower-roman">
<li>Using the Step (ii) model and the explanatory variables from the validation subsample, “predict” the dependent variables in the validation subsample, <span class="math inline">\(\hat{y}_i\)</span>, where <span class="math inline">\(i=n_{1}+1,...,n_{1}+n_{2}\)</span>.</li>
</ol></li>
<li><ol start="3" style="list-style-type: lower-roman">
<li>Calc the *sum of absolute prediction errors**</li>
</ol></li>
</ul>
<p><span class="math display">\[SAPE=\sum_{i=n_{1}+1}^{n_{1}+n_{2}} |y_{i}-\hat{y}_{i}| . \]</span></p>
<p>Repeat Steps (i) through (iii) for each candidate model. Choose the model with the smallest <em>SAPE</em>.</p>
</p>
</div>
<div id="Over4.6E" class="tabcontent">
<span class="topright" onclick="this.parentElement.style.display=&#39;none&#39;">Hide</span>
<h3>
E Details. Cross - validation
</h3>
<p>
<ul>
<li>With out-of-sample validation, the statistic depends on a random split between in-sample and out-of-sample data (a problem for data sets that are not large)</li>
<li>Alternatively, one may use <em>cross-validation</em>
<ul>
<li>Use a random mechanism to split the data into <em>k</em> subsets, (e.g., 5-10)</li>
<li>Use the first <em>k-1</em> subsamples to estimate model parameters. Then, “predict” the outcomes for the <em>k</em>th subsample and use <em>SAE</em> to summarize the fit</li>
<li>Repeat this by holding out each of the <em>k</em> sub-samples, summarizing with a cumulative <em>SAE</em>.</li>
</ul></li>
<li>Repeat these steps for several candidate models.
<ul>
<li>Choose the model with the lowest cumulative <em>SAE</em> statistic.</li>
</ul></li>
</ul>
</p>
</div>
</div>
</div>
<div id="exercise.-cross-validation-and-term-life" class="section level3">
<h3><span class="header-section-number">4.6.2</span> Exercise. Cross-validation and term life</h3>
<p><strong>Assignment Text</strong></p>
<p>Here is some sample code to give you a better feel for cross-validation.</p>
<p>The first part of the randomly re-orders (“shuffles”) the data. It also identifies explanatory variables <code>explvars</code>.</p>
<p>The function starts by pulling out only the needed data into <code>cvdata</code>. Then, for each subsample, a model is fit based on all the data except for the subsample, in <code>train_mlr</code> with the subsample in <code>test</code>. This is repeated for each subsample, then results are summarized.</p>
<h6 style="text-align: center;">
<a id="displayExer4.6" href="javascript:togglecode('toggleExer4.6','displayExer4.6');"><i><strong>Show Code</strong></i></a>
</h6>
<div id="toggleExer4.6" style="display: none">
<pre><code># Randomly re-order data - &quot;shuffle it&quot;
n &lt;- nrow(Term1)
set.seed(12347)
shuffled_Term1 &lt;- Term1[sample(n), ]
explvars &lt;- c(&quot;education&quot;, &quot;numhh&quot;, &quot;logincome&quot;)

## Cross - Validation
crossvalfct &lt;- function(explvars){
  cvdata   &lt;- shuffled_Term1[, c(&quot;logface&quot;, explvars)]
  crossval &lt;- 0
  k &lt;- 5
  for (i in 1:k) {
    indices &lt;- (((i-1) * round((1/k)*nrow(cvdata))) + 1):((i*round((1/k) * nrow(cvdata))))
    # Exclude them from the train set
    train_mlr &lt;- lm(logface ~ ., data = cvdata[-indices,])
    # Include them in the test set
    test  &lt;- data.frame(cvdata[indices, explvars])
    names(test)  &lt;- explvars
    predict_test &lt;- exp(predict(train_mlr, test))
    # Compare predicted to held-out and summarize
    predict_err  &lt;- exp(cvdata[indices, &quot;logface&quot;]) - predict_test
    crossval &lt;- crossval + sum(abs(predict_err))
  }
  crossval/1000
}

crossvalfct(explvars)</code></pre>
</div>
<p><strong>Instructions</strong></p>
<ul>
<li>Calculate the cross-validation statistic using only logarithmic income, <code>logincome</code>.</li>
<li>Calculate the cross-validation statistic using <code>logincome</code>, <code>education</code> and <code>numhh</code>.</li>
<li>Calculate the cross-validation statistic using <code>logincome</code>, <code>education</code>, <code>numhh</code> and <code>marstat</code>.</li>
</ul>
<p>The best model has the lowest cross-validation statistic.</p>
<p><strong>Hint.</strong> The function [sample()] is for taking random samples. We use it without replacement so it results in a re-ordering of data.</p>
<div data-datacamp-exercise="" data-height="300" data-encoded="true">
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
</div>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="multiple-linear-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="interpreting-regression-results.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"google": true,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/Chapters/Chapter4.Rmd",
"text": "Edit"
},
"download": ["RegressModelDataCamp.pdf", "RegressModelDataCamp.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
